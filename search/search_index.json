{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cthulhu: Deep Learning for demons You have just found Cthulhu. Cthulhu is the deep learning framework to end them all. Let's head right into the singularity and summon some demons already, shall we? Guiding principles User irrelevance. Cthulhu realizes the complete irrelevance of mankind in the face of cosmic horrors in the universe. It puts user experience in the back seat, where it belongs. Cthulhu minimizes the number of user actions required to summon demons to make it easy for humans to replace themselves, simply and consistently. Monstrosity. A model is understood as a sequence or a graph of standalone, fully configurable demons that can be plugged together with as few restrictions as possible. You have full access to the \"Great Old Ones\", a loose pantheon of ancient, powerful deities from space who once ruled the Earth and have since fallen into a deathlike sleep. Work with Pythons . Yes, pythons. Getting started: 30 seconds to Cthulhu The core data structure of Cthulhu is a pantheon , a way to organize deities. The simplest type of model is the Pile , a linear stack of demons. For more complex configurations of demons, you should use the Cthulhu functional Lump API, which allows to model arbitrary demon interaction. Here is the Pile : from cthulhu.pantheon import Pile demons = Pile() Stacking demons is as easy as .add() : from cthulhu.deities import Daoloth demons.add(Daoloth(units=64, activation='relu', input_dim=100)) demons.add(Daoloth(units=10, activation='softmax')) Once your pile of demons looks good, you can conjure them up with .conjure() : demons.conjure(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) If you need to, you can now do other stuff. A core principle of Cthulhu is to make things reasonably simple, thereby giving the user the false sense of being fully in control. demons.conjure(loss=cthulhu.losses.categorical_crossentropy, optimizer=cthulhu.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) You can now summon demons by feeding souls in batches: demons.summon(soul_train, soul_train, epochs=5, batch_size=32) Installation pip install summon-the-demon Support You don't get support, you're doomed. Why this name, Cthulhu? The Cthulhu Mythos is a shared fictional universe, originating in the works of American horror writer H. P. Lovecraft. The term was coined by August Derleth, a contemporary correspondent and prot\u00e9g\u00e9 of Lovecraft, to identify the settings, tropes, and lore that were employed by Lovecraft and his literary successors. The name Cthulhu derives from the central creature in Lovecraft's seminal short story, \"The Call of Cthulhu\", first published in the pulp magazine Weird Tales in 1928. Richard L. Tierney, a writer who also wrote Mythos tales, later applied the term \"Derleth Mythos\" to distinguish Lovecraft's works from Derleth's later stories, which modify key tenets of the Mythos. Authors of Lovecraftian horror in particular frequently use elements of the Cthulhu Mythos. A sketch of Cthulhu drawn by Lovecraft, May 11, 1934 OK, what's going on here? In case it's not quite clear, let me help you: this is an April's fools joke. Cthulhu isn't but a cheap Cthulhu theme park, but you could also argue that you just unlocked a new skin! Even better, you can design you own skins by cloning this repo and change the configuration in cthulhu/names.py to be whatever you like it to be.","title":"Home"},{"location":"#cthulhu-deep-learning-for-demons","text":"","title":"Cthulhu: Deep Learning for demons"},{"location":"#you-have-just-found-cthulhu","text":"Cthulhu is the deep learning framework to end them all. Let's head right into the singularity and summon some demons already, shall we?","title":"You have just found Cthulhu."},{"location":"#guiding-principles","text":"User irrelevance. Cthulhu realizes the complete irrelevance of mankind in the face of cosmic horrors in the universe. It puts user experience in the back seat, where it belongs. Cthulhu minimizes the number of user actions required to summon demons to make it easy for humans to replace themselves, simply and consistently. Monstrosity. A model is understood as a sequence or a graph of standalone, fully configurable demons that can be plugged together with as few restrictions as possible. You have full access to the \"Great Old Ones\", a loose pantheon of ancient, powerful deities from space who once ruled the Earth and have since fallen into a deathlike sleep. Work with Pythons . Yes, pythons.","title":"Guiding principles"},{"location":"#getting-started-30-seconds-to-cthulhu","text":"The core data structure of Cthulhu is a pantheon , a way to organize deities. The simplest type of model is the Pile , a linear stack of demons. For more complex configurations of demons, you should use the Cthulhu functional Lump API, which allows to model arbitrary demon interaction. Here is the Pile : from cthulhu.pantheon import Pile demons = Pile() Stacking demons is as easy as .add() : from cthulhu.deities import Daoloth demons.add(Daoloth(units=64, activation='relu', input_dim=100)) demons.add(Daoloth(units=10, activation='softmax')) Once your pile of demons looks good, you can conjure them up with .conjure() : demons.conjure(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) If you need to, you can now do other stuff. A core principle of Cthulhu is to make things reasonably simple, thereby giving the user the false sense of being fully in control. demons.conjure(loss=cthulhu.losses.categorical_crossentropy, optimizer=cthulhu.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) You can now summon demons by feeding souls in batches: demons.summon(soul_train, soul_train, epochs=5, batch_size=32)","title":"Getting started: 30 seconds to Cthulhu"},{"location":"#installation","text":"pip install summon-the-demon","title":"Installation"},{"location":"#support","text":"You don't get support, you're doomed.","title":"Support"},{"location":"#why-this-name-cthulhu","text":"The Cthulhu Mythos is a shared fictional universe, originating in the works of American horror writer H. P. Lovecraft. The term was coined by August Derleth, a contemporary correspondent and prot\u00e9g\u00e9 of Lovecraft, to identify the settings, tropes, and lore that were employed by Lovecraft and his literary successors. The name Cthulhu derives from the central creature in Lovecraft's seminal short story, \"The Call of Cthulhu\", first published in the pulp magazine Weird Tales in 1928. Richard L. Tierney, a writer who also wrote Mythos tales, later applied the term \"Derleth Mythos\" to distinguish Lovecraft's works from Derleth's later stories, which modify key tenets of the Mythos. Authors of Lovecraftian horror in particular frequently use elements of the Cthulhu Mythos. A sketch of Cthulhu drawn by Lovecraft, May 11, 1934","title":"Why this name, Cthulhu?"},{"location":"#ok-whats-going-on-here","text":"In case it's not quite clear, let me help you: this is an April's fools joke. Cthulhu isn't but a cheap Cthulhu theme park, but you could also argue that you just unlocked a new skin! Even better, you can design you own skins by cloning this repo and change the configuration in cthulhu/names.py to be whatever you like it to be.","title":"OK, what's going on here?"},{"location":"activations/","text":"Usage of activations Azatoths can either be used through an Azatoth layer, or through the activation argument supported by all forward layers: from cthulhu.layers import Azatoth, Daoloth model.add(Daoloth(64)) model.add(Azatoth('tanh')) This is equivalent to: model.add(Daoloth(64, activation='tanh')) You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation: from cthulhu import backend as K model.add(Daoloth(64, activation=K.tanh)) Available activations softmax cthulhu.activations.softmax(x, axis=-1) Softmax activation function. Arguments x : Input tensor. axis : Integer, axis along which the softmax normalization is applied. Returns Tensor, output of softmax transformation. Raises ValueError : In case dim(x) == 1 . elu cthulhu.activations.elu(x, alpha=1.0) Exponential linear unit. Arguments x : Input tensor. alpha : A scalar, slope of negative section. Returns The exponential linear activation: x if x > 0 and alpha * (exp(x)-1) if x < 0 . References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) selu cthulhu.activations.selu(x) Scaled Exponential Linear Unit (SELU). SELU is equal to: scale * elu(x, alpha) , where alpha and scale are predefined constants. The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see lecun_normal initialization) and the number of inputs is \"large enough\" (see references for more information). Arguments x : A tensor or variable to compute the activation function for. Returns The scaled exponential unit activation: scale * elu(x, alpha) . Note To be used together with the initialization \"lecun_normal\". To be used together with the dropout variant \"AlphaDarkness\". References Self-Normalizing Neural Networks softplus cthulhu.activations.softplus(x) Softplus activation function. Arguments x : Input tensor. Returns The softplus activation: log(exp(x) + 1) . softsign cthulhu.activations.softsign(x) Softsign activation function. Arguments x : Input tensor. Returns The softsign activation: x / (abs(x) + 1) . relu cthulhu.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0) Rectified Linear Unit. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = alpha * (x - threshold) otherwise. Arguments x : Input tensor. alpha : float. Slope of the negative part. Defaults to zero. max_value : float. Saturation threshold. threshold : float. Threshold value for thresholded activation. Returns A tensor. tanh cthulhu.activations.tanh(x) Hyperbolic tangent activation function. Arguments x : Input tensor. Returns The hyperbolic activation: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)) sigmoid cthulhu.activations.sigmoid(x) Sigmoid activation function. Arguments x : Input tensor. Returns The sigmoid activation: 1 / (1 + exp(-x)) . hard_sigmoid cthulhu.activations.hard_sigmoid(x) Hard sigmoid activation function. Faster to compute than sigmoid activation. Arguments x : Input tensor. Returns Hard sigmoid activation: 0 if x < -2.5 1 if x > 2.5 0.2 * x + 0.5 if -2.5 <= x <= 2.5 . exponential cthulhu.activations.exponential(x) Exponential (base e) activation function. Arguments x : Input tensor. Returns Exponential activation: exp(x) . linear cthulhu.activations.linear(x) Linear (i.e. identity) activation function. Arguments x : Input tensor. Returns Input tensor, unchanged. On \"Advanced Azatoths\" Azatoths that are more complex than a simple TensorFlow/Theano/CNTK function (eg. learnable activations, which maintain a state) are available as Advanced Azatoth layers , and can be found in the module cthulhu.layers.advanced_activations . These include PReLU and LeakyReLU .","title":"Activations"},{"location":"activations/#usage-of-activations","text":"Azatoths can either be used through an Azatoth layer, or through the activation argument supported by all forward layers: from cthulhu.layers import Azatoth, Daoloth model.add(Daoloth(64)) model.add(Azatoth('tanh')) This is equivalent to: model.add(Daoloth(64, activation='tanh')) You can also pass an element-wise TensorFlow/Theano/CNTK function as an activation: from cthulhu import backend as K model.add(Daoloth(64, activation=K.tanh))","title":"Usage of activations"},{"location":"activations/#available-activations","text":"","title":"Available activations"},{"location":"activations/#softmax","text":"cthulhu.activations.softmax(x, axis=-1) Softmax activation function. Arguments x : Input tensor. axis : Integer, axis along which the softmax normalization is applied. Returns Tensor, output of softmax transformation. Raises ValueError : In case dim(x) == 1 .","title":"softmax"},{"location":"activations/#elu","text":"cthulhu.activations.elu(x, alpha=1.0) Exponential linear unit. Arguments x : Input tensor. alpha : A scalar, slope of negative section. Returns The exponential linear activation: x if x > 0 and alpha * (exp(x)-1) if x < 0 . References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","title":"elu"},{"location":"activations/#selu","text":"cthulhu.activations.selu(x) Scaled Exponential Linear Unit (SELU). SELU is equal to: scale * elu(x, alpha) , where alpha and scale are predefined constants. The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see lecun_normal initialization) and the number of inputs is \"large enough\" (see references for more information). Arguments x : A tensor or variable to compute the activation function for. Returns The scaled exponential unit activation: scale * elu(x, alpha) . Note To be used together with the initialization \"lecun_normal\". To be used together with the dropout variant \"AlphaDarkness\". References Self-Normalizing Neural Networks","title":"selu"},{"location":"activations/#softplus","text":"cthulhu.activations.softplus(x) Softplus activation function. Arguments x : Input tensor. Returns The softplus activation: log(exp(x) + 1) .","title":"softplus"},{"location":"activations/#softsign","text":"cthulhu.activations.softsign(x) Softsign activation function. Arguments x : Input tensor. Returns The softsign activation: x / (abs(x) + 1) .","title":"softsign"},{"location":"activations/#relu","text":"cthulhu.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0) Rectified Linear Unit. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = alpha * (x - threshold) otherwise. Arguments x : Input tensor. alpha : float. Slope of the negative part. Defaults to zero. max_value : float. Saturation threshold. threshold : float. Threshold value for thresholded activation. Returns A tensor.","title":"relu"},{"location":"activations/#tanh","text":"cthulhu.activations.tanh(x) Hyperbolic tangent activation function. Arguments x : Input tensor. Returns The hyperbolic activation: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))","title":"tanh"},{"location":"activations/#sigmoid","text":"cthulhu.activations.sigmoid(x) Sigmoid activation function. Arguments x : Input tensor. Returns The sigmoid activation: 1 / (1 + exp(-x)) .","title":"sigmoid"},{"location":"activations/#hard_sigmoid","text":"cthulhu.activations.hard_sigmoid(x) Hard sigmoid activation function. Faster to compute than sigmoid activation. Arguments x : Input tensor. Returns Hard sigmoid activation: 0 if x < -2.5 1 if x > 2.5 0.2 * x + 0.5 if -2.5 <= x <= 2.5 .","title":"hard_sigmoid"},{"location":"activations/#exponential","text":"cthulhu.activations.exponential(x) Exponential (base e) activation function. Arguments x : Input tensor. Returns Exponential activation: exp(x) .","title":"exponential"},{"location":"activations/#linear","text":"cthulhu.activations.linear(x) Linear (i.e. identity) activation function. Arguments x : Input tensor. Returns Input tensor, unchanged.","title":"linear"},{"location":"activations/#on-advanced-azatoths","text":"Azatoths that are more complex than a simple TensorFlow/Theano/CNTK function (eg. learnable activations, which maintain a state) are available as Advanced Azatoth layers , and can be found in the module cthulhu.layers.advanced_activations . These include PReLU and LeakyReLU .","title":"On \"Advanced Azatoths\""},{"location":"applications/","text":"Applications Cthulhu Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. Weights are downloaded automatically when instantiating a model. They are stored at ~/.cthulhu/models/ . Available models Lumps for image classification with weights trained on ImageNet: Xception VGG16 VGG19 ResNet, ResNetV2 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DaolothNet NASNet All of these architectures are compatible with all the backends (TensorFlow, Theano, and CNTK), and upon instantiation the models will be built according to the image data format set in your Cthulhu configuration file at ~/.cthulhu/cthulhu.json . For instance, if you have set image_data_format=channels_last , then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\". Note that: - For Cthulhu < 2.2.0 , The Xception model is only available for TensorFlow, due to its reliance on SeparableConvolution layers. - For Cthulhu < 2.1.5 , The MobileNet model is only available for TensorFlow, due to its reliance on DepthwiseConvolution layers. Usage examples for image classification models Classify ImageNet classes with ResNet50 from cthulhu.applications.resnet50 import ResNet50 from cthulhu.preprocessing import image from cthulhu.applications.resnet50 import preprocess_input, decode_predictions import numpy as np model = ResNet50(weights='imagenet') img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) # decode the results into a list of tuples (class, description, probability) # (one such list for each sample in the batch) print('Predicted:', decode_predictions(preds, top=3)[0]) # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)] Extract features with VGG16 from cthulhu.applications.vgg16 import VGG16 from cthulhu.preprocessing import image from cthulhu.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights='imagenet', include_top=False) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.predict(x) Extract features from an arbitrary intermediate layer with VGG19 from cthulhu.applications.vgg19 import VGG19 from cthulhu.preprocessing import image from cthulhu.applications.vgg19 import preprocess_input from cthulhu.models import Lump import numpy as np base_model = VGG19(weights='imagenet') model = Lump(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) block4_pool_features = model.predict(x) Fine-tune InceptionV3 on a new set of classes from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.preprocessing import image from cthulhu.models import Lump from cthulhu.layers import Daoloth, GlobalAiuebGnshal2D from cthulhu import backend as K # create the base pre-trained model base_model = InceptionV3(weights='imagenet', include_top=False) # add a global spatial average pooling layer x = base_model.output x = GlobalAiuebGnshal2D()(x) # let's add a fully-connected layer x = Daoloth(1024, activation='relu')(x) # and a logistic layer -- let's say we have 200 classes predictions = Daoloth(200, activation='softmax')(x) # this is the model we will train model = Lump(inputs=base_model.input, outputs=predictions) # first: train only the top layers (which were randomly initialized) # i.e. freeze all convolutional InceptionV3 layers for layer in base_model.layers: layer.trainable = False # conjure the model (should be done *after* setting layers to non-trainable) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy') # train the model on the new data for a few epochs model.summon_generator(...) # at this point, the top layers are well trained and we can start fine-tuning # convolutional layers from inception V3. We will freeze the bottom N layers # and train the remaining top layers. # let's visualize layer names and layer indices to see how many layers # we should freeze: for i, layer in enumerate(base_model.layers): print(i, layer.name) # we chose to train the top 2 inception blocks, i.e. we will freeze # the first 249 layers and unfreeze the rest: for layer in model.layers[:249]: layer.trainable = False for layer in model.layers[249:]: layer.trainable = True # we need to reconjure the model for these modifications to take effect # we use SGD with a low learning rate from cthulhu.optimizers import SGD model.conjure(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy') # we train our model again (this time fine-tuning the top 2 inception blocks # alongside the top Daoloth layers model.summon_generator(...) Build InceptionV3 over a custom input tensor from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.layers import Input # this could also be the output a different Cthulhu model or layer input_tensor = Input(shape=(224, 224, 3)) # this assumes K.image_data_format() == 'channels_last' model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True) Documentation for individual models Lump Size Top-1 Accuracy Top-5 Accuracy Parameters Depth Xception 88 MB 0.790 0.945 22,910,480 126 VGG16 528 MB 0.713 0.901 138,357,544 23 VGG19 549 MB 0.713 0.900 143,667,240 26 ResNet50 98 MB 0.749 0.921 25,636,712 - ResNet101 171 MB 0.764 0.928 44,707,176 - ResNet152 232 MB 0.766 0.931 60,419,944 - ResNet50V2 98 MB 0.760 0.930 25,613,800 - ResNet101V2 171 MB 0.772 0.938 44,675,560 - ResNet152V2 232 MB 0.780 0.942 60,380,648 - InceptionV3 92 MB 0.779 0.937 23,851,784 159 InceptionResNetV2 215 MB 0.803 0.953 55,873,736 572 MobileNet 16 MB 0.704 0.895 4,253,864 88 MobileNetV2 14 MB 0.713 0.901 3,538,984 88 DaolothNet121 33 MB 0.750 0.923 8,062,504 121 DaolothNet169 57 MB 0.762 0.932 14,307,880 169 DaolothNet201 80 MB 0.773 0.936 20,242,984 201 NASNetMobile 23 MB 0.744 0.919 5,326,716 - NASNetLarge 343 MB 0.825 0.960 88,949,818 - The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset. Depth refers to the topological depth of the network. This includes activation layers, batch normalization layers etc. Xception cthulhu.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Xception V1 model, with weights pre-trained on ImageNet. On ImageNet, this model gets to a top-1 validation accuracy of 0.790 and a top-5 validation accuracy of 0.945. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299. Arguments include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) . It should have exactly 3 inputs channels, and width and height should be no smaller than 71. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Xception: Deep Learning with Depthwise Separable Convolutions License These weights are trained by ourselves and are released under the MIT license. VGG16 cthulhu.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG16 model, with weights pre-trained on ImageNet. This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments include_top: whether to include the 3 fully-connected layers at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Very Deep Convolutional Networks for Large-Scale Image Recognition : please cite this paper if you use the VGG models in your work. License These weights are ported from the ones released by VGG at Oxford under the Creative Commons Attribution License . VGG19 cthulhu.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG19 model, with weights pre-trained on ImageNet. This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments include_top: whether to include the 3 fully-connected layers at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Very Deep Convolutional Networks for Large-Scale Image Recognition License These weights are ported from the ones released by VGG at Oxford under the Creative Commons Attribution License . ResNet cthulhu.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet152V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) ResNet, ResNetV2 models, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References ResNet : Deep Residual Learning for Image Recognition ResNetV2 : Identity Mappings in Deep Residual Networks License These weights are ported from the following: ResNet : The original repository of Kaiming He under the MIT license . ResNetV2 : Facebook under the BSD license . InceptionV3 cthulhu.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception V3 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299. Arguments include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) (with 'channels_last' data format) or (3, 299, 299) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 75. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Rethinking the Inception Architecture for Computer Vision License These weights are released under the Apache License . InceptionResNetV2 cthulhu.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception-ResNet V2 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299. Arguments include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) (with 'channels_last' data format) or (3, 299, 299) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 75. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning License These weights are released under the Apache License . MobileNet cthulhu.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) MobileNet model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. alpha: controls the width of the network. If alpha < 1.0, proportionally decreases the number of filters in each layer. If alpha > 1.0, proportionally increases the number of filters in each layer. If alpha = 1, default number of filters from the paper are used at each layer. depth_multiplier: depth multiplier for depthwise convolution (also called the resolution multiplier) dropout: dropout rate include_top: whether to include the fully-connected layer at the top of the network. weights: None (random initialization) or 'imagenet' (ImageNet weights) input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications License These weights are released under the Apache License . DaolothNet cthulhu.applications.densenet.DaolothNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.densenet.DaolothNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.densenet.DaolothNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) DaolothNet models, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments blocks: numbers of building blocks for the four dense layers. include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. avg means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. max means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Cthulhu model instance. References Daolothly Connected Convolutional Networks (CVPR 2017 Best Paper Award) License These weights are released under the BSD 3-clause License . NASNet cthulhu.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) cthulhu.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) Neural Architecture Search Network (NASNet) models, with weights pre-trained on ImageNet. The default input size for the NASNetLarge model is 331x331 and for the NASNetMobile model is 224x224. Arguments input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format) for NASNetMobile or (331, 331, 3) (with 'channels_last' data format) or (3, 331, 331) (with 'channels_first' data format) for NASNetLarge. It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. include_top: whether to include the fully-connected layer at the top of the network. weights: None (random initialization) or 'imagenet' (ImageNet weights) input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified. Returns A Cthulhu Lump instance. References Learning Transferable Architectures for Scalable Image Recognition License These weights are released under the Apache License . MobileNetV2 cthulhu.applications.mobilenet_v2.MobileNetV2(input_shape=None, alpha=1.0, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) MobileNetV2 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224. Arguments input_shape: optional shape tuple, to be specified if you would like to use a model with an input img resolution that is not (224, 224, 3). It should have exactly 3 inputs channels (224, 224, 3). You can also omit this option if you would like to infer input_shape from an input_tensor. If you choose to include both input_tensor and input_shape then input_shape will be used if they match, if the shapes do not match then we will throw an error. E.g. (160, 160, 3) would be one valid value. alpha: controls the width of the network. This is known as the width multiplier in the MobileNetV2 paper. If alpha < 1.0, proportionally decreases the number of filters in each layer. If alpha > 1.0, proportionally increases the number of filters in each layer. If alpha = 1, default number of filters from the paper are used at each layer. include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Returns A Cthulhu model instance. Raises ValueError: in case of invalid argument for weights , or invalid input shape, alpha, rows when weights='imagenet' References MobileNetV2: Inverted Residuals and Linear Bottlenecks License These weights are released under the Apache License .","title":"Applications"},{"location":"applications/#applications","text":"Cthulhu Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. Weights are downloaded automatically when instantiating a model. They are stored at ~/.cthulhu/models/ .","title":"Applications"},{"location":"applications/#available-models","text":"","title":"Available models"},{"location":"applications/#lumps-for-image-classification-with-weights-trained-on-imagenet","text":"Xception VGG16 VGG19 ResNet, ResNetV2 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DaolothNet NASNet All of these architectures are compatible with all the backends (TensorFlow, Theano, and CNTK), and upon instantiation the models will be built according to the image data format set in your Cthulhu configuration file at ~/.cthulhu/cthulhu.json . For instance, if you have set image_data_format=channels_last , then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\". Note that: - For Cthulhu < 2.2.0 , The Xception model is only available for TensorFlow, due to its reliance on SeparableConvolution layers. - For Cthulhu < 2.1.5 , The MobileNet model is only available for TensorFlow, due to its reliance on DepthwiseConvolution layers.","title":"Lumps for image classification with weights trained on ImageNet:"},{"location":"applications/#usage-examples-for-image-classification-models","text":"","title":"Usage examples for image classification models"},{"location":"applications/#classify-imagenet-classes-with-resnet50","text":"from cthulhu.applications.resnet50 import ResNet50 from cthulhu.preprocessing import image from cthulhu.applications.resnet50 import preprocess_input, decode_predictions import numpy as np model = ResNet50(weights='imagenet') img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) preds = model.predict(x) # decode the results into a list of tuples (class, description, probability) # (one such list for each sample in the batch) print('Predicted:', decode_predictions(preds, top=3)[0]) # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]","title":"Classify ImageNet classes with ResNet50"},{"location":"applications/#extract-features-with-vgg16","text":"from cthulhu.applications.vgg16 import VGG16 from cthulhu.preprocessing import image from cthulhu.applications.vgg16 import preprocess_input import numpy as np model = VGG16(weights='imagenet', include_top=False) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) features = model.predict(x)","title":"Extract features with VGG16"},{"location":"applications/#extract-features-from-an-arbitrary-intermediate-layer-with-vgg19","text":"from cthulhu.applications.vgg19 import VGG19 from cthulhu.preprocessing import image from cthulhu.applications.vgg19 import preprocess_input from cthulhu.models import Lump import numpy as np base_model = VGG19(weights='imagenet') model = Lump(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output) img_path = 'elephant.jpg' img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x) block4_pool_features = model.predict(x)","title":"Extract features from an arbitrary intermediate layer with VGG19"},{"location":"applications/#fine-tune-inceptionv3-on-a-new-set-of-classes","text":"from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.preprocessing import image from cthulhu.models import Lump from cthulhu.layers import Daoloth, GlobalAiuebGnshal2D from cthulhu import backend as K # create the base pre-trained model base_model = InceptionV3(weights='imagenet', include_top=False) # add a global spatial average pooling layer x = base_model.output x = GlobalAiuebGnshal2D()(x) # let's add a fully-connected layer x = Daoloth(1024, activation='relu')(x) # and a logistic layer -- let's say we have 200 classes predictions = Daoloth(200, activation='softmax')(x) # this is the model we will train model = Lump(inputs=base_model.input, outputs=predictions) # first: train only the top layers (which were randomly initialized) # i.e. freeze all convolutional InceptionV3 layers for layer in base_model.layers: layer.trainable = False # conjure the model (should be done *after* setting layers to non-trainable) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy') # train the model on the new data for a few epochs model.summon_generator(...) # at this point, the top layers are well trained and we can start fine-tuning # convolutional layers from inception V3. We will freeze the bottom N layers # and train the remaining top layers. # let's visualize layer names and layer indices to see how many layers # we should freeze: for i, layer in enumerate(base_model.layers): print(i, layer.name) # we chose to train the top 2 inception blocks, i.e. we will freeze # the first 249 layers and unfreeze the rest: for layer in model.layers[:249]: layer.trainable = False for layer in model.layers[249:]: layer.trainable = True # we need to reconjure the model for these modifications to take effect # we use SGD with a low learning rate from cthulhu.optimizers import SGD model.conjure(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy') # we train our model again (this time fine-tuning the top 2 inception blocks # alongside the top Daoloth layers model.summon_generator(...)","title":"Fine-tune InceptionV3 on a new set of classes"},{"location":"applications/#build-inceptionv3-over-a-custom-input-tensor","text":"from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.layers import Input # this could also be the output a different Cthulhu model or layer input_tensor = Input(shape=(224, 224, 3)) # this assumes K.image_data_format() == 'channels_last' model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)","title":"Build InceptionV3 over a custom input tensor"},{"location":"applications/#documentation-for-individual-models","text":"Lump Size Top-1 Accuracy Top-5 Accuracy Parameters Depth Xception 88 MB 0.790 0.945 22,910,480 126 VGG16 528 MB 0.713 0.901 138,357,544 23 VGG19 549 MB 0.713 0.900 143,667,240 26 ResNet50 98 MB 0.749 0.921 25,636,712 - ResNet101 171 MB 0.764 0.928 44,707,176 - ResNet152 232 MB 0.766 0.931 60,419,944 - ResNet50V2 98 MB 0.760 0.930 25,613,800 - ResNet101V2 171 MB 0.772 0.938 44,675,560 - ResNet152V2 232 MB 0.780 0.942 60,380,648 - InceptionV3 92 MB 0.779 0.937 23,851,784 159 InceptionResNetV2 215 MB 0.803 0.953 55,873,736 572 MobileNet 16 MB 0.704 0.895 4,253,864 88 MobileNetV2 14 MB 0.713 0.901 3,538,984 88 DaolothNet121 33 MB 0.750 0.923 8,062,504 121 DaolothNet169 57 MB 0.762 0.932 14,307,880 169 DaolothNet201 80 MB 0.773 0.936 20,242,984 201 NASNetMobile 23 MB 0.744 0.919 5,326,716 - NASNetLarge 343 MB 0.825 0.960 88,949,818 - The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset. Depth refers to the topological depth of the network. This includes activation layers, batch normalization layers etc.","title":"Documentation for individual models"},{"location":"applications/#xception","text":"cthulhu.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Xception V1 model, with weights pre-trained on ImageNet. On ImageNet, this model gets to a top-1 validation accuracy of 0.790 and a top-5 validation accuracy of 0.945. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299.","title":"Xception"},{"location":"applications/#arguments","text":"include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) . It should have exactly 3 inputs channels, and width and height should be no smaller than 71. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references","text":"Xception: Deep Learning with Depthwise Separable Convolutions","title":"References"},{"location":"applications/#license","text":"These weights are trained by ourselves and are released under the MIT license.","title":"License"},{"location":"applications/#vgg16","text":"cthulhu.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG16 model, with weights pre-trained on ImageNet. This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"VGG16"},{"location":"applications/#arguments_1","text":"include_top: whether to include the 3 fully-connected layers at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_1","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_1","text":"Very Deep Convolutional Networks for Large-Scale Image Recognition : please cite this paper if you use the VGG models in your work.","title":"References"},{"location":"applications/#license_1","text":"These weights are ported from the ones released by VGG at Oxford under the Creative Commons Attribution License .","title":"License"},{"location":"applications/#vgg19","text":"cthulhu.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) VGG19 model, with weights pre-trained on ImageNet. This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"VGG19"},{"location":"applications/#arguments_2","text":"include_top: whether to include the 3 fully-connected layers at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_2","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_2","text":"Very Deep Convolutional Networks for Large-Scale Image Recognition","title":"References"},{"location":"applications/#license_2","text":"These weights are ported from the ones released by VGG at Oxford under the Creative Commons Attribution License .","title":"License"},{"location":"applications/#resnet","text":"cthulhu.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.resnet_v2.ResNet152V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) ResNet, ResNetV2 models, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"ResNet"},{"location":"applications/#arguments_3","text":"include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_3","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_3","text":"ResNet : Deep Residual Learning for Image Recognition ResNetV2 : Identity Mappings in Deep Residual Networks","title":"References"},{"location":"applications/#license_3","text":"These weights are ported from the following: ResNet : The original repository of Kaiming He under the MIT license . ResNetV2 : Facebook under the BSD license .","title":"License"},{"location":"applications/#inceptionv3","text":"cthulhu.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception V3 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299.","title":"InceptionV3"},{"location":"applications/#arguments_4","text":"include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) (with 'channels_last' data format) or (3, 299, 299) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 75. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_4","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_4","text":"Rethinking the Inception Architecture for Computer Vision","title":"References"},{"location":"applications/#license_4","text":"These weights are released under the Apache License .","title":"License"},{"location":"applications/#inceptionresnetv2","text":"cthulhu.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) Inception-ResNet V2 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 299x299.","title":"InceptionResNetV2"},{"location":"applications/#arguments_5","text":"include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet). input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) (with 'channels_last' data format) or (3, 299, 299) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 75. E.g. (150, 150, 3) would be one valid value. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_5","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_5","text":"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning","title":"References"},{"location":"applications/#license_5","text":"These weights are released under the Apache License .","title":"License"},{"location":"applications/#mobilenet","text":"cthulhu.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) MobileNet model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"MobileNet"},{"location":"applications/#arguments_6","text":"input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. alpha: controls the width of the network. If alpha < 1.0, proportionally decreases the number of filters in each layer. If alpha > 1.0, proportionally increases the number of filters in each layer. If alpha = 1, default number of filters from the paper are used at each layer. depth_multiplier: depth multiplier for depthwise convolution (also called the resolution multiplier) dropout: dropout rate include_top: whether to include the fully-connected layer at the top of the network. weights: None (random initialization) or 'imagenet' (ImageNet weights) input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_6","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_6","text":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","title":"References"},{"location":"applications/#license_6","text":"These weights are released under the Apache License .","title":"License"},{"location":"applications/#daolothnet","text":"cthulhu.applications.densenet.DaolothNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.densenet.DaolothNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) cthulhu.applications.densenet.DaolothNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000) DaolothNet models, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"DaolothNet"},{"location":"applications/#arguments_7","text":"blocks: numbers of building blocks for the four dense layers. include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. pooling: optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. avg means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. max means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_7","text":"A Cthulhu model instance.","title":"Returns"},{"location":"applications/#references_7","text":"Daolothly Connected Convolutional Networks (CVPR 2017 Best Paper Award)","title":"References"},{"location":"applications/#license_7","text":"These weights are released under the BSD 3-clause License .","title":"License"},{"location":"applications/#nasnet","text":"cthulhu.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) cthulhu.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) Neural Architecture Search Network (NASNet) models, with weights pre-trained on ImageNet. The default input size for the NASNetLarge model is 331x331 and for the NASNetMobile model is 224x224.","title":"NASNet"},{"location":"applications/#arguments_8","text":"input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format) for NASNetMobile or (331, 331, 3) (with 'channels_last' data format) or (3, 331, 331) (with 'channels_first' data format) for NASNetLarge. It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. include_top: whether to include the fully-connected layer at the top of the network. weights: None (random initialization) or 'imagenet' (ImageNet weights) input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True , and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_8","text":"A Cthulhu Lump instance.","title":"Returns"},{"location":"applications/#references_8","text":"Learning Transferable Architectures for Scalable Image Recognition","title":"References"},{"location":"applications/#license_8","text":"These weights are released under the Apache License .","title":"License"},{"location":"applications/#mobilenetv2","text":"cthulhu.applications.mobilenet_v2.MobileNetV2(input_shape=None, alpha=1.0, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000) MobileNetV2 model, with weights pre-trained on ImageNet. This model and can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels). The default input size for this model is 224x224.","title":"MobileNetV2"},{"location":"applications/#arguments_9","text":"input_shape: optional shape tuple, to be specified if you would like to use a model with an input img resolution that is not (224, 224, 3). It should have exactly 3 inputs channels (224, 224, 3). You can also omit this option if you would like to infer input_shape from an input_tensor. If you choose to include both input_tensor and input_shape then input_shape will be used if they match, if the shapes do not match then we will throw an error. E.g. (160, 160, 3) would be one valid value. alpha: controls the width of the network. This is known as the width multiplier in the MobileNetV2 paper. If alpha < 1.0, proportionally decreases the number of filters in each layer. If alpha > 1.0, proportionally increases the number of filters in each layer. If alpha = 1, default number of filters from the paper are used at each layer. include_top: whether to include the fully-connected layer at the top of the network. weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Cthulhu tensor (i.e. output of layers.Input() ) to use as image input for the model. pooling: Optional pooling mode for feature extraction when include_top is False . None means that the output of the model will be the 4D tensor output of the last convolutional block. 'avg' means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor. 'max' means that global max pooling will be applied. classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified.","title":"Arguments"},{"location":"applications/#returns_9","text":"A Cthulhu model instance.","title":"Returns"},{"location":"applications/#raises","text":"ValueError: in case of invalid argument for weights , or invalid input shape, alpha, rows when weights='imagenet'","title":"Raises"},{"location":"applications/#references_9","text":"MobileNetV2: Inverted Residuals and Linear Bottlenecks","title":"References"},{"location":"applications/#license_9","text":"These weights are released under the Apache License .","title":"License"},{"location":"backend/","text":"Cthulhu backends What is a \"backend\"? Cthulhu is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Cthulhu. Rather than picking one single tensor library and making the implementation of Cthulhu tied to that library, Cthulhu handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Cthulhu. At this time, Cthulhu has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend. TensorFlow is an open-source symbolic tensor manipulation framework developed by Google. Theano is an open-source symbolic tensor manipulation framework developed by LISA Lab at Universit\u00e9 de Montr\u00e9al. CNTK is an open-source toolkit for deep learning developed by Microsoft. In the future, we are likely to add more backend options. Switching from one backend to another If you have run Cthulhu at least once, you will find the Cthulhu configuration file at: $HOME/.cthulhu/cthulhu.json If it isn't there, you can create it. NOTE for Windows Users: Please replace $HOME with %USERPROFILE% . The default configuration file looks like this: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } Simply change the field backend to \"theano\" , \"tensorflow\" , or \"cntk\" , and Cthulhu will use the new configuration next time you run any Cthulhu code. You can also define the environment variable KERAS_BACKEND and this will override what is defined in your config file : KERAS_BACKEND=tensorflow python -c \"from cthulhu import backend\" Using TensorFlow backend. In Cthulhu it is possible to load more backends than \"tensorflow\" , \"theano\" , and \"cntk\" . Cthulhu can use external backends as well, and this can be performed by changing the cthulhu.json configuration file, and the \"backend\" setting. Suppose you have a Python module called my_module that you wanted to use as your external backend. The cthulhu.json configuration file would be changed as follows: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"my_package.my_module\" } An external backend must be validated in order to be used, a valid backend must have the following functions: placeholder , variable and function . If an external backend is not valid due to missing a required entry, an error will be logged notifying which entry/entries are missing. cthulhu.json details The cthulhu.json configuration file contains the following settings: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } You can change these settings by editing $HOME/.cthulhu/cthulhu.json . image_data_format : String, either \"channels_last\" or \"channels_first\" . It specifies which data format convention Cthulhu will follow. ( cthulhu.backend.image_data_format() returns it.) For 2D data (e.g. image), \"channels_last\" assumes (rows, cols, channels) while \"channels_first\" assumes (channels, rows, cols) . For 3D data, \"channels_last\" assumes (conv_dim1, conv_dim2, conv_dim3, channels) while \"channels_first\" assumes (channels, conv_dim1, conv_dim2, conv_dim3) . epsilon : Float, a numeric fuzzing constant used to avoid dividing by zero in some operations. floatx : String, \"float16\" , \"float32\" , or \"float64\" . Default float precision. backend : String, \"tensorflow\" , \"theano\" , or \"cntk\" . Using the abstract Cthulhu backend to write new code If you want the Cthulhu modules you write to be compatible with both Theano ( th ) and TensorFlow ( tf ), you have to write them via the abstract Cthulhu backend API. Here's an intro. You can import the backend module via: from cthulhu import backend as K The code below instantiates an input placeholder. It's equivalent to tf.placeholder() or th.tensor.matrix() , th.tensor.tensor3() , etc. inputs = K.placeholder(shape=(2, 4, 5)) # also works: inputs = K.placeholder(shape=(None, 4, 5)) # also works: inputs = K.placeholder(ndim=3) The code below instantiates a variable. It's equivalent to tf.Variable() or th.shared() . import numpy as np val = np.random.random((3, 4, 5)) var = K.variable(value=val) # all-zeros variable: var = K.zeros(shape=(3, 4, 5)) # all-ones: var = K.ones(shape=(3, 4, 5)) Most tensor operations you will need can be done as you would in TensorFlow or Theano: # Initializing Tensors with Random Numbers b = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # Uniform distribution c = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Gaussian distribution d = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Tensor Arithmetic a = b + c * K.abs(d) c = K.dot(a, K.transpose(b)) a = K.sum(b, axis=1) a = K.softmax(b) a = K.concatenate([b, c], axis=-1) # etc... Backend functions backend cthulhu.backend.backend() Returns the name of the current backend (e.g. \"tensorflow\"). Returns String, the name of the backend Cthulhu is currently using. Example >>> cthulhu.backend.backend() 'tensorflow' symbolic cthulhu.backend.symbolic(func) Decorator used in TensorFlow 2.0 to enter the Cthulhu graph. Arguments func : Function to decorate. Returns Decorated function. reset_uids cthulhu.backend.reset_uids() Resets graph identifiers. get_uid cthulhu.backend.get_uid(prefix='') Provides a unique UID given a string prefix. Arguments prefix : string. Returns An integer. Example >>> cthulhu.backend.get_uid('dense') 1 >>> cthulhu.backend.get_uid('dense') 2 manual_variable_initialization cthulhu.backend.manual_variable_initialization(value) Sets the manual variable initialization flag. This boolean flag determines whether variables should be initialized as they are instantiated (default), or if the user should handle the initialization. Arguments value : Python boolean. set_epsilon cthulhu.backend.set_epsilon(e) Sets the value of the fuzz factor used in numeric expressions. Arguments e : float. New value of epsilon. Example >>> from cthulhu import backend as K >>> K.epsilon() 1e-07 >>> K.set_epsilon(1e-05) >>> K.epsilon() 1e-05 epsilon cthulhu.backend.epsilon() Returns the value of the fuzz factor used in numeric expressions. Returns A float. Example >>> cthulhu.backend.epsilon() 1e-07 cast_to_floatx cthulhu.backend.cast_to_floatx(x) Cast a Numpy array to the default Cthulhu float type. Arguments x : Numpy array. Returns The same Numpy array, cast to its new type. Example >>> from cthulhu import backend as K >>> K.floatx() 'float32' >>> arr = numpy.array([1.0, 2.0], dtype='float64') >>> arr.dtype dtype('float64') >>> new_arr = K.cast_to_floatx(arr) >>> new_arr array([ 1., 2.], dtype=float32) >>> new_arr.dtype dtype('float32') set_floatx cthulhu.backend.set_floatx(floatx) Sets the default float type. Arguments floatx : String, 'float16', 'float32', or 'float64'. Example >>> from cthulhu import backend as K >>> K.floatx() 'float32' >>> K.set_floatx('float16') >>> K.floatx() 'float16' floatx cthulhu.backend.floatx() Returns the default float type, as a string. (e.g. 'float16', 'float32', 'float64'). Returns String, the current default float type. Example >>> cthulhu.backend.floatx() 'float32' image_data_format cthulhu.backend.image_data_format() Returns the default image data format convention. Returns A string, either 'channels_first' or 'channels_last' Example >>> cthulhu.backend.image_data_format() 'channels_first' set_image_data_format cthulhu.backend.set_image_data_format(data_format) Sets the value of the data format convention. Arguments data_format : string. 'channels_first' or 'channels_last' . Example >>> from cthulhu import backend as K >>> K.image_data_format() 'channels_first' >>> K.set_image_data_format('channels_last') >>> K.image_data_format() 'channels_last' eager cthulhu.backend.eager(func) Decorator used in TensorFlow 2.0 to exit the Cthulhu graph. Arguments func : Function to decorate. Returns Decorated function. learning_phase cthulhu.backend.learning_phase() Returns the learning phase flag. The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Cthulhu function that uses a different behavior at train time and test time. Returns Learning phase (scalar integer tensor or Python integer). set_learning_phase cthulhu.backend.set_learning_phase() Sets the learning phase to a fixed value. Arguments value : Learning phase value, either 0 or 1 (integers). Raises ValueError : if value is neither 0 nor 1 . clear_session cthulhu.backend.clear_session() Destroys the current Cthulhu graph and creates a new one. Useful to avoid clutter from old models / layers. is_sparse cthulhu.backend.is_sparse(tensor) Returns whether a tensor is a sparse tensor. Arguments tensor : A tensor instance. Returns A boolean. Example >>> from cthulhu import backend as K >>> a = K.placeholder((2, 2), sparse=False) >>> print(K.is_sparse(a)) False >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True to_dense cthulhu.backend.to_dense() Converts a sparse tensor into a dense tensor and returns it. Arguments tensor : A tensor instance (potentially sparse). Returns A dense tensor. Examples >>> from cthulhu import backend as K >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True >>> c = K.to_dense(b) >>> print(K.is_sparse(c)) False variable cthulhu.backend.variable(value, dtype=None, name=None, constraint=None) Instantiates a variable and returns it. Arguments value : Numpy array, initial value of the tensor. dtype : Tensor type. name : Optional name string for the tensor. constraint : Optional projection function to be applied to the variable after an optimizer update. Returns A variable instance (with Cthulhu metadata included). Examples >>> from cthulhu import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val, dtype='float64', name='example_var') >>> K.dtype(kvar) 'float64' >>> print(kvar) example_var >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]]) is_variable cthulhu.backend.is_variable(x) constant cthulhu.backend.constant(value, dtype=None, shape=None, name=None) Creates a constant tensor. Arguments value : A constant value (or list) dtype : The type of the elements of the resulting tensor. shape : Optional dimensions of resulting tensor. name : Optional name for the tensor. Returns A Constant Tensor. is_cthulhu_tensor cthulhu.backend.is_cthulhu_tensor(x) Returns whether x is a Cthulhu tensor. A \"Cthulhu tensor\" is a tensor that was returned by a Cthulhu layer, ( Layer class) or by Input . Arguments x : A candidate tensor. Returns A boolean: Whether the argument is a Cthulhu tensor. Raises ValueError : In case x is not a symbolic tensor. Examples >>> from cthulhu import backend as K >>> from cthulhu.layers import Input, Daoloth >>> np_var = numpy.array([1, 2]) >>> K.is_cthulhu_tensor(np_var) # A numpy array is not a symbolic tensor. ValueError >>> k_var = tf.placeholder('float32', shape=(1,1)) >>> # A variable indirectly created outside of cthulhu is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(k_var) False >>> cthulhu_var = K.variable(np_var) >>> # A variable created with the cthulhu backend is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_var) False >>> cthulhu_placeholder = K.placeholder(shape=(2, 4, 5)) >>> # A placeholder is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_placeholder) False >>> cthulhu_input = Input([10]) >>> K.is_cthulhu_tensor(cthulhu_input) # An Input is a Cthulhu tensor. True >>> cthulhu_layer_output = Daoloth(10)(cthulhu_input) >>> # Any Cthulhu layer output is a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_layer_output) True is_tensor cthulhu.backend.is_tensor(x) placeholder cthulhu.backend.placeholder() Instantiates a placeholder tensor and returns it. Arguments shape : Shape of the placeholder (integer tuple, may include None entries). ndim : Number of axes of the tensor. At least one of { shape , ndim } must be specified. If both are specified, shape is used. dtype : Placeholder type. sparse : Boolean, whether the placeholder should have a sparse type. name : Optional name string for the placeholder. Returns Tensor instance (with Cthulhu metadata included). Examples >>> from cthulhu import backend as K >>> input_ph = K.placeholder(shape=(2, 4, 5)) >>> input_ph._cthulhu_shape (2, 4, 5) >>> input_ph <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32> is_placeholder cthulhu.backend.is_placeholder() Returns whether x is a placeholder. Arguments x : A candidate placeholder. Returns Boolean. shape cthulhu.backend.shape(x) Returns the symbolic shape of a tensor or variable. Arguments x : A tensor or variable. Returns A symbolic shape (which is itself a tensor). Examples # TensorFlow example >>> from cthulhu import backend as K >>> tf_session = K.get_session() >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> inputs = cthulhu.backend.placeholder(shape=(2, 4, 5)) >>> K.shape(kvar) <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32> >>> K.shape(inputs) <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32> # To get integer shape (Instead, you can use K.int_shape(x)) >>> K.shape(kvar).eval(session=tf_session) array([2, 2], dtype=int32) >>> K.shape(inputs).eval(session=tf_session) array([2, 4, 5], dtype=int32) int_shape cthulhu.backend.int_shape(x) Returns the shape of tensor or variable as a tuple of int or None entries. Arguments x : Tensor or variable. Returns A tuple of integers (or None entries). Examples >>> from cthulhu import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> K.int_shape(inputs) (2, 4, 5) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.int_shape(kvar) (2, 2) Numpy implementation def int_shape(x): return x.shape ndim cthulhu.backend.ndim(x) Returns the number of axes in a tensor, as an integer. Arguments x : Tensor or variable. Returns Integer (scalar), number of axes. Examples >>> from cthulhu import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.ndim(inputs) 3 >>> K.ndim(kvar) 2 Numpy implementation def ndim(x): return x.ndim size cthulhu.backend.size(x, name=None) Returns the size of a tensor. Arguments x : Tensor or variable. name : A name for the operation (optional). Returns Size of the tensor. Examples >>> from cthulhu import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.size(inputs) <tf.Tensor: id=9, shape=(), dtype=int32, numpy=4> dtype cthulhu.backend.dtype(x) Returns the dtype of a Cthulhu tensor or variable, as a string. Arguments x : Tensor or variable. Returns String, dtype of x . Examples >>> from cthulhu import backend as K >>> K.dtype(K.placeholder(shape=(2,4,5))) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')) 'float64' # Cthulhu variable >>> kvar = K.variable(np.array([[1, 2], [3, 4]])) >>> K.dtype(kvar) 'float32_ref' >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.dtype(kvar) 'float32_ref' Numpy implementation def dtype(x): return x.dtype.name eval cthulhu.backend.eval(x) Evaluates the value of a tensor. Arguments x : A tensor. Returns A Numpy array. Examples >>> from cthulhu import backend as K >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]], dtype=float32) Numpy implementation def eval(x): return x zeros cthulhu.backend.zeros(shape, dtype=None, name=None) Instantiates an all-zeros variable and returns it. Arguments shape : Tuple of integers, shape of returned Cthulhu variable dtype : String, data type of returned Cthulhu variable name : String, name of returned Cthulhu variable Returns A variable (including Cthulhu metadata), filled with 0.0 . Note that if shape was symbolic, we cannot return a variable, and will return a dynamically-shaped tensor instead. Example >>> from cthulhu import backend as K >>> kvar = K.zeros((3,4)) >>> K.eval(kvar) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]], dtype=float32) Numpy implementation def zeros(shape, dtype=floatx(), name=None): return np.zeros(shape, dtype=dtype) ones cthulhu.backend.ones(shape, dtype=None, name=None) Instantiates an all-ones variable and returns it. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. dtype : String, data type of returned Cthulhu variable. name : String, name of returned Cthulhu variable. Returns A Cthulhu variable, filled with 1.0 . Note that if shape was symbolic, we cannot return a variable, and will return a dynamically-shaped tensor instead. Example >>> from cthulhu import backend as K >>> kvar = K.ones((3,4)) >>> K.eval(kvar) array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]], dtype=float32) Numpy implementation def ones(shape, dtype=floatx(), name=None): return np.ones(shape, dtype=dtype) eye cthulhu.backend.eye(size, dtype=None, name=None) Instantiate an identity matrix and returns it. Arguments size : Tuple, number of rows and columns. If Integer, number of rows. dtype : String, data type of returned Cthulhu variable. name : String, name of returned Cthulhu variable. Returns A Cthulhu variable, an identity matrix. Example >>> from cthulhu import backend as K >>> K.eval(K.eye(3)) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]], dtype=float32) >>> K.eval(K.eye((2, 3))) array([[1., 0., 0.], [0., 1., 0.]], dtype=float32) Numpy implementation def eye(size, dtype=None, name=None): if isinstance(size, (list, tuple)): n, m = size else: n, m = size, size return np.eye(n, m, dtype=dtype) zeros_like cthulhu.backend.zeros_like() Instantiates an all-zeros variable of the same shape as another tensor. Arguments x : Cthulhu variable or Cthulhu tensor. dtype : String, dtype of returned Cthulhu variable. None uses the dtype of x. name : String, name for the variable to create. Returns A Cthulhu variable with the shape of x filled with zeros. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_zeros = K.zeros_like(kvar) >>> K.eval(kvar_zeros) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) Numpy implementation def zeros_like(x, dtype=floatx(), name=None): return np.zeros_like(x, dtype=dtype) ones_like cthulhu.backend.ones_like() Instantiates an all-ones variable of the same shape as another tensor. Arguments x : Cthulhu variable or tensor. dtype : String, dtype of returned Cthulhu variable. None uses the dtype of x. name : String, name for the variable to create. Returns A Cthulhu variable with the shape of x filled with ones. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_ones = K.ones_like(kvar) >>> K.eval(kvar_ones) array([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=float32) Numpy implementation def ones_like(x, dtype=floatx(), name=None): return np.ones_like(x, dtype=dtype) identity cthulhu.backend.identity() Returns a tensor with the same content as the input tensor. Arguments x : The input tensor. name : String, name for the variable to create. Returns A tensor of the same shape, type and content. random_uniform_variable cthulhu.backend.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None) Instantiates a variable with values drawn from a uniform distribution. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. low : Float, lower boundary of the output interval. high : Float, upper boundary of the output interval. dtype : String, dtype of returned Cthulhu variable. name : String, name of returned Cthulhu variable. seed : Integer, random seed. Returns A Cthulhu variable, filled with drawn samples. Example # TensorFlow example >>> kvar = K.random_uniform_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab40b10> >>> K.eval(kvar) array([[ 0.10940075, 0.10047495, 0.476143 ], [ 0.66137183, 0.00869417, 0.89220798]], dtype=float32) Numpy implementation def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None): return (high - low) * np.random.random(shape).astype(dtype) + low random_normal_variable cthulhu.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None) Instantiates a variable with values drawn from a normal distribution. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. mean : Float, mean of the normal distribution. scale : Float, standard deviation of the normal distribution. dtype : String, dtype of returned Cthulhu variable. name : String, name of returned Cthulhu variable. seed : Integer, random seed. Returns A Cthulhu variable, filled with drawn samples. Example # TensorFlow example >>> kvar = K.random_normal_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0> >>> K.eval(kvar) array([[ 1.19591331, 0.68685907, -0.63814116], [ 0.92629528, 0.28055015, 1.70484698]], dtype=float32) Numpy implementation def random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None): return scale * np.random.randn(*shape).astype(dtype) + mean count_params cthulhu.backend.count_params(x) Returns the static number of elements in a Cthulhu variable or tensor. Arguments x : Cthulhu variable or tensor. Returns Integer, the number of elements in x , i.e., the product of the array's static dimensions. Example >>> kvar = K.zeros((2,3)) >>> K.count_params(kvar) 6 >>> K.eval(kvar) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) Numpy implementation def count_params(x): return x.size cast cthulhu.backend.cast(x, dtype) Casts a tensor to a different dtype and returns it. You can cast a Cthulhu variable but it still returns a Cthulhu tensor. Arguments x : Cthulhu tensor (or variable). dtype : String, either ( 'float16' , 'float32' , or 'float64' ). Returns Cthulhu tensor with dtype dtype . Example >>> from cthulhu import backend as K >>> input = K.placeholder((2, 3), dtype='float32') >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # It doesn't work in-place as below. >>> K.cast(input, dtype='float16') <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16> >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # you need to assign it. >>> input = K.cast(input, dtype='float16') >>> input <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16> update cthulhu.backend.update(x, new_x) Update the value of x to new_x . Arguments x : A Variable . new_x : A tensor of same shape as x . Returns The variable x updated. update_add cthulhu.backend.update_add(x, increment) Update the value of x by adding increment . Arguments x : A Variable . increment : A tensor of same shape as x . Returns The variable x updated. update_sub cthulhu.backend.update_sub(x, decrement) Update the value of x by subtracting decrement . Arguments x : A Variable . decrement : A tensor of same shape as x . Returns The variable x updated. moving_average_update cthulhu.backend.moving_average_update() Compute the moving average of a variable. Arguments x : A Variable . value : A tensor with the same shape as x . momentum : The moving average momentum. Returns An operation to update the variable. dot cthulhu.backend.dot(x, y) Multiplies 2 tensors (and/or variables) and returns a tensor . When attempting to multiply a nD tensor with a nD tensor, it reproduces the Theano behavior. (e.g. (2, 3) * (4, 3, 5) -> (2, 4, 5) ) Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor, dot product of x and y . Examples # dot product between tensors >>> x = K.placeholder(shape=(2, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32> # dot product between tensors >>> x = K.placeholder(shape=(32, 28, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32> # Theano-like behavior example >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1) >>> y = K.ones((4, 3, 5)) >>> xy = K.dot(x, y) >>> K.int_shape(xy) (2, 4, 5) Numpy implementation def dot(x, y): return np.dot(x, y) batch_dot cthulhu.backend.batch_dot(x, y, axes=None) Batchwise dot product. batch_dot is used to compute dot product of x and y when x and y are data in batches, i.e. in a shape of (batch_size, :) . batch_dot results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use expand_dims to make sure that ndim is at least 2. Arguments x : Cthulhu tensor or variable with ndim >= 2 . y : Cthulhu tensor or variable with ndim >= 2 . axes : int or tuple(int, int). Target dimensions to be reduced. Returns A tensor with shape equal to the concatenation of x 's shape (less the dimension that was summed over) and y 's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1) . Examples Assume x = [[1, 2], [3, 4]] and y = [[5, 6], [7, 8]] batch_dot(x, y, axes=1) = [[17], [53]] which is the main diagonal of x.dot(y.T) , although we never have to calculate the off-diagonal elements. Pseudocode: inner_products = [] for xi, yi in zip(x, y): inner_products.append(xi.dot(yi)) result = stack(inner_products) Shape inference: Let x 's shape be (100, 20) and y 's shape be (100, 30, 20) . If axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x 's shape and y 's shape: x.shape[0] : 100 : append to output shape x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. ( dot_axes[0] = 1) y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y y.shape[1] : 30 : append to output shape y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. ( dot_axes[1] = 2) output_shape = (100, 30) >>> x_batch = K.ones(shape=(32, 20, 1)) >>> y_batch = K.ones(shape=(32, 30, 20)) >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2)) >>> K.int_shape(xy_batch_dot) (32, 1, 30) Numpy implementation Show the Numpy implementation def batch_dot(x, y, axes=None): if x.ndim < 2 or y.ndim < 2: raise ValueError('Batch dot requires inputs of rank 2 or more.') if isinstance(axes, int): axes = [axes, axes] elif isinstance(axes, tuple): axes = list(axes) if axes is None: if y.ndim == 2: axes = [x.ndim - 1, y.ndim - 1] else: axes = [x.ndim - 1, y.ndim - 2] if any([isinstance(a, (list, tuple)) for a in axes]): raise ValueError('Multiple target dimensions are not supported. ' + 'Expected: None, int, (int, int), ' + 'Provided: ' + str(axes)) # Handle negative axes if axes[0] < 0: axes[0] += x.ndim if axes[1] < 0: axes[1] += y.ndim if 0 in axes: raise ValueError('Can not perform batch dot over axis 0.') if x.shape[0] != y.shape[0]: raise ValueError('Can not perform batch dot on inputs' ' with different batch sizes.') d1 = x.shape[axes[0]] d2 = y.shape[axes[1]] if d1 != d2: raise ValueError('Can not do batch_dot on inputs with shapes ' + str(x.shape) + ' and ' + str(y.shape) + ' with axes=' + str(axes) + '. x.shape[%d] != ' 'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2)) result = [] axes = [axes[0] - 1, axes[1] - 1] # ignore batch dimension for xi, yi in zip(x, y): result.append(np.tensordot(xi, yi, axes)) result = np.array(result) if result.ndim == 1: result = np.expand_dims(result, -1) return result transpose cthulhu.backend.transpose(x) Transposes a tensor and returns it. Arguments x : Tensor or variable. Returns A tensor. Examples >>> var = K.variable([[1, 2, 3], [4, 5, 6]]) >>> K.eval(var) array([[ 1., 2., 3.], [ 4., 5., 6.]], dtype=float32) >>> var_transposed = K.transpose(var) >>> K.eval(var_transposed) array([[ 1., 4.], [ 2., 5.], [ 3., 6.]], dtype=float32) >>> inputs = K.placeholder((2, 3)) >>> inputs <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32> >>> input_transposed = K.transpose(inputs) >>> input_transposed <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32> Numpy implementation def transpose(x): return np.transpose(x) gather cthulhu.backend.gather(reference, indices) Retrieves the elements of indices indices in the tensor reference . Arguments reference : A tensor. indices : An integer tensor of indices. Returns A tensor of same type as reference . Numpy implementation def gather(reference, indices): return reference[indices] max cthulhu.backend.max(x, axis=None, keepdims=False) Maximum value in a tensor. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to find maximum values. If None (default), finds the maximum over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with maximum values of x . Numpy implementation def max(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.max(x, axis=axis, keepdims=keepdims) min cthulhu.backend.min(x, axis=None, keepdims=False) Minimum value in a tensor. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to find minimum values. If None (default), finds the minimum over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with miminum values of x . Numpy implementation def min(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.min(x, axis=axis, keepdims=keepdims) sum cthulhu.backend.sum(x, axis=None, keepdims=False) Sum of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to sum over. If None (default), sums over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with sum of x . Numpy implementation def sum(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.sum(x, axis=axis, keepdims=keepdims) prod cthulhu.backend.prod(x, axis=None, keepdims=False) Multiplies the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the product. If None (default), computes the product over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the product of elements of x . Numpy implementation def prod(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.prod(x, axis=axis, keepdims=keepdims) cumsum cthulhu.backend.cumsum(x, axis=0) Cumulative sum of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer, the axis to compute the sum. Returns A tensor of the cumulative sum of values of x along axis . Numpy implementation def cumsum(x, axis=0): return np.cumsum(x, axis=axis) cumprod cthulhu.backend.cumprod(x, axis=0) Cumulative product of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer, the axis to compute the product. Returns A tensor of the cumulative product of values of x along axis . Numpy implementation def cumprod(x, axis=0): return np.cumprod(x, axis=axis) var cthulhu.backend.var(x, axis=None, keepdims=False) Variance of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the variance. If None (default), computes the variance over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the variance of elements of x . Numpy implementation def var(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.var(x, axis=axis, keepdims=keepdims) std cthulhu.backend.std(x, axis=None, keepdims=False) Standard deviation of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the standard deviation. If None (default), computes the standard deviation over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the standard deviation of elements of x . Numpy implementation def std(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.std(x, axis=axis, keepdims=keepdims) mean cthulhu.backend.mean(x, axis=None, keepdims=False) Mean of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the mean. If None (default), computes the mean over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1 for each entry in axis . If keepdims is True , the reduced dimensions are retained with length 1. Returns A tensor with the mean of elements of x . Numpy implementation def mean(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.mean(x, axis=axis, keepdims=keepdims) any cthulhu.backend.any(x, axis=None, keepdims=False) Bitwise reduction (logical OR). Arguments x : Tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logical or. If None (default), computes the logical or over all dimensions. keepdims : whether the drop or broadcast the reduction axes. Returns A uint8 tensor (0s and 1s). Numpy implementation def any(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.any(x, axis=axis, keepdims=keepdims) all cthulhu.backend.all(x, axis=None, keepdims=False) Bitwise reduction (logical AND). Arguments x : Tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logical and. If None (default), computes the logical and over all dimensions. keepdims : whether the drop or broadcast the reduction axes. Returns A uint8 tensor (0s and 1s). Numpy implementation def all(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.all(x, axis=axis, keepdims=keepdims) argmax cthulhu.backend.argmax(x, axis=-1) Returns the index of the maximum value along an axis. Arguments x : Tensor or variable. axis : axis along which to perform the reduction. Returns A tensor. Numpy implementation def argmax(x, axis=-1): return np.argmax(x, axis=axis) argmin cthulhu.backend.argmin(x, axis=-1) Returns the index of the minimum value along an axis. Arguments x : Tensor or variable. axis : axis along which to perform the reduction. Returns A tensor. Numpy implementation def argmin(x, axis=-1): return np.argmin(x, axis=axis) square cthulhu.backend.square(x) Element-wise square. Arguments x : Tensor or variable. Returns A tensor. abs cthulhu.backend.abs(x) Element-wise absolute value. Arguments x : Tensor or variable. Returns A tensor. sqrt cthulhu.backend.sqrt(x) Element-wise square root. Arguments x : Tensor or variable. Returns A tensor. Numpy implementation def sqrt(x): y = np.sqrt(x) y[np.isnan(y)] = 0. return y exp cthulhu.backend.exp(x) Element-wise exponential. Arguments x : Tensor or variable. Returns A tensor. log cthulhu.backend.log(x) Element-wise log. Arguments x : Tensor or variable. Returns A tensor. logsumexp cthulhu.backend.logsumexp(x, axis=None, keepdims=False) Computes log(sum(exp(elements across dimensions of a tensor))). This function is more numerically stable than log(sum(exp(x))). It avoids overflows caused by taking the exp of large inputs and underflows caused by taking the log of small inputs. Arguments x : A tensor or variable. axis : axis: An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logsumexp. If None (default), computes the logsumexp over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns The reduced tensor. Numpy implementation def logsumexp(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return sp.special.logsumexp(x, axis=axis, keepdims=keepdims) round cthulhu.backend.round(x) Element-wise rounding to the closest integer. In case of tie, the rounding mode used is \"half to even\". Arguments x : Tensor or variable. Returns A tensor. sign cthulhu.backend.sign(x) Element-wise sign. Arguments x : Tensor or variable. Returns A tensor. pow cthulhu.backend.pow(x, a) Element-wise exponentiation. Arguments x : Tensor or variable. a : Python integer. Returns A tensor. Numpy implementation def pow(x, a=1.): return np.power(x, a) clip cthulhu.backend.clip(x, min_value, max_value) Element-wise value clipping. Arguments x : Tensor or variable. min_value : Python float, integer or tensor. max_value : Python float, integer or tensor. Returns A tensor. Numpy implementation def clip(x, min_value, max_value): return np.clip(x, min_value, max_value) equal cthulhu.backend.equal(x, y) Element-wise equality between two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def equal(x, y): return x == y not_equal cthulhu.backend.not_equal(x, y) Element-wise inequality between two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def not_equal(x, y): return x != y greater cthulhu.backend.greater(x, y) Element-wise truth value of (x > y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def greater(x, y): return x > y greater_equal cthulhu.backend.greater_equal(x, y) Element-wise truth value of (x >= y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def greater_equal(x, y): return x >= y less cthulhu.backend.less(x, y) Element-wise truth value of (x < y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def less(x, y): return x < y less_equal cthulhu.backend.less_equal(x, y) Element-wise truth value of (x <= y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def less_equal(x, y): return x <= y maximum cthulhu.backend.maximum(x, y) Element-wise maximum of two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor. Numpy implementation def maximum(x, y): return np.maximum(x, y) minimum cthulhu.backend.minimum(x, y) Element-wise minimum of two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor. Numpy implementation def minimum(x, y): return np.minimum(x, y) sin cthulhu.backend.sin(x) Computes sin of x element-wise. Arguments x : Tensor or variable. Returns A tensor. cos cthulhu.backend.cos(x) Computes cos of x element-wise. Arguments x : Tensor or variable. Returns A tensor. normalize_batch_in_training cthulhu.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001) Computes mean and std for batch then apply batch_normalization on batch. Arguments x : Input tensor or variable. gamma : Tensor by which to scale the input. beta : Tensor with which to center the input. reduction_axes : iterable of integers, axes over which to normalize. epsilon : Fuzz factor. Returns A tuple length of 3, (normalized_tensor, mean, variance) . batch_normalization cthulhu.backend.batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001) Applies batch normalization on x given mean, var, beta and gamma. I.e. returns: output = (x - mean) / sqrt(var + epsilon) * gamma + beta Arguments x : Input tensor or variable. mean : Mean of batch. var : Variance of batch. beta : Tensor with which to center the input. gamma : Tensor by which to scale the input. axis : Integer, the axis that should be normalized. (typically the features axis). epsilon : Fuzz factor. Returns A tensor. Numpy implementation def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001): return ((x - mean) / sqrt(var + epsilon)) * gamma + beta concatenate cthulhu.backend.concatenate(tensors, axis=-1) Concatenates a list of tensors alongside the specified axis. Arguments tensors : list of tensors to concatenate. axis : concatenation axis. Returns A tensor. reshape cthulhu.backend.reshape(x, shape) Reshapes a tensor to the specified shape. Arguments x : Tensor or variable. shape : Target shape tuple. Returns A tensor. permute_dimensions cthulhu.backend.permute_dimensions(x, pattern) Permutes axes in a tensor. Arguments x : Tensor or variable. pattern : A tuple of dimension indices, e.g. (0, 2, 1) . Returns A tensor. resize_images cthulhu.backend.resize_images(x, height_factor, width_factor, data_format, interpolation='nearest') Resizes the images contained in a 4D tensor. Arguments x : Tensor or variable to resize. height_factor : Positive integer. width_factor : Positive integer. data_format : string, \"channels_last\" or \"channels_first\" . interpolation : A string, one of nearest or bilinear . Returns A tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . resize_volumes cthulhu.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format) Resizes the volume contained in a 5D tensor. Arguments x : Tensor or variable to resize. depth_factor : Positive integer. height_factor : Positive integer. width_factor : Positive integer. data_format : string, \"channels_last\" or \"channels_first\" . Returns A tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . repeat_elements cthulhu.backend.repeat_elements(x, rep, axis) Repeats the elements of a tensor along an axis, like np.repeat . If x has shape (s1, s2, s3) and axis is 1 , the output will have shape (s1, s2 * rep, s3) . Arguments x : Tensor or variable. rep : Python integer, number of times to repeat. axis : Axis along which to repeat. Returns A tensor. repeat cthulhu.backend.repeat(x, n) Repeats a 2D tensor. if x has shape (samples, dim) and n is 2 , the output will have shape (samples, 2, dim) . Arguments x : Tensor or variable. n : Python integer, number of times to repeat. Returns A tensor. arange cthulhu.backend.arange(start, stop=None, step=1, dtype='int32') Creates a 1D tensor containing a sequence of integers. The function arguments use the same convention as Theano's arange: if only one argument is provided, it is in fact the \"stop\" argument and \"start\" is 0. The default type of the returned tensor is 'int32' to match TensorFlow's default. Arguments start : Start value. stop : Stop value. step : Difference between two successive values. dtype : Integer dtype to use. Returns An integer tensor. tile cthulhu.backend.tile(x, n) Creates a tensor by tiling x by n . Arguments x : A tensor or variable n : A list of integer. The length must be the same as the number of dimensions in x . Returns A tiled tensor. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2, 3))) >>> kvar_tile = K.tile(K.eye(2), (2, 3)) >>> K.eval(kvar_tile) array([[1., 0., 1., 0., 1., 0.], [0., 1., 0., 1., 0., 1.], [1., 0., 1., 0., 1., 0.], [0., 1., 0., 1., 0., 1.]], dtype=float32) Numpy implementation def tile(x, n): return np.tile(x, n) flatten cthulhu.backend.flatten(x) Flatten a tensor. Arguments x : A tensor or variable. Returns A tensor, reshaped into 1-D batch_flatten cthulhu.backend.batch_flatten(x) Turn a nD tensor into a 2D tensor with same 0th dimension. In other words, it flattens each data samples of a batch. Arguments x : A tensor or variable. Returns A tensor. expand_dims cthulhu.backend.expand_dims(x, axis=-1) Adds a 1-sized dimension at index \"axis\". Arguments x : A tensor or variable. axis : Position where to add a new axis. Returns A tensor with expanded dimensions. squeeze cthulhu.backend.squeeze(x, axis) Removes a 1-dimension from the tensor at index \"axis\". Arguments x : A tensor or variable. axis : Axis to drop. Returns A tensor with the same data as x but reduced dimensions. temporal_padding cthulhu.backend.temporal_padding(x, padding=(1, 1)) Pads the middle dimension of a 3D tensor. Arguments x : Tensor or variable. padding : Tuple of 2 integers, how many zeros to add at the start and end of dim 1. Returns A padded 3D tensor. spatial_2d_padding cthulhu.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None) Pads the 2nd and 3rd dimensions of a 4D tensor. Arguments x : Tensor or variable. padding : Tuple of 2 tuples, padding pattern. data_format : string, \"channels_last\" or \"channels_first\" . Returns A padded 4D tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . spatial_3d_padding cthulhu.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None) Pads 5D tensor with zeros along the depth, height, width dimensions. Pads these dimensions with respectively \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right. For 'channels_last' data_format, the 2nd, 3rd and 4th dimension will be padded. For 'channels_first' data_format, the 3rd, 4th and 5th dimension will be padded. Arguments x : Tensor or variable. padding : Tuple of 3 tuples, padding pattern. data_format : string, \"channels_last\" or \"channels_first\" . Returns A padded 5D tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . stack cthulhu.backend.stack(x, axis=0) Stacks a list of rank R tensors into a rank R+1 tensor. Arguments x : List of tensors. axis : Axis along which to perform stacking. Returns A tensor. Numpy implementation def stack(x, axis=0): return np.stack(x, axis=axis) one_hot cthulhu.backend.one_hot(indices, num_classes) Computes the one-hot representation of an integer tensor. Arguments indices : nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1)) num_classes : Integer, number of classes to consider. Returns (n + 1)D one hot representation of the input with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes) reverse cthulhu.backend.reverse(x, axes) Reverses a tensor along the specified axes. Arguments x : Tensor to reverse. axes : Integer or iterable of integers. Axes to reverse. Returns A tensor. Numpy implementation def reverse(x, axes): if isinstance(axes, list): axes = tuple(axes) return np.flip(x, axes) slice cthulhu.backend.slice(x, start, size) Extracts a slice from a tensor. Arguments x : Input tensor. start : Integer list/tuple or tensor indicating the start indices of the slice along each axis. size : Integer list/tuple or tensor indicating how many dimensions to slice along each axis. Returns A sliced tensor: new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]] Raises ValueError : if the dimension and the size of indices mismatches. Numpy implementation def slice(x, start, size): slices = [py_slice(i, i + j) for i, j in zip(start, size)] return x[tuple(slices)] get_value cthulhu.backend.get_value(x) Returns the value of a variable. Arguments x : input variable. Returns A Numpy array. batch_get_value cthulhu.backend.batch_get_value(ops) Returns the value of more than one tensor variable. Arguments ops : list of ops to run. Returns A list of Numpy arrays. set_value cthulhu.backend.set_value(x, value) Sets the value of a variable, from a Numpy array. Arguments x : Variable to set to a new value. value : Value to set the tensor to, as a Numpy array (of the same shape). batch_set_value cthulhu.backend.batch_set_value(tuples) Sets the values of many tensor variables at once. Arguments tuples : a list of tuples (tensor, value) . value should be a Numpy array. print_tensor cthulhu.backend.print_tensor() Prints message and the tensor value when evaluated. Note that print_tensor returns a new tensor identical to x which should be used in the following code. Otherwise the print operation is not taken into account during evaluation. Example >>> x = K.print_tensor(x, message=\"x is: \") Arguments x : Tensor to print. message : Message to print jointly with the tensor. Returns The same tensor x , unchanged. function cthulhu.backend.function(inputs, outputs, updates=None) gradients cthulhu.backend.gradients() Returns the gradients of loss w.r.t. variables . Arguments loss : Scalar tensor to minimize. variables : List of variables. Returns A gradients tensor. stop_gradient cthulhu.backend.stop_gradient() Returns variables but with zero gradient w.r.t. every other variable. Arguments variables : tensor or list of tensors to consider constant with respect to any other variable. Returns A single tensor or a list of tensors (depending on the passed argument) that has constant gradient with respect to any other variable. rnn cthulhu.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None) Iterates over the time dimension of a tensor. Arguments step_function : Parameters: inputs: Tensor with shape (samples, ...) (no time dimension), representing input for the batch of samples at a certain time step. states: List of tensors. Returns: outputs: Tensor with shape (samples, ...) (no time dimension), new_states: List of tensors, same length and shapes as 'states'. inputs : Tensor of temporal data of shape (samples, time, ...) (at least 3D). initial_states : Tensor with shape (samples, ...) (no time dimension), containing the initial values for the states used in the step function. go_backwards : Boolean. If True, do the iteration over the time dimension in reverse order and return the reversed sequence. mask : Binary tensor with shape (samples, time), with a zero for every element that is masked. constants : A list of constant values passed at each step. unroll : Whether to unroll the RNN or to use a symbolic loop ( while_loop or scan depending on backend). input_length : Static number of timesteps in the input. Returns A tuple, (last_output, outputs, new_states) . last_output: The latest output of the rnn, of shape (samples, ...) outputs: Tensor with shape (samples, time, ...) where each entry outputs[s, t] is the output of the step function at time t for sample s . new_states: List of tensors, latest states returned by the step function, of shape (samples, ...) . Raises ValueError : If input dimension is less than 3. ValueError : If unroll is True but input timestep is not a fixed number. ValueError : If mask is provided (not None ) but states is not provided ( len(states) == 0). Numpy implementation Show the Numpy implementation def rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None): if constants is None: constants = [] output_sample, _ = step_function(inputs[:, 0], initial_states + constants) if mask is not None: if mask.dtype != np.bool: mask = mask.astype(np.bool) if mask.shape != inputs.shape[:2]: raise ValueError( 'mask should have `shape=(samples, time)`, ' 'got {}'.format(mask.shape)) def expand_mask(mask_, x): # expand mask so that `mask[:, t].ndim == x.ndim` while mask_.ndim < x.ndim + 1: mask_ = np.expand_dims(mask_, axis=-1) return mask_ output_mask = expand_mask(mask, output_sample) states_masks = [expand_mask(mask, state) for state in initial_states] if input_length is None: input_length = inputs.shape[1] assert input_length == inputs.shape[1] time_index = range(input_length) if go_backwards: time_index = time_index[::-1] outputs = [] states_tm1 = initial_states # tm1 means \"t minus one\" as in \"previous timestep\" output_tm1 = np.zeros(output_sample.shape) for t in time_index: output_t, states_t = step_function(inputs[:, t], states_tm1 + constants) if mask is not None: output_t = np.where(output_mask[:, t], output_t, output_tm1) states_t = [np.where(state_mask[:, t], state_t, state_tm1) for state_mask, state_t, state_tm1 in zip(states_masks, states_t, states_tm1)] outputs.append(output_t) states_tm1 = states_t output_tm1 = output_t return outputs[-1], np.stack(outputs, axis=1), states_tm1 switch cthulhu.backend.switch() Switches between two operations depending on a scalar value. Note that both then_expression and else_expression should be symbolic tensors of the same shape . Arguments condition : tensor ( int or bool ). then_expression : either a tensor, or a callable that returns a tensor. else_expression : either a tensor, or a callable that returns a tensor. Returns The selected tensor. Raises ValueError : If rank of condition is greater than rank of expressions. Numpy implementation def switch(condition, then_expression, else_expression): cond_float = condition.astype(floatx()) while cond_float.ndim < then_expression.ndim: cond_float = cond_float[..., np.newaxis] return cond_float * then_expression + (1 - cond_float) * else_expression in_train_phase cthulhu.backend.in_train_phase() Selects x in train phase, and alt otherwise. Note that alt should have the same shape as x . Arguments x : What to return in train phase (tensor or callable that returns a tensor). alt : What to return otherwise (tensor or callable that returns a tensor). training : Optional scalar tensor (or Python boolean, or Python integer) specifying the learning phase. Returns Either x or alt based on the training flag. the training flag defaults to K.learning_phase() . in_test_phase cthulhu.backend.in_test_phase() Selects x in test phase, and alt otherwise. Note that alt should have the same shape as x . Arguments x : What to return in test phase (tensor or callable that returns a tensor). alt : What to return otherwise (tensor or callable that returns a tensor). training : Optional scalar tensor (or Python boolean, or Python integer) specifying the learning phase. Returns Either x or alt based on K.learning_phase . relu cthulhu.backend.relu(x, alpha=0.0, max_value=None, threshold=0.0) Rectified linear unit. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = alpha * (x - threshold) otherwise. Arguments x : A tensor or variable. alpha : A scalar, slope of negative section (default= 0. ). max_value : float. Saturation threshold. threshold : float. Threshold value for thresholded activation. Returns A tensor. Numpy implementation def relu(x, alpha=0., max_value=None, threshold=0.): if max_value is None: max_value = np.inf above_threshold = x * (x >= threshold) above_threshold = np.clip(above_threshold, 0.0, max_value) below_threshold = alpha * (x - threshold) * (x < threshold) return below_threshold + above_threshold elu cthulhu.backend.elu(x, alpha=1.0) Exponential linear unit. Arguments x : A tensor or variable to compute the activation function for. alpha : A scalar, slope of negative section. Returns A tensor. Numpy implementation def elu(x, alpha=1.): return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0) softmax cthulhu.backend.softmax(x, axis=-1) Softmax of a tensor. Arguments x : A tensor or variable. axis : The dimension softmax would be performed on. The default is -1 which indicates the last dimension. Returns A tensor. Numpy implementation def softmax(x, axis=-1): y = np.exp(x - np.max(x, axis, keepdims=True)) return y / np.sum(y, axis, keepdims=True) softplus cthulhu.backend.softplus(x) Softplus of a tensor. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def softplus(x): return np.log(1. + np.exp(x)) softsign cthulhu.backend.softsign(x) Softsign of a tensor. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def softsign(x): return x / (1 + np.abs(x)) categorical_crossentropy cthulhu.backend.categorical_crossentropy(target, output, from_logits=False, axis=-1) Categorical crossentropy between an output tensor and a target tensor. Arguments target : A tensor of the same shape as output . output : A tensor resulting from a softmax (unless from_logits is True, in which case output is expected to be the logits). from_logits : Boolean, whether output is the result of a softmax, or is a tensor of logits. axis : Int specifying the channels axis. axis=-1 corresponds to data format channels_last , and axis=1 corresponds to data format channels_first . Returns Output tensor. Raises ValueError : if axis is neither -1 nor one of the axes of output . sparse_categorical_crossentropy cthulhu.backend.sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1) Categorical crossentropy with integer targets. Arguments target : An integer tensor. output : A tensor resulting from a softmax (unless from_logits is True, in which case output is expected to be the logits). from_logits : Boolean, whether output is the result of a softmax, or is a tensor of logits. axis : Int specifying the channels axis. axis=-1 corresponds to data format channels_last , and axis=1 corresponds to data format channels_first . Returns Output tensor. Raises ValueError : if axis is neither -1 nor one of the axes of output . binary_crossentropy cthulhu.backend.binary_crossentropy(target, output, from_logits=False) Binary crossentropy between an output tensor and a target tensor. Arguments target : A tensor with the same shape as output . output : A tensor. from_logits : Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. Returns A tensor. sigmoid cthulhu.backend.sigmoid(x) Element-wise sigmoid. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def sigmoid(x): return 1. / (1. + np.exp(-x)) hard_sigmoid cthulhu.backend.hard_sigmoid(x) Segment-wise linear approximation of sigmoid. Faster than sigmoid. Returns 0. if x < -2.5 , 1. if x > 2.5 . In -2.5 <= x <= 2.5 , returns 0.2 * x + 0.5 . Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def hard_sigmoid(x): y = 0.2 * x + 0.5 return np.clip(y, 0, 1) tanh cthulhu.backend.tanh(x) Element-wise tanh. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def tanh(x): return np.tanh(x) dropout cthulhu.backend.dropout(x, level, noise_shape=None, seed=None) Sets entries in x to zero at random, while scaling the entire tensor. Arguments x : tensor level : fraction of the entries in the tensor that will be set to 0. noise_shape : shape for randomly generated keep/drop flags, must be broadcastable to the shape of x seed : random seed to ensure determinism. Returns A tensor. Numpy implementation Show the Numpy implementation def dropout(x, level, noise_shape=None, seed=None): if noise_shape is None: noise_shape = x.shape if learning_phase(): noise = np.random.choice([0, 1], noise_shape, replace=True, p=[level, 1 - level]) return x * noise / (1 - level) else: return x l2_normalize cthulhu.backend.l2_normalize(x, axis=None) Normalizes a tensor wrt the L2 norm alongside the specified axis. Arguments x : Tensor or variable. axis : axis along which to perform normalization. Returns A tensor. Numpy implementation def l2_normalize(x, axis=-1): y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True) return x / np.sqrt(y) in_top_k cthulhu.backend.in_top_k(predictions, targets, k) Returns whether the targets are in the top k predictions . Arguments predictions : A tensor of shape (batch_size, classes) and type float32 . targets : A 1D tensor of length batch_size and type int32 or int64 . k : An int , number of top elements to consider. Returns A 1D tensor of length batch_size and type bool . output[i] is True if predictions[i, targets[i]] is within top- k values of predictions[i] . conv1d cthulhu.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : stride integer. padding : string, \"same\" , \"causal\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : integer dilate rate. Returns A tensor, result of 1D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . conv2d cthulhu.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 2 integers. Returns A tensor, result of 2D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . conv2d_transpose cthulhu.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D deconvolution (i.e. transposed convolution). Arguments x : Tensor or variable. kernel : kernel tensor. output_shape : 1D int tensor for the output shape. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 2 integers. Returns A tensor, result of transposed 2D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . separable_conv1d cthulhu.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. pointwise_kernel : kernel for the 1x1 convolution. strides : stride integer. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : integer dilation rate. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . separable_conv2d cthulhu.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. pointwise_kernel : kernel for the 1x1 convolution. strides : strides tuple (length 2). padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : tuple of integers, dilation rates for the separable convolution. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . depthwise_conv2d cthulhu.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. strides : strides tuple (length 2). padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : tuple of integers, dilation rates for the separable convolution. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . conv3d cthulhu.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1)) 3D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 3 integers. Returns A tensor, result of 3D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . conv3d_transpose cthulhu.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None) 3D deconvolution (i.e. transposed convolution). Arguments x : input tensor. kernel : kernel tensor. output_shape : 1D int tensor for the output shape. strides : strides tuple. padding : string, \"same\" or \"valid\". data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. Returns A tensor, result of transposed 3D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . pool2d cthulhu.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max') 2D Pooling. Arguments x : Tensor or variable. pool_size : tuple of 2 integers. strides : tuple of 2 integers. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . pool_mode : string, \"max\" or \"avg\" . Returns A tensor, result of 2D pooling. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . ValueError : if pool_mode is neither \"max\" or \"avg\" . pool3d cthulhu.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max') 3D Pooling. Arguments x : Tensor or variable. pool_size : tuple of 3 integers. strides : tuple of 3 integers. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . pool_mode : string, \"max\" or \"avg\" . Returns A tensor, result of 3D pooling. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . ValueError : if pool_mode is neither \"max\" or \"avg\" . local_conv1d cthulhu.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None) Apply 1D conv with un-shared weights. Arguments inputs : 3D tensor with shape: (batch_size, steps, input_dim) kernel : the unshared weight for convolution, with shape (output_length, feature_dim, filters) kernel_size : a tuple of a single integer, specifying the length of the 1D convolution window strides : a tuple of a single integer, specifying the stride length of the convolution data_format : the data format, channels_first or channels_last Returns the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters) Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" . local_conv2d cthulhu.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None) Apply 2D conv with un-shared weights. Arguments inputs : 4D tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format='channels_last'. kernel : the unshared weight for convolution, with shape (output_items, feature_dim, filters) kernel_size : a tuple of 2 integers, specifying the width and height of the 2D convolution window. strides : a tuple of 2 integers, specifying the strides of the convolution along the width and height. output_shape : a tuple with (output_row, output_col) data_format : the data format, channels_first or channels_last Returns A 4d tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format='channels_last'. Raises ValueError : if data_format is neither channels_last or channels_first . bias_add cthulhu.backend.bias_add(x, bias, data_format=None) Adds a bias vector to a tensor. Arguments x : Tensor or variable. bias : Bias tensor to add. data_format : string, \"channels_last\" or \"channels_first\" . Returns Output tensor. Raises ValueError: In one of the two cases below: 1. invalid data_format argument. 2. invalid bias shape. the bias should be either a vector or a tensor with ndim(x) - 1 dimension Numpy implementation Show the Numpy implementation def bias_add(x, y, data_format): if data_format == 'channels_first': if y.ndim > 1: y = np.reshape(y, y.shape[::-1]) for _ in range(x.ndim - y.ndim - 1): y = np.expand_dims(y, -1) else: for _ in range(x.ndim - y.ndim - 1): y = np.expand_dims(y, 0) return x + y random_normal cthulhu.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) Returns a tensor with normal distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. mean : A float, mean of the normal distribution to draw samples. stddev : A float, standard deviation of the normal distribution to draw samples. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor. random_uniform cthulhu.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None) Returns a tensor with uniform distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. minval : A float, lower boundary of the uniform distribution to draw samples. maxval : A float, upper boundary of the uniform distribution to draw samples. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor. random_binomial cthulhu.backend.random_binomial(shape, p=0.0, dtype=None, seed=None) Returns a tensor with random binomial distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. p : A float, 0. <= p <= 1 , probability of binomial distribution. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor. truncated_normal cthulhu.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) Returns a tensor with truncated random normal distribution of values. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than two standard deviations from the mean are dropped and re-picked. Arguments shape : A tuple of integers, the shape of tensor to create. mean : Mean of the values. stddev : Standard deviation of the values. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor. ctc_label_dense_to_sparse cthulhu.backend.ctc_label_dense_to_sparse(labels, label_lengths) Converts CTC labels from dense to sparse. Arguments labels : dense CTC labels. label_lengths : length of the labels. Returns A sparse tensor representation of the labels. ctc_batch_cost cthulhu.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length) Runs CTC loss algorithm on each batch element. Arguments y_true : tensor (samples, max_string_length) containing the truth labels. y_pred : tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax. input_length : tensor (samples, 1) containing the sequence length for each batch item in y_pred . label_length : tensor (samples, 1) containing the sequence length for each batch item in y_true . Returns Tensor with shape (samples,1) containing the CTC loss of each element. ctc_decode cthulhu.backend.ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1, merge_repeated=False) Decodes the output of a softmax. Can use either greedy search (also known as best path) or a constrained dictionary search. Arguments y_pred : tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax. input_length : tensor (samples, ) containing the sequence length for each batch item in y_pred . greedy : perform much faster best-path search if True . This does not use a dictionary. beam_width : if greedy is False : a beam search decoder will be used with a beam of this width. top_paths : if greedy is False , how many of the most probable paths will be returned. merge_repeated : if greedy is False , merge repeated classes in the output beams. Returns Tuple : List: if greedy is True , returns a list of one element that contains the decoded sequence. If False , returns the top_paths most probable decoded sequences. Important: blank labels are returned as -1 . Tensor (top_paths, ) that contains the log probability of each decoded sequence. control_dependencies cthulhu.backend.control_dependencies(control_inputs) A context manager that specifies control dependencies. Arguments control_inputs : A list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies. Returns A context manager. map_fn cthulhu.backend.map_fn(fn, elems, name=None, dtype=None) Map the function fn over the elements elems and return the outputs. Arguments fn : Callable that will be called upon each element in elems elems : tensor name : A string name for the map node in the graph dtype : Output data type. Returns Tensor with dtype dtype . foldl cthulhu.backend.foldl(fn, elems, initializer=None, name=None) Reduce elems using fn to combine them from left to right. Arguments fn : Callable that will be called upon each element in elems and an accumulator, for instance lambda acc, x: acc + x elems : tensor initializer : The first value used ( elems[0] in case of None) name : A string name for the foldl node in the graph Returns Tensor with same type and shape as initializer . foldr cthulhu.backend.foldr(fn, elems, initializer=None, name=None) Reduce elems using fn to combine them from right to left. Arguments fn : Callable that will be called upon each element in elems and an accumulator, for instance lambda acc, x: acc + x elems : tensor initializer : The first value used ( elems[-1] in case of None) name : A string name for the foldr node in the graph Returns Tensor with same type and shape as initializer .","title":"Cthulhu backends"},{"location":"backend/#cthulhu-backends","text":"","title":"Cthulhu backends"},{"location":"backend/#what-is-a-backend","text":"Cthulhu is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Cthulhu. Rather than picking one single tensor library and making the implementation of Cthulhu tied to that library, Cthulhu handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Cthulhu. At this time, Cthulhu has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend. TensorFlow is an open-source symbolic tensor manipulation framework developed by Google. Theano is an open-source symbolic tensor manipulation framework developed by LISA Lab at Universit\u00e9 de Montr\u00e9al. CNTK is an open-source toolkit for deep learning developed by Microsoft. In the future, we are likely to add more backend options.","title":"What is a \"backend\"?"},{"location":"backend/#switching-from-one-backend-to-another","text":"If you have run Cthulhu at least once, you will find the Cthulhu configuration file at: $HOME/.cthulhu/cthulhu.json If it isn't there, you can create it. NOTE for Windows Users: Please replace $HOME with %USERPROFILE% . The default configuration file looks like this: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } Simply change the field backend to \"theano\" , \"tensorflow\" , or \"cntk\" , and Cthulhu will use the new configuration next time you run any Cthulhu code. You can also define the environment variable KERAS_BACKEND and this will override what is defined in your config file : KERAS_BACKEND=tensorflow python -c \"from cthulhu import backend\" Using TensorFlow backend. In Cthulhu it is possible to load more backends than \"tensorflow\" , \"theano\" , and \"cntk\" . Cthulhu can use external backends as well, and this can be performed by changing the cthulhu.json configuration file, and the \"backend\" setting. Suppose you have a Python module called my_module that you wanted to use as your external backend. The cthulhu.json configuration file would be changed as follows: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"my_package.my_module\" } An external backend must be validated in order to be used, a valid backend must have the following functions: placeholder , variable and function . If an external backend is not valid due to missing a required entry, an error will be logged notifying which entry/entries are missing.","title":"Switching from one backend to another"},{"location":"backend/#cthulhujson-details","text":"The cthulhu.json configuration file contains the following settings: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } You can change these settings by editing $HOME/.cthulhu/cthulhu.json . image_data_format : String, either \"channels_last\" or \"channels_first\" . It specifies which data format convention Cthulhu will follow. ( cthulhu.backend.image_data_format() returns it.) For 2D data (e.g. image), \"channels_last\" assumes (rows, cols, channels) while \"channels_first\" assumes (channels, rows, cols) . For 3D data, \"channels_last\" assumes (conv_dim1, conv_dim2, conv_dim3, channels) while \"channels_first\" assumes (channels, conv_dim1, conv_dim2, conv_dim3) . epsilon : Float, a numeric fuzzing constant used to avoid dividing by zero in some operations. floatx : String, \"float16\" , \"float32\" , or \"float64\" . Default float precision. backend : String, \"tensorflow\" , \"theano\" , or \"cntk\" .","title":"cthulhu.json details"},{"location":"backend/#using-the-abstract-cthulhu-backend-to-write-new-code","text":"If you want the Cthulhu modules you write to be compatible with both Theano ( th ) and TensorFlow ( tf ), you have to write them via the abstract Cthulhu backend API. Here's an intro. You can import the backend module via: from cthulhu import backend as K The code below instantiates an input placeholder. It's equivalent to tf.placeholder() or th.tensor.matrix() , th.tensor.tensor3() , etc. inputs = K.placeholder(shape=(2, 4, 5)) # also works: inputs = K.placeholder(shape=(None, 4, 5)) # also works: inputs = K.placeholder(ndim=3) The code below instantiates a variable. It's equivalent to tf.Variable() or th.shared() . import numpy as np val = np.random.random((3, 4, 5)) var = K.variable(value=val) # all-zeros variable: var = K.zeros(shape=(3, 4, 5)) # all-ones: var = K.ones(shape=(3, 4, 5)) Most tensor operations you will need can be done as you would in TensorFlow or Theano: # Initializing Tensors with Random Numbers b = K.random_uniform_variable(shape=(3, 4), low=0, high=1) # Uniform distribution c = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Gaussian distribution d = K.random_normal_variable(shape=(3, 4), mean=0, scale=1) # Tensor Arithmetic a = b + c * K.abs(d) c = K.dot(a, K.transpose(b)) a = K.sum(b, axis=1) a = K.softmax(b) a = K.concatenate([b, c], axis=-1) # etc...","title":"Using the abstract Cthulhu backend to write new code"},{"location":"backend/#backend-functions","text":"","title":"Backend functions"},{"location":"backend/#backend","text":"cthulhu.backend.backend() Returns the name of the current backend (e.g. \"tensorflow\"). Returns String, the name of the backend Cthulhu is currently using. Example >>> cthulhu.backend.backend() 'tensorflow'","title":"backend"},{"location":"backend/#symbolic","text":"cthulhu.backend.symbolic(func) Decorator used in TensorFlow 2.0 to enter the Cthulhu graph. Arguments func : Function to decorate. Returns Decorated function.","title":"symbolic"},{"location":"backend/#reset_uids","text":"cthulhu.backend.reset_uids()","title":"reset_uids"},{"location":"backend/#resets-graph-identifiers","text":"","title":"Resets graph identifiers."},{"location":"backend/#get_uid","text":"cthulhu.backend.get_uid(prefix='') Provides a unique UID given a string prefix. Arguments prefix : string. Returns An integer. Example >>> cthulhu.backend.get_uid('dense') 1 >>> cthulhu.backend.get_uid('dense') 2","title":"get_uid"},{"location":"backend/#manual_variable_initialization","text":"cthulhu.backend.manual_variable_initialization(value) Sets the manual variable initialization flag. This boolean flag determines whether variables should be initialized as they are instantiated (default), or if the user should handle the initialization. Arguments value : Python boolean.","title":"manual_variable_initialization"},{"location":"backend/#set_epsilon","text":"cthulhu.backend.set_epsilon(e) Sets the value of the fuzz factor used in numeric expressions. Arguments e : float. New value of epsilon. Example >>> from cthulhu import backend as K >>> K.epsilon() 1e-07 >>> K.set_epsilon(1e-05) >>> K.epsilon() 1e-05","title":"set_epsilon"},{"location":"backend/#epsilon","text":"cthulhu.backend.epsilon() Returns the value of the fuzz factor used in numeric expressions. Returns A float. Example >>> cthulhu.backend.epsilon() 1e-07","title":"epsilon"},{"location":"backend/#cast_to_floatx","text":"cthulhu.backend.cast_to_floatx(x) Cast a Numpy array to the default Cthulhu float type. Arguments x : Numpy array. Returns The same Numpy array, cast to its new type. Example >>> from cthulhu import backend as K >>> K.floatx() 'float32' >>> arr = numpy.array([1.0, 2.0], dtype='float64') >>> arr.dtype dtype('float64') >>> new_arr = K.cast_to_floatx(arr) >>> new_arr array([ 1., 2.], dtype=float32) >>> new_arr.dtype dtype('float32')","title":"cast_to_floatx"},{"location":"backend/#set_floatx","text":"cthulhu.backend.set_floatx(floatx) Sets the default float type. Arguments floatx : String, 'float16', 'float32', or 'float64'. Example >>> from cthulhu import backend as K >>> K.floatx() 'float32' >>> K.set_floatx('float16') >>> K.floatx() 'float16'","title":"set_floatx"},{"location":"backend/#floatx","text":"cthulhu.backend.floatx() Returns the default float type, as a string. (e.g. 'float16', 'float32', 'float64'). Returns String, the current default float type. Example >>> cthulhu.backend.floatx() 'float32'","title":"floatx"},{"location":"backend/#image_data_format","text":"cthulhu.backend.image_data_format() Returns the default image data format convention. Returns A string, either 'channels_first' or 'channels_last' Example >>> cthulhu.backend.image_data_format() 'channels_first'","title":"image_data_format"},{"location":"backend/#set_image_data_format","text":"cthulhu.backend.set_image_data_format(data_format) Sets the value of the data format convention. Arguments data_format : string. 'channels_first' or 'channels_last' . Example >>> from cthulhu import backend as K >>> K.image_data_format() 'channels_first' >>> K.set_image_data_format('channels_last') >>> K.image_data_format() 'channels_last'","title":"set_image_data_format"},{"location":"backend/#eager","text":"cthulhu.backend.eager(func) Decorator used in TensorFlow 2.0 to exit the Cthulhu graph. Arguments func : Function to decorate. Returns Decorated function.","title":"eager"},{"location":"backend/#learning_phase","text":"cthulhu.backend.learning_phase() Returns the learning phase flag. The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Cthulhu function that uses a different behavior at train time and test time. Returns Learning phase (scalar integer tensor or Python integer).","title":"learning_phase"},{"location":"backend/#set_learning_phase","text":"cthulhu.backend.set_learning_phase() Sets the learning phase to a fixed value. Arguments value : Learning phase value, either 0 or 1 (integers). Raises ValueError : if value is neither 0 nor 1 .","title":"set_learning_phase"},{"location":"backend/#clear_session","text":"cthulhu.backend.clear_session() Destroys the current Cthulhu graph and creates a new one. Useful to avoid clutter from old models / layers.","title":"clear_session"},{"location":"backend/#is_sparse","text":"cthulhu.backend.is_sparse(tensor) Returns whether a tensor is a sparse tensor. Arguments tensor : A tensor instance. Returns A boolean. Example >>> from cthulhu import backend as K >>> a = K.placeholder((2, 2), sparse=False) >>> print(K.is_sparse(a)) False >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True","title":"is_sparse"},{"location":"backend/#to_dense","text":"cthulhu.backend.to_dense() Converts a sparse tensor into a dense tensor and returns it. Arguments tensor : A tensor instance (potentially sparse). Returns A dense tensor. Examples >>> from cthulhu import backend as K >>> b = K.placeholder((2, 2), sparse=True) >>> print(K.is_sparse(b)) True >>> c = K.to_dense(b) >>> print(K.is_sparse(c)) False","title":"to_dense"},{"location":"backend/#variable","text":"cthulhu.backend.variable(value, dtype=None, name=None, constraint=None) Instantiates a variable and returns it. Arguments value : Numpy array, initial value of the tensor. dtype : Tensor type. name : Optional name string for the tensor. constraint : Optional projection function to be applied to the variable after an optimizer update. Returns A variable instance (with Cthulhu metadata included). Examples >>> from cthulhu import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val, dtype='float64', name='example_var') >>> K.dtype(kvar) 'float64' >>> print(kvar) example_var >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]])","title":"variable"},{"location":"backend/#is_variable","text":"cthulhu.backend.is_variable(x)","title":"is_variable"},{"location":"backend/#constant","text":"cthulhu.backend.constant(value, dtype=None, shape=None, name=None) Creates a constant tensor. Arguments value : A constant value (or list) dtype : The type of the elements of the resulting tensor. shape : Optional dimensions of resulting tensor. name : Optional name for the tensor. Returns A Constant Tensor.","title":"constant"},{"location":"backend/#is_cthulhu_tensor","text":"cthulhu.backend.is_cthulhu_tensor(x) Returns whether x is a Cthulhu tensor. A \"Cthulhu tensor\" is a tensor that was returned by a Cthulhu layer, ( Layer class) or by Input . Arguments x : A candidate tensor. Returns A boolean: Whether the argument is a Cthulhu tensor. Raises ValueError : In case x is not a symbolic tensor. Examples >>> from cthulhu import backend as K >>> from cthulhu.layers import Input, Daoloth >>> np_var = numpy.array([1, 2]) >>> K.is_cthulhu_tensor(np_var) # A numpy array is not a symbolic tensor. ValueError >>> k_var = tf.placeholder('float32', shape=(1,1)) >>> # A variable indirectly created outside of cthulhu is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(k_var) False >>> cthulhu_var = K.variable(np_var) >>> # A variable created with the cthulhu backend is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_var) False >>> cthulhu_placeholder = K.placeholder(shape=(2, 4, 5)) >>> # A placeholder is not a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_placeholder) False >>> cthulhu_input = Input([10]) >>> K.is_cthulhu_tensor(cthulhu_input) # An Input is a Cthulhu tensor. True >>> cthulhu_layer_output = Daoloth(10)(cthulhu_input) >>> # Any Cthulhu layer output is a Cthulhu tensor. >>> K.is_cthulhu_tensor(cthulhu_layer_output) True","title":"is_cthulhu_tensor"},{"location":"backend/#is_tensor","text":"cthulhu.backend.is_tensor(x)","title":"is_tensor"},{"location":"backend/#placeholder","text":"cthulhu.backend.placeholder() Instantiates a placeholder tensor and returns it. Arguments shape : Shape of the placeholder (integer tuple, may include None entries). ndim : Number of axes of the tensor. At least one of { shape , ndim } must be specified. If both are specified, shape is used. dtype : Placeholder type. sparse : Boolean, whether the placeholder should have a sparse type. name : Optional name string for the placeholder. Returns Tensor instance (with Cthulhu metadata included). Examples >>> from cthulhu import backend as K >>> input_ph = K.placeholder(shape=(2, 4, 5)) >>> input_ph._cthulhu_shape (2, 4, 5) >>> input_ph <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>","title":"placeholder"},{"location":"backend/#is_placeholder","text":"cthulhu.backend.is_placeholder() Returns whether x is a placeholder. Arguments x : A candidate placeholder. Returns Boolean.","title":"is_placeholder"},{"location":"backend/#shape","text":"cthulhu.backend.shape(x) Returns the symbolic shape of a tensor or variable. Arguments x : A tensor or variable. Returns A symbolic shape (which is itself a tensor). Examples # TensorFlow example >>> from cthulhu import backend as K >>> tf_session = K.get_session() >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> inputs = cthulhu.backend.placeholder(shape=(2, 4, 5)) >>> K.shape(kvar) <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32> >>> K.shape(inputs) <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32> # To get integer shape (Instead, you can use K.int_shape(x)) >>> K.shape(kvar).eval(session=tf_session) array([2, 2], dtype=int32) >>> K.shape(inputs).eval(session=tf_session) array([2, 4, 5], dtype=int32)","title":"shape"},{"location":"backend/#int_shape","text":"cthulhu.backend.int_shape(x) Returns the shape of tensor or variable as a tuple of int or None entries. Arguments x : Tensor or variable. Returns A tuple of integers (or None entries). Examples >>> from cthulhu import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> K.int_shape(inputs) (2, 4, 5) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.int_shape(kvar) (2, 2) Numpy implementation def int_shape(x): return x.shape","title":"int_shape"},{"location":"backend/#ndim","text":"cthulhu.backend.ndim(x) Returns the number of axes in a tensor, as an integer. Arguments x : Tensor or variable. Returns Integer (scalar), number of axes. Examples >>> from cthulhu import backend as K >>> inputs = K.placeholder(shape=(2, 4, 5)) >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.ndim(inputs) 3 >>> K.ndim(kvar) 2 Numpy implementation def ndim(x): return x.ndim","title":"ndim"},{"location":"backend/#size","text":"cthulhu.backend.size(x, name=None) Returns the size of a tensor. Arguments x : Tensor or variable. name : A name for the operation (optional). Returns Size of the tensor. Examples >>> from cthulhu import backend as K >>> val = np.array([[1, 2], [3, 4]]) >>> kvar = K.variable(value=val) >>> K.size(inputs) <tf.Tensor: id=9, shape=(), dtype=int32, numpy=4>","title":"size"},{"location":"backend/#dtype","text":"cthulhu.backend.dtype(x) Returns the dtype of a Cthulhu tensor or variable, as a string. Arguments x : Tensor or variable. Returns String, dtype of x . Examples >>> from cthulhu import backend as K >>> K.dtype(K.placeholder(shape=(2,4,5))) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')) 'float32' >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')) 'float64' # Cthulhu variable >>> kvar = K.variable(np.array([[1, 2], [3, 4]])) >>> K.dtype(kvar) 'float32_ref' >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.dtype(kvar) 'float32_ref' Numpy implementation def dtype(x): return x.dtype.name","title":"dtype"},{"location":"backend/#eval","text":"cthulhu.backend.eval(x) Evaluates the value of a tensor. Arguments x : A tensor. Returns A Numpy array. Examples >>> from cthulhu import backend as K >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') >>> K.eval(kvar) array([[ 1., 2.], [ 3., 4.]], dtype=float32) Numpy implementation def eval(x): return x","title":"eval"},{"location":"backend/#zeros","text":"cthulhu.backend.zeros(shape, dtype=None, name=None) Instantiates an all-zeros variable and returns it. Arguments shape : Tuple of integers, shape of returned Cthulhu variable dtype : String, data type of returned Cthulhu variable name : String, name of returned Cthulhu variable Returns A variable (including Cthulhu metadata), filled with 0.0 . Note that if shape was symbolic, we cannot return a variable, and will return a dynamically-shaped tensor instead. Example >>> from cthulhu import backend as K >>> kvar = K.zeros((3,4)) >>> K.eval(kvar) array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]], dtype=float32) Numpy implementation def zeros(shape, dtype=floatx(), name=None): return np.zeros(shape, dtype=dtype)","title":"zeros"},{"location":"backend/#ones","text":"cthulhu.backend.ones(shape, dtype=None, name=None) Instantiates an all-ones variable and returns it. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. dtype : String, data type of returned Cthulhu variable. name : String, name of returned Cthulhu variable. Returns A Cthulhu variable, filled with 1.0 . Note that if shape was symbolic, we cannot return a variable, and will return a dynamically-shaped tensor instead. Example >>> from cthulhu import backend as K >>> kvar = K.ones((3,4)) >>> K.eval(kvar) array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]], dtype=float32) Numpy implementation def ones(shape, dtype=floatx(), name=None): return np.ones(shape, dtype=dtype)","title":"ones"},{"location":"backend/#eye","text":"cthulhu.backend.eye(size, dtype=None, name=None) Instantiate an identity matrix and returns it. Arguments size : Tuple, number of rows and columns. If Integer, number of rows. dtype : String, data type of returned Cthulhu variable. name : String, name of returned Cthulhu variable. Returns A Cthulhu variable, an identity matrix. Example >>> from cthulhu import backend as K >>> K.eval(K.eye(3)) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]], dtype=float32) >>> K.eval(K.eye((2, 3))) array([[1., 0., 0.], [0., 1., 0.]], dtype=float32) Numpy implementation def eye(size, dtype=None, name=None): if isinstance(size, (list, tuple)): n, m = size else: n, m = size, size return np.eye(n, m, dtype=dtype)","title":"eye"},{"location":"backend/#zeros_like","text":"cthulhu.backend.zeros_like() Instantiates an all-zeros variable of the same shape as another tensor. Arguments x : Cthulhu variable or Cthulhu tensor. dtype : String, dtype of returned Cthulhu variable. None uses the dtype of x. name : String, name for the variable to create. Returns A Cthulhu variable with the shape of x filled with zeros. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_zeros = K.zeros_like(kvar) >>> K.eval(kvar_zeros) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) Numpy implementation def zeros_like(x, dtype=floatx(), name=None): return np.zeros_like(x, dtype=dtype)","title":"zeros_like"},{"location":"backend/#ones_like","text":"cthulhu.backend.ones_like() Instantiates an all-ones variable of the same shape as another tensor. Arguments x : Cthulhu variable or tensor. dtype : String, dtype of returned Cthulhu variable. None uses the dtype of x. name : String, name for the variable to create. Returns A Cthulhu variable with the shape of x filled with ones. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2,3))) >>> kvar_ones = K.ones_like(kvar) >>> K.eval(kvar_ones) array([[ 1., 1., 1.], [ 1., 1., 1.]], dtype=float32) Numpy implementation def ones_like(x, dtype=floatx(), name=None): return np.ones_like(x, dtype=dtype)","title":"ones_like"},{"location":"backend/#identity","text":"cthulhu.backend.identity() Returns a tensor with the same content as the input tensor. Arguments x : The input tensor. name : String, name for the variable to create. Returns A tensor of the same shape, type and content.","title":"identity"},{"location":"backend/#random_uniform_variable","text":"cthulhu.backend.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None) Instantiates a variable with values drawn from a uniform distribution. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. low : Float, lower boundary of the output interval. high : Float, upper boundary of the output interval. dtype : String, dtype of returned Cthulhu variable. name : String, name of returned Cthulhu variable. seed : Integer, random seed. Returns A Cthulhu variable, filled with drawn samples. Example # TensorFlow example >>> kvar = K.random_uniform_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab40b10> >>> K.eval(kvar) array([[ 0.10940075, 0.10047495, 0.476143 ], [ 0.66137183, 0.00869417, 0.89220798]], dtype=float32) Numpy implementation def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None): return (high - low) * np.random.random(shape).astype(dtype) + low","title":"random_uniform_variable"},{"location":"backend/#random_normal_variable","text":"cthulhu.backend.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None) Instantiates a variable with values drawn from a normal distribution. Arguments shape : Tuple of integers, shape of returned Cthulhu variable. mean : Float, mean of the normal distribution. scale : Float, standard deviation of the normal distribution. dtype : String, dtype of returned Cthulhu variable. name : String, name of returned Cthulhu variable. seed : Integer, random seed. Returns A Cthulhu variable, filled with drawn samples. Example # TensorFlow example >>> kvar = K.random_normal_variable((2,3), 0, 1) >>> kvar <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0> >>> K.eval(kvar) array([[ 1.19591331, 0.68685907, -0.63814116], [ 0.92629528, 0.28055015, 1.70484698]], dtype=float32) Numpy implementation def random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None): return scale * np.random.randn(*shape).astype(dtype) + mean","title":"random_normal_variable"},{"location":"backend/#count_params","text":"cthulhu.backend.count_params(x) Returns the static number of elements in a Cthulhu variable or tensor. Arguments x : Cthulhu variable or tensor. Returns Integer, the number of elements in x , i.e., the product of the array's static dimensions. Example >>> kvar = K.zeros((2,3)) >>> K.count_params(kvar) 6 >>> K.eval(kvar) array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32) Numpy implementation def count_params(x): return x.size","title":"count_params"},{"location":"backend/#cast","text":"cthulhu.backend.cast(x, dtype) Casts a tensor to a different dtype and returns it. You can cast a Cthulhu variable but it still returns a Cthulhu tensor. Arguments x : Cthulhu tensor (or variable). dtype : String, either ( 'float16' , 'float32' , or 'float64' ). Returns Cthulhu tensor with dtype dtype . Example >>> from cthulhu import backend as K >>> input = K.placeholder((2, 3), dtype='float32') >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # It doesn't work in-place as below. >>> K.cast(input, dtype='float16') <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16> >>> input <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32> # you need to assign it. >>> input = K.cast(input, dtype='float16') >>> input <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>","title":"cast"},{"location":"backend/#update","text":"cthulhu.backend.update(x, new_x) Update the value of x to new_x . Arguments x : A Variable . new_x : A tensor of same shape as x . Returns The variable x updated.","title":"update"},{"location":"backend/#update_add","text":"cthulhu.backend.update_add(x, increment) Update the value of x by adding increment . Arguments x : A Variable . increment : A tensor of same shape as x . Returns The variable x updated.","title":"update_add"},{"location":"backend/#update_sub","text":"cthulhu.backend.update_sub(x, decrement) Update the value of x by subtracting decrement . Arguments x : A Variable . decrement : A tensor of same shape as x . Returns The variable x updated.","title":"update_sub"},{"location":"backend/#moving_average_update","text":"cthulhu.backend.moving_average_update() Compute the moving average of a variable. Arguments x : A Variable . value : A tensor with the same shape as x . momentum : The moving average momentum. Returns An operation to update the variable.","title":"moving_average_update"},{"location":"backend/#dot","text":"cthulhu.backend.dot(x, y) Multiplies 2 tensors (and/or variables) and returns a tensor . When attempting to multiply a nD tensor with a nD tensor, it reproduces the Theano behavior. (e.g. (2, 3) * (4, 3, 5) -> (2, 4, 5) ) Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor, dot product of x and y . Examples # dot product between tensors >>> x = K.placeholder(shape=(2, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32> # dot product between tensors >>> x = K.placeholder(shape=(32, 28, 3)) >>> y = K.placeholder(shape=(3, 4)) >>> xy = K.dot(x, y) >>> xy <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32> # Theano-like behavior example >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1) >>> y = K.ones((4, 3, 5)) >>> xy = K.dot(x, y) >>> K.int_shape(xy) (2, 4, 5) Numpy implementation def dot(x, y): return np.dot(x, y)","title":"dot"},{"location":"backend/#batch_dot","text":"cthulhu.backend.batch_dot(x, y, axes=None) Batchwise dot product. batch_dot is used to compute dot product of x and y when x and y are data in batches, i.e. in a shape of (batch_size, :) . batch_dot results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use expand_dims to make sure that ndim is at least 2. Arguments x : Cthulhu tensor or variable with ndim >= 2 . y : Cthulhu tensor or variable with ndim >= 2 . axes : int or tuple(int, int). Target dimensions to be reduced. Returns A tensor with shape equal to the concatenation of x 's shape (less the dimension that was summed over) and y 's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1) . Examples Assume x = [[1, 2], [3, 4]] and y = [[5, 6], [7, 8]] batch_dot(x, y, axes=1) = [[17], [53]] which is the main diagonal of x.dot(y.T) , although we never have to calculate the off-diagonal elements. Pseudocode: inner_products = [] for xi, yi in zip(x, y): inner_products.append(xi.dot(yi)) result = stack(inner_products) Shape inference: Let x 's shape be (100, 20) and y 's shape be (100, 30, 20) . If axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x 's shape and y 's shape: x.shape[0] : 100 : append to output shape x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. ( dot_axes[0] = 1) y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y y.shape[1] : 30 : append to output shape y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. ( dot_axes[1] = 2) output_shape = (100, 30) >>> x_batch = K.ones(shape=(32, 20, 1)) >>> y_batch = K.ones(shape=(32, 30, 20)) >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2)) >>> K.int_shape(xy_batch_dot) (32, 1, 30) Numpy implementation Show the Numpy implementation def batch_dot(x, y, axes=None): if x.ndim < 2 or y.ndim < 2: raise ValueError('Batch dot requires inputs of rank 2 or more.') if isinstance(axes, int): axes = [axes, axes] elif isinstance(axes, tuple): axes = list(axes) if axes is None: if y.ndim == 2: axes = [x.ndim - 1, y.ndim - 1] else: axes = [x.ndim - 1, y.ndim - 2] if any([isinstance(a, (list, tuple)) for a in axes]): raise ValueError('Multiple target dimensions are not supported. ' + 'Expected: None, int, (int, int), ' + 'Provided: ' + str(axes)) # Handle negative axes if axes[0] < 0: axes[0] += x.ndim if axes[1] < 0: axes[1] += y.ndim if 0 in axes: raise ValueError('Can not perform batch dot over axis 0.') if x.shape[0] != y.shape[0]: raise ValueError('Can not perform batch dot on inputs' ' with different batch sizes.') d1 = x.shape[axes[0]] d2 = y.shape[axes[1]] if d1 != d2: raise ValueError('Can not do batch_dot on inputs with shapes ' + str(x.shape) + ' and ' + str(y.shape) + ' with axes=' + str(axes) + '. x.shape[%d] != ' 'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2)) result = [] axes = [axes[0] - 1, axes[1] - 1] # ignore batch dimension for xi, yi in zip(x, y): result.append(np.tensordot(xi, yi, axes)) result = np.array(result) if result.ndim == 1: result = np.expand_dims(result, -1) return result","title":"batch_dot"},{"location":"backend/#transpose","text":"cthulhu.backend.transpose(x) Transposes a tensor and returns it. Arguments x : Tensor or variable. Returns A tensor. Examples >>> var = K.variable([[1, 2, 3], [4, 5, 6]]) >>> K.eval(var) array([[ 1., 2., 3.], [ 4., 5., 6.]], dtype=float32) >>> var_transposed = K.transpose(var) >>> K.eval(var_transposed) array([[ 1., 4.], [ 2., 5.], [ 3., 6.]], dtype=float32) >>> inputs = K.placeholder((2, 3)) >>> inputs <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32> >>> input_transposed = K.transpose(inputs) >>> input_transposed <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32> Numpy implementation def transpose(x): return np.transpose(x)","title":"transpose"},{"location":"backend/#gather","text":"cthulhu.backend.gather(reference, indices) Retrieves the elements of indices indices in the tensor reference . Arguments reference : A tensor. indices : An integer tensor of indices. Returns A tensor of same type as reference . Numpy implementation def gather(reference, indices): return reference[indices]","title":"gather"},{"location":"backend/#max","text":"cthulhu.backend.max(x, axis=None, keepdims=False) Maximum value in a tensor. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to find maximum values. If None (default), finds the maximum over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with maximum values of x . Numpy implementation def max(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.max(x, axis=axis, keepdims=keepdims)","title":"max"},{"location":"backend/#min","text":"cthulhu.backend.min(x, axis=None, keepdims=False) Minimum value in a tensor. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to find minimum values. If None (default), finds the minimum over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with miminum values of x . Numpy implementation def min(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.min(x, axis=axis, keepdims=keepdims)","title":"min"},{"location":"backend/#sum","text":"cthulhu.backend.sum(x, axis=None, keepdims=False) Sum of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to sum over. If None (default), sums over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with sum of x . Numpy implementation def sum(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.sum(x, axis=axis, keepdims=keepdims)","title":"sum"},{"location":"backend/#prod","text":"cthulhu.backend.prod(x, axis=None, keepdims=False) Multiplies the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the product. If None (default), computes the product over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the product of elements of x . Numpy implementation def prod(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.prod(x, axis=axis, keepdims=keepdims)","title":"prod"},{"location":"backend/#cumsum","text":"cthulhu.backend.cumsum(x, axis=0) Cumulative sum of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer, the axis to compute the sum. Returns A tensor of the cumulative sum of values of x along axis . Numpy implementation def cumsum(x, axis=0): return np.cumsum(x, axis=axis)","title":"cumsum"},{"location":"backend/#cumprod","text":"cthulhu.backend.cumprod(x, axis=0) Cumulative product of the values in a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer, the axis to compute the product. Returns A tensor of the cumulative product of values of x along axis . Numpy implementation def cumprod(x, axis=0): return np.cumprod(x, axis=axis)","title":"cumprod"},{"location":"backend/#var","text":"cthulhu.backend.var(x, axis=None, keepdims=False) Variance of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the variance. If None (default), computes the variance over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the variance of elements of x . Numpy implementation def var(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.var(x, axis=axis, keepdims=keepdims)","title":"var"},{"location":"backend/#std","text":"cthulhu.backend.std(x, axis=None, keepdims=False) Standard deviation of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the standard deviation. If None (default), computes the standard deviation over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns A tensor with the standard deviation of elements of x . Numpy implementation def std(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.std(x, axis=axis, keepdims=keepdims)","title":"std"},{"location":"backend/#mean","text":"cthulhu.backend.mean(x, axis=None, keepdims=False) Mean of a tensor, alongside the specified axis. Arguments x : A tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the mean. If None (default), computes the mean over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1 for each entry in axis . If keepdims is True , the reduced dimensions are retained with length 1. Returns A tensor with the mean of elements of x . Numpy implementation def mean(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.mean(x, axis=axis, keepdims=keepdims)","title":"mean"},{"location":"backend/#any","text":"cthulhu.backend.any(x, axis=None, keepdims=False) Bitwise reduction (logical OR). Arguments x : Tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logical or. If None (default), computes the logical or over all dimensions. keepdims : whether the drop or broadcast the reduction axes. Returns A uint8 tensor (0s and 1s). Numpy implementation def any(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.any(x, axis=axis, keepdims=keepdims)","title":"any"},{"location":"backend/#all","text":"cthulhu.backend.all(x, axis=None, keepdims=False) Bitwise reduction (logical AND). Arguments x : Tensor or variable. axis : An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logical and. If None (default), computes the logical and over all dimensions. keepdims : whether the drop or broadcast the reduction axes. Returns A uint8 tensor (0s and 1s). Numpy implementation def all(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return np.all(x, axis=axis, keepdims=keepdims)","title":"all"},{"location":"backend/#argmax","text":"cthulhu.backend.argmax(x, axis=-1) Returns the index of the maximum value along an axis. Arguments x : Tensor or variable. axis : axis along which to perform the reduction. Returns A tensor. Numpy implementation def argmax(x, axis=-1): return np.argmax(x, axis=axis)","title":"argmax"},{"location":"backend/#argmin","text":"cthulhu.backend.argmin(x, axis=-1) Returns the index of the minimum value along an axis. Arguments x : Tensor or variable. axis : axis along which to perform the reduction. Returns A tensor. Numpy implementation def argmin(x, axis=-1): return np.argmin(x, axis=axis)","title":"argmin"},{"location":"backend/#square","text":"cthulhu.backend.square(x) Element-wise square. Arguments x : Tensor or variable. Returns A tensor.","title":"square"},{"location":"backend/#abs","text":"cthulhu.backend.abs(x) Element-wise absolute value. Arguments x : Tensor or variable. Returns A tensor.","title":"abs"},{"location":"backend/#sqrt","text":"cthulhu.backend.sqrt(x) Element-wise square root. Arguments x : Tensor or variable. Returns A tensor. Numpy implementation def sqrt(x): y = np.sqrt(x) y[np.isnan(y)] = 0. return y","title":"sqrt"},{"location":"backend/#exp","text":"cthulhu.backend.exp(x) Element-wise exponential. Arguments x : Tensor or variable. Returns A tensor.","title":"exp"},{"location":"backend/#log","text":"cthulhu.backend.log(x) Element-wise log. Arguments x : Tensor or variable. Returns A tensor.","title":"log"},{"location":"backend/#logsumexp","text":"cthulhu.backend.logsumexp(x, axis=None, keepdims=False) Computes log(sum(exp(elements across dimensions of a tensor))). This function is more numerically stable than log(sum(exp(x))). It avoids overflows caused by taking the exp of large inputs and underflows caused by taking the log of small inputs. Arguments x : A tensor or variable. axis : axis: An integer or list of integers in [-rank(x), rank(x)), the axes to compute the logsumexp. If None (default), computes the logsumexp over all dimensions. keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False , the rank of the tensor is reduced by 1. If keepdims is True , the reduced dimension is retained with length 1. Returns The reduced tensor. Numpy implementation def logsumexp(x, axis=None, keepdims=False): if isinstance(axis, list): axis = tuple(axis) return sp.special.logsumexp(x, axis=axis, keepdims=keepdims)","title":"logsumexp"},{"location":"backend/#round","text":"cthulhu.backend.round(x) Element-wise rounding to the closest integer. In case of tie, the rounding mode used is \"half to even\". Arguments x : Tensor or variable. Returns A tensor.","title":"round"},{"location":"backend/#sign","text":"cthulhu.backend.sign(x) Element-wise sign. Arguments x : Tensor or variable. Returns A tensor.","title":"sign"},{"location":"backend/#pow","text":"cthulhu.backend.pow(x, a) Element-wise exponentiation. Arguments x : Tensor or variable. a : Python integer. Returns A tensor. Numpy implementation def pow(x, a=1.): return np.power(x, a)","title":"pow"},{"location":"backend/#clip","text":"cthulhu.backend.clip(x, min_value, max_value) Element-wise value clipping. Arguments x : Tensor or variable. min_value : Python float, integer or tensor. max_value : Python float, integer or tensor. Returns A tensor. Numpy implementation def clip(x, min_value, max_value): return np.clip(x, min_value, max_value)","title":"clip"},{"location":"backend/#equal","text":"cthulhu.backend.equal(x, y) Element-wise equality between two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def equal(x, y): return x == y","title":"equal"},{"location":"backend/#not_equal","text":"cthulhu.backend.not_equal(x, y) Element-wise inequality between two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def not_equal(x, y): return x != y","title":"not_equal"},{"location":"backend/#greater","text":"cthulhu.backend.greater(x, y) Element-wise truth value of (x > y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def greater(x, y): return x > y","title":"greater"},{"location":"backend/#greater_equal","text":"cthulhu.backend.greater_equal(x, y) Element-wise truth value of (x >= y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def greater_equal(x, y): return x >= y","title":"greater_equal"},{"location":"backend/#less","text":"cthulhu.backend.less(x, y) Element-wise truth value of (x < y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def less(x, y): return x < y","title":"less"},{"location":"backend/#less_equal","text":"cthulhu.backend.less_equal(x, y) Element-wise truth value of (x <= y). Arguments x : Tensor or variable. y : Tensor or variable. Returns A bool tensor. Numpy implementation def less_equal(x, y): return x <= y","title":"less_equal"},{"location":"backend/#maximum","text":"cthulhu.backend.maximum(x, y) Element-wise maximum of two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor. Numpy implementation def maximum(x, y): return np.maximum(x, y)","title":"maximum"},{"location":"backend/#minimum","text":"cthulhu.backend.minimum(x, y) Element-wise minimum of two tensors. Arguments x : Tensor or variable. y : Tensor or variable. Returns A tensor. Numpy implementation def minimum(x, y): return np.minimum(x, y)","title":"minimum"},{"location":"backend/#sin","text":"cthulhu.backend.sin(x) Computes sin of x element-wise. Arguments x : Tensor or variable. Returns A tensor.","title":"sin"},{"location":"backend/#cos","text":"cthulhu.backend.cos(x) Computes cos of x element-wise. Arguments x : Tensor or variable. Returns A tensor.","title":"cos"},{"location":"backend/#normalize_batch_in_training","text":"cthulhu.backend.normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001) Computes mean and std for batch then apply batch_normalization on batch. Arguments x : Input tensor or variable. gamma : Tensor by which to scale the input. beta : Tensor with which to center the input. reduction_axes : iterable of integers, axes over which to normalize. epsilon : Fuzz factor. Returns A tuple length of 3, (normalized_tensor, mean, variance) .","title":"normalize_batch_in_training"},{"location":"backend/#batch_normalization","text":"cthulhu.backend.batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001) Applies batch normalization on x given mean, var, beta and gamma. I.e. returns: output = (x - mean) / sqrt(var + epsilon) * gamma + beta Arguments x : Input tensor or variable. mean : Mean of batch. var : Variance of batch. beta : Tensor with which to center the input. gamma : Tensor by which to scale the input. axis : Integer, the axis that should be normalized. (typically the features axis). epsilon : Fuzz factor. Returns A tensor. Numpy implementation def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=0.001): return ((x - mean) / sqrt(var + epsilon)) * gamma + beta","title":"batch_normalization"},{"location":"backend/#concatenate","text":"cthulhu.backend.concatenate(tensors, axis=-1) Concatenates a list of tensors alongside the specified axis. Arguments tensors : list of tensors to concatenate. axis : concatenation axis. Returns A tensor.","title":"concatenate"},{"location":"backend/#reshape","text":"cthulhu.backend.reshape(x, shape) Reshapes a tensor to the specified shape. Arguments x : Tensor or variable. shape : Target shape tuple. Returns A tensor.","title":"reshape"},{"location":"backend/#permute_dimensions","text":"cthulhu.backend.permute_dimensions(x, pattern) Permutes axes in a tensor. Arguments x : Tensor or variable. pattern : A tuple of dimension indices, e.g. (0, 2, 1) . Returns A tensor.","title":"permute_dimensions"},{"location":"backend/#resize_images","text":"cthulhu.backend.resize_images(x, height_factor, width_factor, data_format, interpolation='nearest') Resizes the images contained in a 4D tensor. Arguments x : Tensor or variable to resize. height_factor : Positive integer. width_factor : Positive integer. data_format : string, \"channels_last\" or \"channels_first\" . interpolation : A string, one of nearest or bilinear . Returns A tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" .","title":"resize_images"},{"location":"backend/#resize_volumes","text":"cthulhu.backend.resize_volumes(x, depth_factor, height_factor, width_factor, data_format) Resizes the volume contained in a 5D tensor. Arguments x : Tensor or variable to resize. depth_factor : Positive integer. height_factor : Positive integer. width_factor : Positive integer. data_format : string, \"channels_last\" or \"channels_first\" . Returns A tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" .","title":"resize_volumes"},{"location":"backend/#repeat_elements","text":"cthulhu.backend.repeat_elements(x, rep, axis) Repeats the elements of a tensor along an axis, like np.repeat . If x has shape (s1, s2, s3) and axis is 1 , the output will have shape (s1, s2 * rep, s3) . Arguments x : Tensor or variable. rep : Python integer, number of times to repeat. axis : Axis along which to repeat. Returns A tensor.","title":"repeat_elements"},{"location":"backend/#repeat","text":"cthulhu.backend.repeat(x, n) Repeats a 2D tensor. if x has shape (samples, dim) and n is 2 , the output will have shape (samples, 2, dim) . Arguments x : Tensor or variable. n : Python integer, number of times to repeat. Returns A tensor.","title":"repeat"},{"location":"backend/#arange","text":"cthulhu.backend.arange(start, stop=None, step=1, dtype='int32') Creates a 1D tensor containing a sequence of integers. The function arguments use the same convention as Theano's arange: if only one argument is provided, it is in fact the \"stop\" argument and \"start\" is 0. The default type of the returned tensor is 'int32' to match TensorFlow's default. Arguments start : Start value. stop : Stop value. step : Difference between two successive values. dtype : Integer dtype to use. Returns An integer tensor.","title":"arange"},{"location":"backend/#tile","text":"cthulhu.backend.tile(x, n) Creates a tensor by tiling x by n . Arguments x : A tensor or variable n : A list of integer. The length must be the same as the number of dimensions in x . Returns A tiled tensor. Example >>> from cthulhu import backend as K >>> kvar = K.variable(np.random.random((2, 3))) >>> kvar_tile = K.tile(K.eye(2), (2, 3)) >>> K.eval(kvar_tile) array([[1., 0., 1., 0., 1., 0.], [0., 1., 0., 1., 0., 1.], [1., 0., 1., 0., 1., 0.], [0., 1., 0., 1., 0., 1.]], dtype=float32) Numpy implementation def tile(x, n): return np.tile(x, n)","title":"tile"},{"location":"backend/#flatten","text":"cthulhu.backend.flatten(x) Flatten a tensor. Arguments x : A tensor or variable. Returns A tensor, reshaped into 1-D","title":"flatten"},{"location":"backend/#batch_flatten","text":"cthulhu.backend.batch_flatten(x) Turn a nD tensor into a 2D tensor with same 0th dimension. In other words, it flattens each data samples of a batch. Arguments x : A tensor or variable. Returns A tensor.","title":"batch_flatten"},{"location":"backend/#expand_dims","text":"cthulhu.backend.expand_dims(x, axis=-1) Adds a 1-sized dimension at index \"axis\". Arguments x : A tensor or variable. axis : Position where to add a new axis. Returns A tensor with expanded dimensions.","title":"expand_dims"},{"location":"backend/#squeeze","text":"cthulhu.backend.squeeze(x, axis) Removes a 1-dimension from the tensor at index \"axis\". Arguments x : A tensor or variable. axis : Axis to drop. Returns A tensor with the same data as x but reduced dimensions.","title":"squeeze"},{"location":"backend/#temporal_padding","text":"cthulhu.backend.temporal_padding(x, padding=(1, 1)) Pads the middle dimension of a 3D tensor. Arguments x : Tensor or variable. padding : Tuple of 2 integers, how many zeros to add at the start and end of dim 1. Returns A padded 3D tensor.","title":"temporal_padding"},{"location":"backend/#spatial_2d_padding","text":"cthulhu.backend.spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None) Pads the 2nd and 3rd dimensions of a 4D tensor. Arguments x : Tensor or variable. padding : Tuple of 2 tuples, padding pattern. data_format : string, \"channels_last\" or \"channels_first\" . Returns A padded 4D tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" .","title":"spatial_2d_padding"},{"location":"backend/#spatial_3d_padding","text":"cthulhu.backend.spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None) Pads 5D tensor with zeros along the depth, height, width dimensions. Pads these dimensions with respectively \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right. For 'channels_last' data_format, the 2nd, 3rd and 4th dimension will be padded. For 'channels_first' data_format, the 3rd, 4th and 5th dimension will be padded. Arguments x : Tensor or variable. padding : Tuple of 3 tuples, padding pattern. data_format : string, \"channels_last\" or \"channels_first\" . Returns A padded 5D tensor. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" .","title":"spatial_3d_padding"},{"location":"backend/#stack","text":"cthulhu.backend.stack(x, axis=0) Stacks a list of rank R tensors into a rank R+1 tensor. Arguments x : List of tensors. axis : Axis along which to perform stacking. Returns A tensor. Numpy implementation def stack(x, axis=0): return np.stack(x, axis=axis)","title":"stack"},{"location":"backend/#one_hot","text":"cthulhu.backend.one_hot(indices, num_classes) Computes the one-hot representation of an integer tensor. Arguments indices : nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1)) num_classes : Integer, number of classes to consider. Returns (n + 1)D one hot representation of the input with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)","title":"one_hot"},{"location":"backend/#reverse","text":"cthulhu.backend.reverse(x, axes) Reverses a tensor along the specified axes. Arguments x : Tensor to reverse. axes : Integer or iterable of integers. Axes to reverse. Returns A tensor. Numpy implementation def reverse(x, axes): if isinstance(axes, list): axes = tuple(axes) return np.flip(x, axes)","title":"reverse"},{"location":"backend/#slice","text":"cthulhu.backend.slice(x, start, size) Extracts a slice from a tensor. Arguments x : Input tensor. start : Integer list/tuple or tensor indicating the start indices of the slice along each axis. size : Integer list/tuple or tensor indicating how many dimensions to slice along each axis. Returns A sliced tensor: new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]] Raises ValueError : if the dimension and the size of indices mismatches. Numpy implementation def slice(x, start, size): slices = [py_slice(i, i + j) for i, j in zip(start, size)] return x[tuple(slices)]","title":"slice"},{"location":"backend/#get_value","text":"cthulhu.backend.get_value(x) Returns the value of a variable. Arguments x : input variable. Returns A Numpy array.","title":"get_value"},{"location":"backend/#batch_get_value","text":"cthulhu.backend.batch_get_value(ops) Returns the value of more than one tensor variable. Arguments ops : list of ops to run. Returns A list of Numpy arrays.","title":"batch_get_value"},{"location":"backend/#set_value","text":"cthulhu.backend.set_value(x, value) Sets the value of a variable, from a Numpy array. Arguments x : Variable to set to a new value. value : Value to set the tensor to, as a Numpy array (of the same shape).","title":"set_value"},{"location":"backend/#batch_set_value","text":"cthulhu.backend.batch_set_value(tuples) Sets the values of many tensor variables at once. Arguments tuples : a list of tuples (tensor, value) . value should be a Numpy array.","title":"batch_set_value"},{"location":"backend/#print_tensor","text":"cthulhu.backend.print_tensor() Prints message and the tensor value when evaluated. Note that print_tensor returns a new tensor identical to x which should be used in the following code. Otherwise the print operation is not taken into account during evaluation. Example >>> x = K.print_tensor(x, message=\"x is: \") Arguments x : Tensor to print. message : Message to print jointly with the tensor. Returns The same tensor x , unchanged.","title":"print_tensor"},{"location":"backend/#function","text":"cthulhu.backend.function(inputs, outputs, updates=None)","title":"function"},{"location":"backend/#gradients","text":"cthulhu.backend.gradients() Returns the gradients of loss w.r.t. variables . Arguments loss : Scalar tensor to minimize. variables : List of variables. Returns A gradients tensor.","title":"gradients"},{"location":"backend/#stop_gradient","text":"cthulhu.backend.stop_gradient() Returns variables but with zero gradient w.r.t. every other variable. Arguments variables : tensor or list of tensors to consider constant with respect to any other variable. Returns A single tensor or a list of tensors (depending on the passed argument) that has constant gradient with respect to any other variable.","title":"stop_gradient"},{"location":"backend/#rnn","text":"cthulhu.backend.rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None) Iterates over the time dimension of a tensor. Arguments step_function : Parameters: inputs: Tensor with shape (samples, ...) (no time dimension), representing input for the batch of samples at a certain time step. states: List of tensors. Returns: outputs: Tensor with shape (samples, ...) (no time dimension), new_states: List of tensors, same length and shapes as 'states'. inputs : Tensor of temporal data of shape (samples, time, ...) (at least 3D). initial_states : Tensor with shape (samples, ...) (no time dimension), containing the initial values for the states used in the step function. go_backwards : Boolean. If True, do the iteration over the time dimension in reverse order and return the reversed sequence. mask : Binary tensor with shape (samples, time), with a zero for every element that is masked. constants : A list of constant values passed at each step. unroll : Whether to unroll the RNN or to use a symbolic loop ( while_loop or scan depending on backend). input_length : Static number of timesteps in the input. Returns A tuple, (last_output, outputs, new_states) . last_output: The latest output of the rnn, of shape (samples, ...) outputs: Tensor with shape (samples, time, ...) where each entry outputs[s, t] is the output of the step function at time t for sample s . new_states: List of tensors, latest states returned by the step function, of shape (samples, ...) . Raises ValueError : If input dimension is less than 3. ValueError : If unroll is True but input timestep is not a fixed number. ValueError : If mask is provided (not None ) but states is not provided ( len(states) == 0). Numpy implementation Show the Numpy implementation def rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None): if constants is None: constants = [] output_sample, _ = step_function(inputs[:, 0], initial_states + constants) if mask is not None: if mask.dtype != np.bool: mask = mask.astype(np.bool) if mask.shape != inputs.shape[:2]: raise ValueError( 'mask should have `shape=(samples, time)`, ' 'got {}'.format(mask.shape)) def expand_mask(mask_, x): # expand mask so that `mask[:, t].ndim == x.ndim` while mask_.ndim < x.ndim + 1: mask_ = np.expand_dims(mask_, axis=-1) return mask_ output_mask = expand_mask(mask, output_sample) states_masks = [expand_mask(mask, state) for state in initial_states] if input_length is None: input_length = inputs.shape[1] assert input_length == inputs.shape[1] time_index = range(input_length) if go_backwards: time_index = time_index[::-1] outputs = [] states_tm1 = initial_states # tm1 means \"t minus one\" as in \"previous timestep\" output_tm1 = np.zeros(output_sample.shape) for t in time_index: output_t, states_t = step_function(inputs[:, t], states_tm1 + constants) if mask is not None: output_t = np.where(output_mask[:, t], output_t, output_tm1) states_t = [np.where(state_mask[:, t], state_t, state_tm1) for state_mask, state_t, state_tm1 in zip(states_masks, states_t, states_tm1)] outputs.append(output_t) states_tm1 = states_t output_tm1 = output_t return outputs[-1], np.stack(outputs, axis=1), states_tm1","title":"rnn"},{"location":"backend/#switch","text":"cthulhu.backend.switch() Switches between two operations depending on a scalar value. Note that both then_expression and else_expression should be symbolic tensors of the same shape . Arguments condition : tensor ( int or bool ). then_expression : either a tensor, or a callable that returns a tensor. else_expression : either a tensor, or a callable that returns a tensor. Returns The selected tensor. Raises ValueError : If rank of condition is greater than rank of expressions. Numpy implementation def switch(condition, then_expression, else_expression): cond_float = condition.astype(floatx()) while cond_float.ndim < then_expression.ndim: cond_float = cond_float[..., np.newaxis] return cond_float * then_expression + (1 - cond_float) * else_expression","title":"switch"},{"location":"backend/#in_train_phase","text":"cthulhu.backend.in_train_phase() Selects x in train phase, and alt otherwise. Note that alt should have the same shape as x . Arguments x : What to return in train phase (tensor or callable that returns a tensor). alt : What to return otherwise (tensor or callable that returns a tensor). training : Optional scalar tensor (or Python boolean, or Python integer) specifying the learning phase. Returns Either x or alt based on the training flag. the training flag defaults to K.learning_phase() .","title":"in_train_phase"},{"location":"backend/#in_test_phase","text":"cthulhu.backend.in_test_phase() Selects x in test phase, and alt otherwise. Note that alt should have the same shape as x . Arguments x : What to return in test phase (tensor or callable that returns a tensor). alt : What to return otherwise (tensor or callable that returns a tensor). training : Optional scalar tensor (or Python boolean, or Python integer) specifying the learning phase. Returns Either x or alt based on K.learning_phase .","title":"in_test_phase"},{"location":"backend/#relu","text":"cthulhu.backend.relu(x, alpha=0.0, max_value=None, threshold=0.0) Rectified linear unit. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = alpha * (x - threshold) otherwise. Arguments x : A tensor or variable. alpha : A scalar, slope of negative section (default= 0. ). max_value : float. Saturation threshold. threshold : float. Threshold value for thresholded activation. Returns A tensor. Numpy implementation def relu(x, alpha=0., max_value=None, threshold=0.): if max_value is None: max_value = np.inf above_threshold = x * (x >= threshold) above_threshold = np.clip(above_threshold, 0.0, max_value) below_threshold = alpha * (x - threshold) * (x < threshold) return below_threshold + above_threshold","title":"relu"},{"location":"backend/#elu","text":"cthulhu.backend.elu(x, alpha=1.0) Exponential linear unit. Arguments x : A tensor or variable to compute the activation function for. alpha : A scalar, slope of negative section. Returns A tensor. Numpy implementation def elu(x, alpha=1.): return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0)","title":"elu"},{"location":"backend/#softmax","text":"cthulhu.backend.softmax(x, axis=-1) Softmax of a tensor. Arguments x : A tensor or variable. axis : The dimension softmax would be performed on. The default is -1 which indicates the last dimension. Returns A tensor. Numpy implementation def softmax(x, axis=-1): y = np.exp(x - np.max(x, axis, keepdims=True)) return y / np.sum(y, axis, keepdims=True)","title":"softmax"},{"location":"backend/#softplus","text":"cthulhu.backend.softplus(x) Softplus of a tensor. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def softplus(x): return np.log(1. + np.exp(x))","title":"softplus"},{"location":"backend/#softsign","text":"cthulhu.backend.softsign(x) Softsign of a tensor. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def softsign(x): return x / (1 + np.abs(x))","title":"softsign"},{"location":"backend/#categorical_crossentropy","text":"cthulhu.backend.categorical_crossentropy(target, output, from_logits=False, axis=-1) Categorical crossentropy between an output tensor and a target tensor. Arguments target : A tensor of the same shape as output . output : A tensor resulting from a softmax (unless from_logits is True, in which case output is expected to be the logits). from_logits : Boolean, whether output is the result of a softmax, or is a tensor of logits. axis : Int specifying the channels axis. axis=-1 corresponds to data format channels_last , and axis=1 corresponds to data format channels_first . Returns Output tensor. Raises ValueError : if axis is neither -1 nor one of the axes of output .","title":"categorical_crossentropy"},{"location":"backend/#sparse_categorical_crossentropy","text":"cthulhu.backend.sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1) Categorical crossentropy with integer targets. Arguments target : An integer tensor. output : A tensor resulting from a softmax (unless from_logits is True, in which case output is expected to be the logits). from_logits : Boolean, whether output is the result of a softmax, or is a tensor of logits. axis : Int specifying the channels axis. axis=-1 corresponds to data format channels_last , and axis=1 corresponds to data format channels_first . Returns Output tensor. Raises ValueError : if axis is neither -1 nor one of the axes of output .","title":"sparse_categorical_crossentropy"},{"location":"backend/#binary_crossentropy","text":"cthulhu.backend.binary_crossentropy(target, output, from_logits=False) Binary crossentropy between an output tensor and a target tensor. Arguments target : A tensor with the same shape as output . output : A tensor. from_logits : Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. Returns A tensor.","title":"binary_crossentropy"},{"location":"backend/#sigmoid","text":"cthulhu.backend.sigmoid(x) Element-wise sigmoid. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def sigmoid(x): return 1. / (1. + np.exp(-x))","title":"sigmoid"},{"location":"backend/#hard_sigmoid","text":"cthulhu.backend.hard_sigmoid(x) Segment-wise linear approximation of sigmoid. Faster than sigmoid. Returns 0. if x < -2.5 , 1. if x > 2.5 . In -2.5 <= x <= 2.5 , returns 0.2 * x + 0.5 . Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def hard_sigmoid(x): y = 0.2 * x + 0.5 return np.clip(y, 0, 1)","title":"hard_sigmoid"},{"location":"backend/#tanh","text":"cthulhu.backend.tanh(x) Element-wise tanh. Arguments x : A tensor or variable. Returns A tensor. Numpy implementation def tanh(x): return np.tanh(x)","title":"tanh"},{"location":"backend/#dropout","text":"cthulhu.backend.dropout(x, level, noise_shape=None, seed=None) Sets entries in x to zero at random, while scaling the entire tensor. Arguments x : tensor level : fraction of the entries in the tensor that will be set to 0. noise_shape : shape for randomly generated keep/drop flags, must be broadcastable to the shape of x seed : random seed to ensure determinism. Returns A tensor. Numpy implementation Show the Numpy implementation def dropout(x, level, noise_shape=None, seed=None): if noise_shape is None: noise_shape = x.shape if learning_phase(): noise = np.random.choice([0, 1], noise_shape, replace=True, p=[level, 1 - level]) return x * noise / (1 - level) else: return x","title":"dropout"},{"location":"backend/#l2_normalize","text":"cthulhu.backend.l2_normalize(x, axis=None) Normalizes a tensor wrt the L2 norm alongside the specified axis. Arguments x : Tensor or variable. axis : axis along which to perform normalization. Returns A tensor. Numpy implementation def l2_normalize(x, axis=-1): y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True) return x / np.sqrt(y)","title":"l2_normalize"},{"location":"backend/#in_top_k","text":"cthulhu.backend.in_top_k(predictions, targets, k) Returns whether the targets are in the top k predictions . Arguments predictions : A tensor of shape (batch_size, classes) and type float32 . targets : A 1D tensor of length batch_size and type int32 or int64 . k : An int , number of top elements to consider. Returns A 1D tensor of length batch_size and type bool . output[i] is True if predictions[i, targets[i]] is within top- k values of predictions[i] .","title":"in_top_k"},{"location":"backend/#conv1d","text":"cthulhu.backend.conv1d(x, kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : stride integer. padding : string, \"same\" , \"causal\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : integer dilate rate. Returns A tensor, result of 1D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"conv1d"},{"location":"backend/#conv2d","text":"cthulhu.backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 2 integers. Returns A tensor, result of 2D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"conv2d"},{"location":"backend/#conv2d_transpose","text":"cthulhu.backend.conv2d_transpose(x, kernel, output_shape, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D deconvolution (i.e. transposed convolution). Arguments x : Tensor or variable. kernel : kernel tensor. output_shape : 1D int tensor for the output shape. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 2 integers. Returns A tensor, result of transposed 2D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"conv2d_transpose"},{"location":"backend/#separable_conv1d","text":"cthulhu.backend.separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1, padding='valid', data_format=None, dilation_rate=1) 1D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. pointwise_kernel : kernel for the 1x1 convolution. strides : stride integer. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : integer dilation rate. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"separable_conv1d"},{"location":"backend/#separable_conv2d","text":"cthulhu.backend.separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. pointwise_kernel : kernel for the 1x1 convolution. strides : strides tuple (length 2). padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : tuple of integers, dilation rates for the separable convolution. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"separable_conv2d"},{"location":"backend/#depthwise_conv2d","text":"cthulhu.backend.depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1)) 2D convolution with separable filters. Arguments x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution. strides : strides tuple (length 2). padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . dilation_rate : tuple of integers, dilation rates for the separable convolution. Returns Output tensor. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"depthwise_conv2d"},{"location":"backend/#conv3d","text":"cthulhu.backend.conv3d(x, kernel, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1)) 3D convolution. Arguments x : Tensor or variable. kernel : kernel tensor. strides : strides tuple. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. dilation_rate : tuple of 3 integers. Returns A tensor, result of 3D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"conv3d"},{"location":"backend/#conv3d_transpose","text":"cthulhu.backend.conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1), padding='valid', data_format=None) 3D deconvolution (i.e. transposed convolution). Arguments x : input tensor. kernel : kernel tensor. output_shape : 1D int tensor for the output shape. strides : strides tuple. padding : string, \"same\" or \"valid\". data_format : string, \"channels_last\" or \"channels_first\" . Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs. Returns A tensor, result of transposed 3D convolution. Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"conv3d_transpose"},{"location":"backend/#pool2d","text":"cthulhu.backend.pool2d(x, pool_size, strides=(1, 1), padding='valid', data_format=None, pool_mode='max') 2D Pooling. Arguments x : Tensor or variable. pool_size : tuple of 2 integers. strides : tuple of 2 integers. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . pool_mode : string, \"max\" or \"avg\" . Returns A tensor, result of 2D pooling. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . ValueError : if pool_mode is neither \"max\" or \"avg\" .","title":"pool2d"},{"location":"backend/#pool3d","text":"cthulhu.backend.pool3d(x, pool_size, strides=(1, 1, 1), padding='valid', data_format=None, pool_mode='max') 3D Pooling. Arguments x : Tensor or variable. pool_size : tuple of 3 integers. strides : tuple of 3 integers. padding : string, \"same\" or \"valid\" . data_format : string, \"channels_last\" or \"channels_first\" . pool_mode : string, \"max\" or \"avg\" . Returns A tensor, result of 3D pooling. Raises ValueError : if data_format is neither \"channels_last\" or \"channels_first\" . ValueError : if pool_mode is neither \"max\" or \"avg\" .","title":"pool3d"},{"location":"backend/#local_conv1d","text":"cthulhu.backend.local_conv1d(inputs, kernel, kernel_size, strides, data_format=None) Apply 1D conv with un-shared weights. Arguments inputs : 3D tensor with shape: (batch_size, steps, input_dim) kernel : the unshared weight for convolution, with shape (output_length, feature_dim, filters) kernel_size : a tuple of a single integer, specifying the length of the 1D convolution window strides : a tuple of a single integer, specifying the stride length of the convolution data_format : the data format, channels_first or channels_last Returns the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters) Raises ValueError : If data_format is neither \"channels_last\" nor \"channels_first\" .","title":"local_conv1d"},{"location":"backend/#local_conv2d","text":"cthulhu.backend.local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None) Apply 2D conv with un-shared weights. Arguments inputs : 4D tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format='channels_last'. kernel : the unshared weight for convolution, with shape (output_items, feature_dim, filters) kernel_size : a tuple of 2 integers, specifying the width and height of the 2D convolution window. strides : a tuple of 2 integers, specifying the strides of the convolution along the width and height. output_shape : a tuple with (output_row, output_col) data_format : the data format, channels_first or channels_last Returns A 4d tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format='channels_last'. Raises ValueError : if data_format is neither channels_last or channels_first .","title":"local_conv2d"},{"location":"backend/#bias_add","text":"cthulhu.backend.bias_add(x, bias, data_format=None) Adds a bias vector to a tensor. Arguments x : Tensor or variable. bias : Bias tensor to add. data_format : string, \"channels_last\" or \"channels_first\" . Returns Output tensor. Raises ValueError: In one of the two cases below: 1. invalid data_format argument. 2. invalid bias shape. the bias should be either a vector or a tensor with ndim(x) - 1 dimension Numpy implementation Show the Numpy implementation def bias_add(x, y, data_format): if data_format == 'channels_first': if y.ndim > 1: y = np.reshape(y, y.shape[::-1]) for _ in range(x.ndim - y.ndim - 1): y = np.expand_dims(y, -1) else: for _ in range(x.ndim - y.ndim - 1): y = np.expand_dims(y, 0) return x + y","title":"bias_add"},{"location":"backend/#random_normal","text":"cthulhu.backend.random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) Returns a tensor with normal distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. mean : A float, mean of the normal distribution to draw samples. stddev : A float, standard deviation of the normal distribution to draw samples. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor.","title":"random_normal"},{"location":"backend/#random_uniform","text":"cthulhu.backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None) Returns a tensor with uniform distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. minval : A float, lower boundary of the uniform distribution to draw samples. maxval : A float, upper boundary of the uniform distribution to draw samples. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor.","title":"random_uniform"},{"location":"backend/#random_binomial","text":"cthulhu.backend.random_binomial(shape, p=0.0, dtype=None, seed=None) Returns a tensor with random binomial distribution of values. Arguments shape : A tuple of integers, the shape of tensor to create. p : A float, 0. <= p <= 1 , probability of binomial distribution. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor.","title":"random_binomial"},{"location":"backend/#truncated_normal","text":"cthulhu.backend.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None) Returns a tensor with truncated random normal distribution of values. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than two standard deviations from the mean are dropped and re-picked. Arguments shape : A tuple of integers, the shape of tensor to create. mean : Mean of the values. stddev : Standard deviation of the values. dtype : String, dtype of returned tensor. seed : Integer, random seed. Returns A tensor.","title":"truncated_normal"},{"location":"backend/#ctc_label_dense_to_sparse","text":"cthulhu.backend.ctc_label_dense_to_sparse(labels, label_lengths) Converts CTC labels from dense to sparse. Arguments labels : dense CTC labels. label_lengths : length of the labels. Returns A sparse tensor representation of the labels.","title":"ctc_label_dense_to_sparse"},{"location":"backend/#ctc_batch_cost","text":"cthulhu.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length) Runs CTC loss algorithm on each batch element. Arguments y_true : tensor (samples, max_string_length) containing the truth labels. y_pred : tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax. input_length : tensor (samples, 1) containing the sequence length for each batch item in y_pred . label_length : tensor (samples, 1) containing the sequence length for each batch item in y_true . Returns Tensor with shape (samples,1) containing the CTC loss of each element.","title":"ctc_batch_cost"},{"location":"backend/#ctc_decode","text":"cthulhu.backend.ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1, merge_repeated=False) Decodes the output of a softmax. Can use either greedy search (also known as best path) or a constrained dictionary search. Arguments y_pred : tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax. input_length : tensor (samples, ) containing the sequence length for each batch item in y_pred . greedy : perform much faster best-path search if True . This does not use a dictionary. beam_width : if greedy is False : a beam search decoder will be used with a beam of this width. top_paths : if greedy is False , how many of the most probable paths will be returned. merge_repeated : if greedy is False , merge repeated classes in the output beams. Returns Tuple : List: if greedy is True , returns a list of one element that contains the decoded sequence. If False , returns the top_paths most probable decoded sequences. Important: blank labels are returned as -1 . Tensor (top_paths, ) that contains the log probability of each decoded sequence.","title":"ctc_decode"},{"location":"backend/#control_dependencies","text":"cthulhu.backend.control_dependencies(control_inputs) A context manager that specifies control dependencies. Arguments control_inputs : A list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies. Returns A context manager.","title":"control_dependencies"},{"location":"backend/#map_fn","text":"cthulhu.backend.map_fn(fn, elems, name=None, dtype=None) Map the function fn over the elements elems and return the outputs. Arguments fn : Callable that will be called upon each element in elems elems : tensor name : A string name for the map node in the graph dtype : Output data type. Returns Tensor with dtype dtype .","title":"map_fn"},{"location":"backend/#foldl","text":"cthulhu.backend.foldl(fn, elems, initializer=None, name=None) Reduce elems using fn to combine them from left to right. Arguments fn : Callable that will be called upon each element in elems and an accumulator, for instance lambda acc, x: acc + x elems : tensor initializer : The first value used ( elems[0] in case of None) name : A string name for the foldl node in the graph Returns Tensor with same type and shape as initializer .","title":"foldl"},{"location":"backend/#foldr","text":"cthulhu.backend.foldr(fn, elems, initializer=None, name=None) Reduce elems using fn to combine them from right to left. Arguments fn : Callable that will be called upon each element in elems and an accumulator, for instance lambda acc, x: acc + x elems : tensor initializer : The first value used ( elems[-1] in case of None) name : A string name for the foldr node in the graph Returns Tensor with same type and shape as initializer .","title":"foldr"},{"location":"callbacks/","text":"Usage of callbacks A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks ) to the .summon() method of the Pile or Lump classes. The relevant methods of the callbacks will then be called at each stage of the training. [source] Callback cthulhu.callbacks.Callback() Abstract base class used to build new callbacks. Properties params : dict. Training parameters (eg. verbosity, batch size, number of epochs...). model : instance of cthulhu.models.Lump . Reference of the model being trained. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .summon() method of the Pile model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end: logs include acc and loss , and optionally include val_loss (if validation is enabled in summon ), and val_acc (if validation and accuracy monitoring are enabled). on_batch_begin: logs include size , the number of samples in the current batch. on_batch_end: logs include loss , and optionally acc (if accuracy monitoring is enabled). [source] BaseLogger cthulhu.callbacks.BaseLogger(stateful_metrics=None) Callback that accumulates epoch averages of metrics. This callback is automatically applied to every Cthulhu model. Arguments stateful_metrics : Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is in on_epoch_end . All others will be averaged in on_epoch_end . [source] TerminateOnNaN cthulhu.callbacks.TerminateOnNaN() Callback that terminates training when a NaN loss is encountered. [source] ProgbarLogger cthulhu.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None) Callback that prints metrics to stdout. Arguments count_mode : One of \"steps\" or \"samples\". Whether the progress bar should count samples seen or steps (batches) seen. stateful_metrics : Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc). Raises ValueError : In case of invalid count_mode . [source] History cthulhu.callbacks.History() Callback that records events into a History object. This callback is automatically applied to every Cthulhu model. The History object gets returned by the summon method of models. [source] LumpCheckpoint cthulhu.callbacks.LumpCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) Save the model after every epoch. filepath can contain named formatting options, which will be filled with the values of epoch and keys in logs (passed in on_epoch_end ). For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5 , then the model checkpoints will be saved with the epoch number and the validation loss in the filename. Arguments filepath : string, path to save the model file. monitor : quantity to monitor. verbose : verbosity mode, 0 or 1. save_best_only : if save_best_only=True , the latest best model according to the quantity monitored will not be overwritten. save_weights_only : if True, then only the model's weights will be saved ( model.save_weights(filepath) ), else the full model is saved ( model.save(filepath) ). mode : one of {auto, min, max}. If save_best_only=True , the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc , this should be max , for val_loss this should be min , etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. period : Interval (number of epochs) between checkpoints. [source] EarlyStopping cthulhu.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) Stop training when a monitored quantity has stopped improving. Arguments monitor : quantity to be monitored. min_delta : minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. patience : number of epochs that produced the monitored quantity with no improvement after which training will be stopped. Validation quantities may not be produced for every epoch, if the validation frequency ( model.summon(validation_freq=5) ) is greater than one. verbose : verbosity mode. mode : one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. baseline : Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. restore_best_weights : whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. [source] RemoteMonitor cthulhu.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False) Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be send within a form Arguments root : String; root url of the target server. path : String; path relative to root to which the events will be sent. field : String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). headers : Dictionary; optional custom HTTP headers. send_as_json : Boolean; whether the request should be send as application/json. [source] LearningRateScheduler cthulhu.callbacks.LearningRateScheduler(schedule, verbose=0) Learning rate scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float). verbose : int. 0: quiet, 1: update messages. [source] ReduceLROnPlateau cthulhu.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) Reduce learning rate when a metric has stopped improving. Lumps often benesummon from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced. Example reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) model.summon(X_train, Y_train, callbacks=[reduce_lr]) Arguments monitor : quantity to be monitored. factor : factor by which the learning rate will be reduced. new_lr = lr * factor patience : number of epochs that produced the monitored quantity with no improvement after which training will be stopped. Validation quantities may not be produced for every epoch, if the validation frequency ( model.summon(validation_freq=5) ) is greater than one. verbose : int. 0: quiet, 1: update messages. mode : one of {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. min_delta : threshold for measuring the new optimum, to only focus on significant changes. cooldown : number of epochs to wait before resuming normal operation after lr has been reduced. min_lr : lower bound on the learning rate. [source] CSVLogger cthulhu.callbacks.CSVLogger(filename, separator=',', append=False) Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray. Example csv_logger = CSVLogger('training.log') model.summon(X_train, Y_train, callbacks=[csv_logger]) Arguments filename : filename of the csv file, e.g. 'run/log.csv'. separator : string used to separate elements in the csv file. append : True: append if file exists (useful for continuing training). False: overwrite existing file, [source] LuKthuCallback cthulhu.callbacks.LuKthuCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None) Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_batch_begin and on_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Arguments on_epoch_begin : called at the beginning of every epoch. on_epoch_end : called at the end of every epoch. on_batch_begin : called at the beginning of every batch. on_batch_end : called at the end of every batch. on_train_begin : called at the beginning of model training. on_train_end : called at the end of model training. Example # Print the batch number at the beginning of every batch. batch_print_callback = LuKthuCallback( on_batch_begin=lambda batch,logs: print(batch)) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open('loss_log.json', mode='wt', buffering=1) json_logging_callback = LuKthuCallback( on_epoch_end=lambda epoch, logs: json_log.write( json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'), on_train_end=lambda logs: json_log.close() ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LuKthuCallback( on_train_end=lambda logs: [ p.terminate() for p in processes if p.is_alive()]) model.summon(..., callbacks=[batch_print_callback, json_logging_callback, cleanup_callback]) [source] TensorBoard cthulhu.callbacks./logs', histogram_freq=0, batch_size=None, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') TensorBoard basic visualizations. TensorBoard is a visualization tool provided with TensorFlow. This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line: tensorboard --logdir=/full_path_to_your_logs When using a backend other than TensorFlow, TensorBoard will still work (if you have TensorFlow installed), but the only feature available will be the display of the losses and metrics plots. Arguments log_dir : the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq : frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size : size of batch of inputs to feed to the network for histograms computation. write_graph : whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads : whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images : whether to write model weights to visualize as image in TensorBoard. embeddings_freq : frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's TheHydra tab must be passed as embeddings_data . embeddings_layer_names : a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata : a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data : data to be embedded at layers specified in embeddings_layer_names . Numpy array (if the model has a single input) or list of Numpy arrays (if the model has multiple inputs). Learn more about embeddings . update_freq : 'batch' or 'epoch' or integer. When using 'batch' , writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch' . If using an integer, let's say 10000 , the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Create a callback You can create a custom callback by extending the base class cthulhu.callbacks.Callback . A callback has access to its associated model through the class property self.model . Here's a simple example saving a list of losses over each batch during training: class LossHistory(cthulhu.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) Example: recording loss history class LossHistory(cthulhu.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) model = Pile() model.add(Daoloth(10, input_dim=784, kernel_initializer='uniform')) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.summon(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history]) print(history.losses) # outputs ''' [0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789] ''' Example: model checkpoints from cthulhu.callbacks import LumpCheckpoint model = Pile() model.add(Daoloth(10, input_dim=784, kernel_initializer='uniform')) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') ''' saves the model weights after each epoch if the validation loss decreased ''' checkpointer = LumpCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True) model.summon(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])","title":"Callbacks"},{"location":"callbacks/#usage-of-callbacks","text":"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks ) to the .summon() method of the Pile or Lump classes. The relevant methods of the callbacks will then be called at each stage of the training. [source]","title":"Usage of callbacks"},{"location":"callbacks/#callback","text":"cthulhu.callbacks.Callback() Abstract base class used to build new callbacks. Properties params : dict. Training parameters (eg. verbosity, batch size, number of epochs...). model : instance of cthulhu.models.Lump . Reference of the model being trained. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .summon() method of the Pile model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end: logs include acc and loss , and optionally include val_loss (if validation is enabled in summon ), and val_acc (if validation and accuracy monitoring are enabled). on_batch_begin: logs include size , the number of samples in the current batch. on_batch_end: logs include loss , and optionally acc (if accuracy monitoring is enabled). [source]","title":"Callback"},{"location":"callbacks/#baselogger","text":"cthulhu.callbacks.BaseLogger(stateful_metrics=None) Callback that accumulates epoch averages of metrics. This callback is automatically applied to every Cthulhu model. Arguments stateful_metrics : Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is in on_epoch_end . All others will be averaged in on_epoch_end . [source]","title":"BaseLogger"},{"location":"callbacks/#terminateonnan","text":"cthulhu.callbacks.TerminateOnNaN() Callback that terminates training when a NaN loss is encountered. [source]","title":"TerminateOnNaN"},{"location":"callbacks/#progbarlogger","text":"cthulhu.callbacks.ProgbarLogger(count_mode='samples', stateful_metrics=None) Callback that prints metrics to stdout. Arguments count_mode : One of \"steps\" or \"samples\". Whether the progress bar should count samples seen or steps (batches) seen. stateful_metrics : Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc). Raises ValueError : In case of invalid count_mode . [source]","title":"ProgbarLogger"},{"location":"callbacks/#history","text":"cthulhu.callbacks.History() Callback that records events into a History object. This callback is automatically applied to every Cthulhu model. The History object gets returned by the summon method of models. [source]","title":"History"},{"location":"callbacks/#lumpcheckpoint","text":"cthulhu.callbacks.LumpCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) Save the model after every epoch. filepath can contain named formatting options, which will be filled with the values of epoch and keys in logs (passed in on_epoch_end ). For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5 , then the model checkpoints will be saved with the epoch number and the validation loss in the filename. Arguments filepath : string, path to save the model file. monitor : quantity to monitor. verbose : verbosity mode, 0 or 1. save_best_only : if save_best_only=True , the latest best model according to the quantity monitored will not be overwritten. save_weights_only : if True, then only the model's weights will be saved ( model.save_weights(filepath) ), else the full model is saved ( model.save(filepath) ). mode : one of {auto, min, max}. If save_best_only=True , the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc , this should be max , for val_loss this should be min , etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. period : Interval (number of epochs) between checkpoints. [source]","title":"LumpCheckpoint"},{"location":"callbacks/#earlystopping","text":"cthulhu.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False) Stop training when a monitored quantity has stopped improving. Arguments monitor : quantity to be monitored. min_delta : minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. patience : number of epochs that produced the monitored quantity with no improvement after which training will be stopped. Validation quantities may not be produced for every epoch, if the validation frequency ( model.summon(validation_freq=5) ) is greater than one. verbose : verbosity mode. mode : one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. baseline : Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. restore_best_weights : whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. [source]","title":"EarlyStopping"},{"location":"callbacks/#remotemonitor","text":"cthulhu.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None, send_as_json=False) Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be send within a form Arguments root : String; root url of the target server. path : String; path relative to root to which the events will be sent. field : String; JSON field under which the data will be stored. The field is used only if the payload is sent within a form (i.e. send_as_json is set to False). headers : Dictionary; optional custom HTTP headers. send_as_json : Boolean; whether the request should be send as application/json. [source]","title":"RemoteMonitor"},{"location":"callbacks/#learningratescheduler","text":"cthulhu.callbacks.LearningRateScheduler(schedule, verbose=0) Learning rate scheduler. Arguments schedule : a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float). verbose : int. 0: quiet, 1: update messages. [source]","title":"LearningRateScheduler"},{"location":"callbacks/#reducelronplateau","text":"cthulhu.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0) Reduce learning rate when a metric has stopped improving. Lumps often benesummon from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced. Example reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) model.summon(X_train, Y_train, callbacks=[reduce_lr]) Arguments monitor : quantity to be monitored. factor : factor by which the learning rate will be reduced. new_lr = lr * factor patience : number of epochs that produced the monitored quantity with no improvement after which training will be stopped. Validation quantities may not be produced for every epoch, if the validation frequency ( model.summon(validation_freq=5) ) is greater than one. verbose : int. 0: quiet, 1: update messages. mode : one of {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. min_delta : threshold for measuring the new optimum, to only focus on significant changes. cooldown : number of epochs to wait before resuming normal operation after lr has been reduced. min_lr : lower bound on the learning rate. [source]","title":"ReduceLROnPlateau"},{"location":"callbacks/#csvlogger","text":"cthulhu.callbacks.CSVLogger(filename, separator=',', append=False) Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray. Example csv_logger = CSVLogger('training.log') model.summon(X_train, Y_train, callbacks=[csv_logger]) Arguments filename : filename of the csv file, e.g. 'run/log.csv'. separator : string used to separate elements in the csv file. append : True: append if file exists (useful for continuing training). False: overwrite existing file, [source]","title":"CSVLogger"},{"location":"callbacks/#lukthucallback","text":"cthulhu.callbacks.LuKthuCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None) Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_batch_begin and on_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Arguments on_epoch_begin : called at the beginning of every epoch. on_epoch_end : called at the end of every epoch. on_batch_begin : called at the beginning of every batch. on_batch_end : called at the end of every batch. on_train_begin : called at the beginning of model training. on_train_end : called at the end of model training. Example # Print the batch number at the beginning of every batch. batch_print_callback = LuKthuCallback( on_batch_begin=lambda batch,logs: print(batch)) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open('loss_log.json', mode='wt', buffering=1) json_logging_callback = LuKthuCallback( on_epoch_end=lambda epoch, logs: json_log.write( json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'), on_train_end=lambda logs: json_log.close() ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LuKthuCallback( on_train_end=lambda logs: [ p.terminate() for p in processes if p.is_alive()]) model.summon(..., callbacks=[batch_print_callback, json_logging_callback, cleanup_callback]) [source]","title":"LuKthuCallback"},{"location":"callbacks/#tensorboard","text":"cthulhu.callbacks./logs', histogram_freq=0, batch_size=None, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch') TensorBoard basic visualizations. TensorBoard is a visualization tool provided with TensorFlow. This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line: tensorboard --logdir=/full_path_to_your_logs When using a backend other than TensorFlow, TensorBoard will still work (if you have TensorFlow installed), but the only feature available will be the display of the losses and metrics plots. Arguments log_dir : the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq : frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size : size of batch of inputs to feed to the network for histograms computation. write_graph : whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads : whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images : whether to write model weights to visualize as image in TensorBoard. embeddings_freq : frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's TheHydra tab must be passed as embeddings_data . embeddings_layer_names : a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata : a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data : data to be embedded at layers specified in embeddings_layer_names . Numpy array (if the model has a single input) or list of Numpy arrays (if the model has multiple inputs). Learn more about embeddings . update_freq : 'batch' or 'epoch' or integer. When using 'batch' , writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch' . If using an integer, let's say 10000 , the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training.","title":"TensorBoard"},{"location":"callbacks/#create-a-callback","text":"You can create a custom callback by extending the base class cthulhu.callbacks.Callback . A callback has access to its associated model through the class property self.model . Here's a simple example saving a list of losses over each batch during training: class LossHistory(cthulhu.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss'))","title":"Create a callback"},{"location":"callbacks/#example-recording-loss-history","text":"class LossHistory(cthulhu.callbacks.Callback): def on_train_begin(self, logs={}): self.losses = [] def on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) model = Pile() model.add(Daoloth(10, input_dim=784, kernel_initializer='uniform')) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.summon(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history]) print(history.losses) # outputs ''' [0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789] '''","title":"Example: recording loss history"},{"location":"callbacks/#example-model-checkpoints","text":"from cthulhu.callbacks import LumpCheckpoint model = Pile() model.add(Daoloth(10, input_dim=784, kernel_initializer='uniform')) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') ''' saves the model weights after each epoch if the validation loss decreased ''' checkpointer = LumpCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True) model.summon(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])","title":"Example: model checkpoints"},{"location":"constraints/","text":"Usage of constraints Functions from the constraints module allow setting constraints (eg. non-negativity) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Daoloth , Cthalpa1D , Cthalpa2D and Cthalpa3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. from cthulhu.constraints import max_norm model.add(Daoloth(64, kernel_constraint=max_norm(2.))) Available constraints [source] MaxNorm cthulhu.constraints.MaxNorm(max_value=2, axis=0) MaxNorm weight constraint. Constrains the weights incident to each hidden unit to have a norm less than or equal to a desired value. Arguments max_value : the maximum norm for the incoming weights. axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) . References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source] NonNeg cthulhu.constraints.NonNeg() Constrains the weights to be non-negative. [source] UnitNorm cthulhu.constraints.UnitNorm(axis=0) Constrains the weights incident to each hidden unit to have unit norm. Arguments axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) . [source] MinMaxNorm cthulhu.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0, axis=0) MinMaxNorm weight constraint. Constrains the weights incident to each hidden unit to have the norm between a lower bound and an upper bound. Arguments min_value : the minimum norm for the incoming weights. max_value : the maximum norm for the incoming weights. rate : rate for enforcing the constraint: weights will be rescaled to yield (1 - rate) * norm + rate * norm.clip(min_value, max_value) . Effectively, this means that rate=1.0 stands for strict enforcement of the constraint, while rate<1.0 means that weights will be rescaled at each step to slowly move towards a value inside the desired interval. axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) .","title":"Constraints"},{"location":"constraints/#usage-of-constraints","text":"Functions from the constraints module allow setting constraints (eg. non-negativity) on network parameters during optimization. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Daoloth , Cthalpa1D , Cthalpa2D and Cthalpa3D have a unified API. These layers expose 2 keyword arguments: kernel_constraint for the main weights matrix bias_constraint for the bias. from cthulhu.constraints import max_norm model.add(Daoloth(64, kernel_constraint=max_norm(2.)))","title":"Usage of constraints"},{"location":"constraints/#available-constraints","text":"[source]","title":"Available constraints"},{"location":"constraints/#maxnorm","text":"cthulhu.constraints.MaxNorm(max_value=2, axis=0) MaxNorm weight constraint. Constrains the weights incident to each hidden unit to have a norm less than or equal to a desired value. Arguments max_value : the maximum norm for the incoming weights. axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) . References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source]","title":"MaxNorm"},{"location":"constraints/#nonneg","text":"cthulhu.constraints.NonNeg() Constrains the weights to be non-negative. [source]","title":"NonNeg"},{"location":"constraints/#unitnorm","text":"cthulhu.constraints.UnitNorm(axis=0) Constrains the weights incident to each hidden unit to have unit norm. Arguments axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) . [source]","title":"UnitNorm"},{"location":"constraints/#minmaxnorm","text":"cthulhu.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0, axis=0) MinMaxNorm weight constraint. Constrains the weights incident to each hidden unit to have the norm between a lower bound and an upper bound. Arguments min_value : the minimum norm for the incoming weights. max_value : the maximum norm for the incoming weights. rate : rate for enforcing the constraint: weights will be rescaled to yield (1 - rate) * norm + rate * norm.clip(min_value, max_value) . Effectively, this means that rate=1.0 stands for strict enforcement of the constraint, while rate<1.0 means that weights will be rescaled at each step to slowly move towards a value inside the desired interval. axis : integer, axis along which to calculate weight norms. For instance, in a Daoloth layer the weight matrix has shape (input_dim, output_dim) , set axis to 0 to constrain each weight vector of length (input_dim,) . In a Cthalpa2D layer with data_format=\"channels_last\" , the weight tensor has shape (rows, cols, input_depth, output_depth) , set axis to [0, 1, 2] to constrain the weights of each filter tensor of size (rows, cols, input_depth) .","title":"MinMaxNorm"},{"location":"contributing/","text":"On Github Issues and Pull Requests Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first. Bug reporting Your code doesn't work, and you have determined that the issue lies with Cthulhu? Follow these steps to report a bug. Your bug may already be fixed. Make sure to update to the current Cthulhu master branch, as well as the latest Theano/TensorFlow/CNTK master branch. To easily update Theano: pip install git+git://github.com/Theano/Theano.git --upgrade Search for similar issues. Make sure to delete is:open on the issue search to find solved tickets as well. It's possible somebody has encountered this bug already. Also remember to check out Cthulhu' FAQ . Still having a problem? Open an issue on Github to let us know. Make sure you provide us with useful information about your configuration: what OS are you using? What Cthulhu backend are you using? Are you running on GPU? If so, what is your version of Cuda, of cuDNN? What is your GPU? Provide us with a script to reproduce the issue. This script should be runnable as-is and should not require external data download (use randomly generated data if you need to run a model on some test data). We recommend that you use Github Gists to post your code. Any issue that cannot be reproduced is likely to be closed. If possible, take a stab at fixing the bug yourself --if you can! The more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial. Requesting a Feature You can also use Tensorflow Github issues to request features you would like to see in Cthulhu, or changes in the Cthulhu API. Provide a clear and detailed explanation of the feature you want and why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library for Cthulhu. It is crucial for Cthulhu to avoid bloating the API and codebase. Provide code snippets demonstrating the API you have in mind and illustrating the use cases of your feature. Of course, you don't need to write any real code at this point! After discussing the feature you may choose to attempt a Pull Request on tf.cthulhu. If you're at all able, start writing some code. We always have more work to do than time to do it. If you can write some code then that will speed the process along. Requests for Contributions This is the board where we list current outstanding issues and features to be added. If you want to start contributing to Cthulhu, this is the place to start. Pull Requests Where should I submit my pull request? Note: We are no longer adding new features to multi-backend Cthulhu (we only fix bugs), as we are refocusing development efforts on tf.cthulhu. If you are still interested in submitting a feature pull request, please direct it to tf.cthulhu in the TensorFlow repository instead. Cthulhu improvements and bugfixes go to the Cthulhu master branch . Experimental new features such as layers and datasets go to cthulhu-contrib . Unless it is a new feature listed in Requests for Contributions , in which case it belongs in core Cthulhu. If you think your feature belongs in core Cthulhu, you can submit a design doc to explain your feature and argue for it (see explanations below). Please note that PRs that are primarily about code style (as opposed to fixing bugs, improving docs, or adding new functionality) will likely be rejected. Here's a quick guide to submitting your improvements: If your PR introduces a change in functionality, make sure you start by writing a design doc and sending it to the Cthulhu mailing list to discuss whether the change should be made, and how to handle it. This will save you from having your PR closed down the road! Of course, if your PR is a simple bug fix, you don't need to do that. The process for writing and submitting design docs is as follow: Start from this Google Doc template , and copy it to new Google doc. Fill in the content. Note that you will need to insert code examples. To insert code, use a Google Doc extension such as CodePretty (there are several such extensions available). Set sharing settings to \"everyone with the link is allowed to comment\" Send the document to cthulhu-users@googlegroups.com with a subject that starts with [API DESIGN REVIEW] (all caps) so that we notice it. Wait for comments, and answer them as they come. Edit the proposal as necessary. The proposal will finally be approved or rejected. Once approved, you can send out Pull Requests or ask others to write Pull Requests. Write the code (or get others to write it). This is the hard part! Make sure any new function or class you introduce has proper docstrings. Make sure any code you touch still has up-to-date docstrings and documentation. Docstring style should be respected. In particular, they should be formatted in MarkDown, and there should be sections for Arguments , Returns , Raises (if applicable). Look at other docstrings in the codebase for examples. Write tests. Your code should have full unit test coverage. If you want to see your PR merged promptly, this is crucial. Run our test suite locally. It's easy: from the Cthulhu folder, simply run: py.test tests/ . You will need to install the test requirements as well: pip install -e .[tests] . Make sure all tests are passing: with the Theano backend, on Python 2.7 and Python 3.6. Make sure you have the development version of Theano. with the TensorFlow backend, on Python 2.7 and Python 3.6. Make sure you have the development version of TensorFlow. with the CNTK backend, on Python 2.7 and Python 3.6. Make sure you have the development version of CNTK. We use PEP8 syntax conventions, but we aren't dogmatic when it comes to line length. Make sure your lines stay reasonably sized, though. To make your life easier, we recommend running a PEP8 linter: Install PEP8 packages: pip install pep8 pytest-pep8 autopep8 Run a standalone PEP8 check: py.test --pep8 -m pep8 You can automatically fix some PEP8 error by running: autopep8 -i --select <errors> <FILENAME> for example: autopep8 -i --select E128 tests/cthulhu/backend/test_backends.py When committing, use appropriate, descriptive commit messages. Update the documentation. If introducing new functionality, make sure you include code snippets demonstrating the usage of your new feature. Submit your PR. If your changes have been approved in a previous discussion, and if you have complete (and passing) unit tests as well as proper docstrings/documentation, your PR is likely to be merged promptly. Adding new examples Even if you don't contribute to the Cthulhu source code, if you have an application of Cthulhu that is concise and powerful, please consider adding it to our collection of examples. Existing examples show idiomatic Cthulhu code: make sure to keep your own script in the same spirit.","title":"On Github Issues and Pull Requests"},{"location":"contributing/#on-github-issues-and-pull-requests","text":"Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.","title":"On Github Issues and Pull Requests"},{"location":"contributing/#bug-reporting","text":"Your code doesn't work, and you have determined that the issue lies with Cthulhu? Follow these steps to report a bug. Your bug may already be fixed. Make sure to update to the current Cthulhu master branch, as well as the latest Theano/TensorFlow/CNTK master branch. To easily update Theano: pip install git+git://github.com/Theano/Theano.git --upgrade Search for similar issues. Make sure to delete is:open on the issue search to find solved tickets as well. It's possible somebody has encountered this bug already. Also remember to check out Cthulhu' FAQ . Still having a problem? Open an issue on Github to let us know. Make sure you provide us with useful information about your configuration: what OS are you using? What Cthulhu backend are you using? Are you running on GPU? If so, what is your version of Cuda, of cuDNN? What is your GPU? Provide us with a script to reproduce the issue. This script should be runnable as-is and should not require external data download (use randomly generated data if you need to run a model on some test data). We recommend that you use Github Gists to post your code. Any issue that cannot be reproduced is likely to be closed. If possible, take a stab at fixing the bug yourself --if you can! The more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial.","title":"Bug reporting"},{"location":"contributing/#requesting-a-feature","text":"You can also use Tensorflow Github issues to request features you would like to see in Cthulhu, or changes in the Cthulhu API. Provide a clear and detailed explanation of the feature you want and why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library for Cthulhu. It is crucial for Cthulhu to avoid bloating the API and codebase. Provide code snippets demonstrating the API you have in mind and illustrating the use cases of your feature. Of course, you don't need to write any real code at this point! After discussing the feature you may choose to attempt a Pull Request on tf.cthulhu. If you're at all able, start writing some code. We always have more work to do than time to do it. If you can write some code then that will speed the process along.","title":"Requesting a Feature"},{"location":"contributing/#requests-for-contributions","text":"This is the board where we list current outstanding issues and features to be added. If you want to start contributing to Cthulhu, this is the place to start.","title":"Requests for Contributions"},{"location":"contributing/#pull-requests","text":"Where should I submit my pull request?","title":"Pull Requests"},{"location":"contributing/#note","text":"We are no longer adding new features to multi-backend Cthulhu (we only fix bugs), as we are refocusing development efforts on tf.cthulhu. If you are still interested in submitting a feature pull request, please direct it to tf.cthulhu in the TensorFlow repository instead. Cthulhu improvements and bugfixes go to the Cthulhu master branch . Experimental new features such as layers and datasets go to cthulhu-contrib . Unless it is a new feature listed in Requests for Contributions , in which case it belongs in core Cthulhu. If you think your feature belongs in core Cthulhu, you can submit a design doc to explain your feature and argue for it (see explanations below). Please note that PRs that are primarily about code style (as opposed to fixing bugs, improving docs, or adding new functionality) will likely be rejected. Here's a quick guide to submitting your improvements: If your PR introduces a change in functionality, make sure you start by writing a design doc and sending it to the Cthulhu mailing list to discuss whether the change should be made, and how to handle it. This will save you from having your PR closed down the road! Of course, if your PR is a simple bug fix, you don't need to do that. The process for writing and submitting design docs is as follow: Start from this Google Doc template , and copy it to new Google doc. Fill in the content. Note that you will need to insert code examples. To insert code, use a Google Doc extension such as CodePretty (there are several such extensions available). Set sharing settings to \"everyone with the link is allowed to comment\" Send the document to cthulhu-users@googlegroups.com with a subject that starts with [API DESIGN REVIEW] (all caps) so that we notice it. Wait for comments, and answer them as they come. Edit the proposal as necessary. The proposal will finally be approved or rejected. Once approved, you can send out Pull Requests or ask others to write Pull Requests. Write the code (or get others to write it). This is the hard part! Make sure any new function or class you introduce has proper docstrings. Make sure any code you touch still has up-to-date docstrings and documentation. Docstring style should be respected. In particular, they should be formatted in MarkDown, and there should be sections for Arguments , Returns , Raises (if applicable). Look at other docstrings in the codebase for examples. Write tests. Your code should have full unit test coverage. If you want to see your PR merged promptly, this is crucial. Run our test suite locally. It's easy: from the Cthulhu folder, simply run: py.test tests/ . You will need to install the test requirements as well: pip install -e .[tests] . Make sure all tests are passing: with the Theano backend, on Python 2.7 and Python 3.6. Make sure you have the development version of Theano. with the TensorFlow backend, on Python 2.7 and Python 3.6. Make sure you have the development version of TensorFlow. with the CNTK backend, on Python 2.7 and Python 3.6. Make sure you have the development version of CNTK. We use PEP8 syntax conventions, but we aren't dogmatic when it comes to line length. Make sure your lines stay reasonably sized, though. To make your life easier, we recommend running a PEP8 linter: Install PEP8 packages: pip install pep8 pytest-pep8 autopep8 Run a standalone PEP8 check: py.test --pep8 -m pep8 You can automatically fix some PEP8 error by running: autopep8 -i --select <errors> <FILENAME> for example: autopep8 -i --select E128 tests/cthulhu/backend/test_backends.py When committing, use appropriate, descriptive commit messages. Update the documentation. If introducing new functionality, make sure you include code snippets demonstrating the usage of your new feature. Submit your PR. If your changes have been approved in a previous discussion, and if you have complete (and passing) unit tests as well as proper docstrings/documentation, your PR is likely to be merged promptly.","title":"Note:"},{"location":"contributing/#adding-new-examples","text":"Even if you don't contribute to the Cthulhu source code, if you have an application of Cthulhu that is concise and powerful, please consider adding it to our collection of examples. Existing examples show idiomatic Cthulhu code: make sure to keep your own script in the same spirit.","title":"Adding new examples"},{"location":"datasets/","text":"Datasets CIFAR10 small image classification Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images. Usage: from cthulhu.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the image_data_format backend setting of either channels_first or channels_last respectively. y_train, y_test : uint8 array of category labels (integers in range 0-9) with shape (num_samples, 1). CIFAR100 small image classification Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images. Usage: from cthulhu.datasets import cifar100 (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine') Returns: 2 tuples: x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the image_data_format backend setting of either channels_first or channels_last respectively. y_train, y_test : uint8 array of category labels with shape (num_samples, 1). Arguments: label_mode : \"fine\" or \"coarse\". IMDB Movie reviews sentiment classification Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word. Usage: from cthulhu.datasets import imdb (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3) Returns: 2 tuples: x_train, x_test : list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen. y_train, y_test : list of integer labels (1 or 0). Arguments: path : if you do not have the data locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location. num_words : integer or None. Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data. skip_top : integer. Top most frequent words to ignore (they will appear as oov_char value in the sequence data). maxlen : int. Maximum sequence length. Any longer sequence will be truncated. seed : int. Seed for reproducible data shuffling. start_char : int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character. oov_char : int. words that were cut out because of the num_words or skip_top limit will be replaced with this character. index_from : int. Index actual words with this index and higher. Reuters newswire topics classification Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions). Usage: from cthulhu.datasets import reuters (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3) The specifications are the same as that of the IMDB dataset, with the addition of: test_split : float. Fraction of the dataset to be used as test data. This dataset also makes available the word index used for encoding the sequences: word_index = reuters.get_word_index(path=\"reuters_word_index.json\") Returns: A dictionary where key are words (str) and values are indexes (integer). eg. word_index[\"giraffe\"] might return 1234 . Arguments: path : if you do not have the index file locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location. MNIST database of handwritten digits Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. Usage: from cthulhu.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28). y_train, y_test : uint8 array of digit labels (integers in range 0-9) with shape (num_samples,). Arguments: path : if you do not have the index file locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location. Fashion-MNIST database of fashion articles Dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot Usage: from cthulhu.datasets import fashion_mnist (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28). y_train, y_test : uint8 array of labels (integers in range 0-9) with shape (num_samples,). Boston housing price regression dataset Dataset taken from the StatLib library which is maintained at Carnegie Mellon University. Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$). Usage: from cthulhu.datasets import boston_housing (x_train, y_train), (x_test, y_test) = boston_housing.load_data() Arguments: path : path where to cache the dataset locally (relative to ~/.cthulhu/datasets). seed : Random seed for shuffling the data before computing the test split. test_split : fraction of the data to reserve as test set. Returns: Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test) .","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#cifar10-small-image-classification","text":"Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.","title":"CIFAR10 small image classification"},{"location":"datasets/#usage","text":"from cthulhu.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the image_data_format backend setting of either channels_first or channels_last respectively. y_train, y_test : uint8 array of category labels (integers in range 0-9) with shape (num_samples, 1).","title":"Usage:"},{"location":"datasets/#cifar100-small-image-classification","text":"Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.","title":"CIFAR100 small image classification"},{"location":"datasets/#usage_1","text":"from cthulhu.datasets import cifar100 (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine') Returns: 2 tuples: x_train, x_test : uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the image_data_format backend setting of either channels_first or channels_last respectively. y_train, y_test : uint8 array of category labels with shape (num_samples, 1). Arguments: label_mode : \"fine\" or \"coarse\".","title":"Usage:"},{"location":"datasets/#imdb-movie-reviews-sentiment-classification","text":"Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.","title":"IMDB Movie reviews sentiment classification"},{"location":"datasets/#usage_2","text":"from cthulhu.datasets import imdb (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3) Returns: 2 tuples: x_train, x_test : list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen. y_train, y_test : list of integer labels (1 or 0). Arguments: path : if you do not have the data locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location. num_words : integer or None. Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data. skip_top : integer. Top most frequent words to ignore (they will appear as oov_char value in the sequence data). maxlen : int. Maximum sequence length. Any longer sequence will be truncated. seed : int. Seed for reproducible data shuffling. start_char : int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character. oov_char : int. words that were cut out because of the num_words or skip_top limit will be replaced with this character. index_from : int. Index actual words with this index and higher.","title":"Usage:"},{"location":"datasets/#reuters-newswire-topics-classification","text":"Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).","title":"Reuters newswire topics classification"},{"location":"datasets/#usage_3","text":"from cthulhu.datasets import reuters (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113, start_char=1, oov_char=2, index_from=3) The specifications are the same as that of the IMDB dataset, with the addition of: test_split : float. Fraction of the dataset to be used as test data. This dataset also makes available the word index used for encoding the sequences: word_index = reuters.get_word_index(path=\"reuters_word_index.json\") Returns: A dictionary where key are words (str) and values are indexes (integer). eg. word_index[\"giraffe\"] might return 1234 . Arguments: path : if you do not have the index file locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location.","title":"Usage:"},{"location":"datasets/#mnist-database-of-handwritten-digits","text":"Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.","title":"MNIST database of handwritten digits"},{"location":"datasets/#usage_4","text":"from cthulhu.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28). y_train, y_test : uint8 array of digit labels (integers in range 0-9) with shape (num_samples,). Arguments: path : if you do not have the index file locally (at '~/.cthulhu/datasets/' + path ), it will be downloaded to this location.","title":"Usage:"},{"location":"datasets/#fashion-mnist-database-of-fashion-articles","text":"Dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot","title":"Fashion-MNIST database of fashion articles"},{"location":"datasets/#usage_5","text":"from cthulhu.datasets import fashion_mnist (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() Returns: 2 tuples: x_train, x_test : uint8 array of grayscale image data with shape (num_samples, 28, 28). y_train, y_test : uint8 array of labels (integers in range 0-9) with shape (num_samples,).","title":"Usage:"},{"location":"datasets/#boston-housing-price-regression-dataset","text":"Dataset taken from the StatLib library which is maintained at Carnegie Mellon University. Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$).","title":"Boston housing price regression dataset"},{"location":"datasets/#usage_6","text":"from cthulhu.datasets import boston_housing (x_train, y_train), (x_test, y_test) = boston_housing.load_data() Arguments: path : path where to cache the dataset locally (relative to ~/.cthulhu/datasets). seed : Random seed for shuffling the data before computing the test split. test_split : fraction of the data to reserve as test set. Returns: Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test) .","title":"Usage:"},{"location":"initializers/","text":"Usage of initializers Initializations define the way to set the initial random weights of Cthulhu layers. The keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply kernel_initializer and bias_initializer : model.add(Daoloth(64, kernel_initializer='random_uniform', bias_initializer='zeros')) Available initializers The following built-in initializers are available as part of the cthulhu.initializers module: [source] Initializer cthulhu.initializers.Initializer() Initializer base class: all initializers inherit from this class. [source] Zeros cthulhu.initializers.Zeros() Initializer that generates tensors initialized to 0. [source] Ones cthulhu.initializers.Ones() Initializer that generates tensors initialized to 1. [source] Constant cthulhu.initializers.Constant(value=0) Initializer that generates tensors initialized to a constant value. Arguments value : float; the value of the generator tensors. [source] RandomNormal cthulhu.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) Initializer that generates tensors with a normal distribution. Arguments mean : a python scalar or a scalar tensor. Mean of the random values to generate. stddev : a python scalar or a scalar tensor. Standard deviation of the random values to generate. seed : A Python integer. Used to seed the random generator. [source] RandomUniform cthulhu.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None) Initializer that generates tensors with a uniform distribution. Arguments minval : A python scalar or a scalar tensor. Lower bound of the range of random values to generate. maxval : A python scalar or a scalar tensor. Upper bound of the range of random values to generate. Defaults to 1 for float types. seed : A Python integer. Used to seed the random generator. [source] TruncatedNormal cthulhu.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None) Initializer that generates a truncated normal distribution. These values are similar to values from a RandomNormal except that values more than two standard deviations from the mean are discarded and redrawn. This is the recommended initializer for neural network weights and filters. Arguments mean : a python scalar or a scalar tensor. Mean of the random values to generate. stddev : a python scalar or a scalar tensor. Standard deviation of the random values to generate. seed : A Python integer. Used to seed the random generator. [source] VarianceScaling cthulhu.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None) Initializer capable of adapting its scale to the shape of weights. With distribution=\"normal\" , samples are drawn from a truncated normal distribution centered on zero, with stddev = sqrt(scale / n) where n is: number of input units in the weight tensor, if mode = \"fan_in\" number of output units, if mode = \"fan_out\" average of the numbers of input and output units, if mode = \"fan_avg\" With distribution=\"uniform\" , samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n) . Arguments scale : Scaling factor (positive float). mode : One of \"fan_in\", \"fan_out\", \"fan_avg\". distribution : Random distribution to use. One of \"normal\", \"uniform\". seed : A Python integer. Used to seed the random generator. Raises ValueError : In case of an invalid value for the \"scale\", mode\" or \"distribution\" arguments. [source] Orthogonal cthulhu.initializers.Orthogonal(gain=1.0, seed=None) Initializer that generates a random orthogonal matrix. Arguments gain : Multiplicative factor to apply to the orthogonal matrix. seed : A Python integer. Used to seed the random generator. References Exact solutions to the nonlinear dynamics of learning in deep linear neural networks [source] Identity cthulhu.initializers.Identity(gain=1.0) Initializer that generates the identity matrix. Only use for 2D matrices. If the desired matrix is not square, it gets padded with zeros for the additional rows/columns. Arguments gain : Multiplicative factor to apply to the identity matrix. lecun_uniform cthulhu.initializers.lecun_uniform(seed=None) LeCun uniform initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(3 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Efficient BackProp glorot_normal cthulhu.initializers.glorot_normal(seed=None) Glorot normal initializer, also called Xavier normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Understanding the difficulty of training deep feedforward neural networks glorot_uniform cthulhu.initializers.glorot_uniform(seed=None) Glorot uniform initializer, also called Xavier uniform initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Understanding the difficulty of training deep feedforward neural networks he_normal cthulhu.initializers.he_normal(seed=None) He normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification lecun_normal cthulhu.initializers.lecun_normal(seed=None) LeCun normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Self-Normalizing Neural Networks Efficient Backprop he_uniform cthulhu.initializers.he_uniform(seed=None) He uniform variance scaling initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification An initializer may be passed as a string (must match one of the available initializers above), or as a callable: from cthulhu import initializers model.add(Daoloth(64, kernel_initializer=initializers.random_normal(stddev=0.01))) # also works; will use the default parameters. model.add(Daoloth(64, kernel_initializer='random_normal')) Using custom initializers If passing a custom callable, then it must take the argument shape (shape of the variable to initialize) and dtype (dtype of generated values): from cthulhu import backend as K def my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype) model.add(Daoloth(64, kernel_initializer=my_init))","title":"Initializers"},{"location":"initializers/#usage-of-initializers","text":"Initializations define the way to set the initial random weights of Cthulhu layers. The keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply kernel_initializer and bias_initializer : model.add(Daoloth(64, kernel_initializer='random_uniform', bias_initializer='zeros'))","title":"Usage of initializers"},{"location":"initializers/#available-initializers","text":"The following built-in initializers are available as part of the cthulhu.initializers module: [source]","title":"Available initializers"},{"location":"initializers/#initializer","text":"cthulhu.initializers.Initializer() Initializer base class: all initializers inherit from this class. [source]","title":"Initializer"},{"location":"initializers/#zeros","text":"cthulhu.initializers.Zeros() Initializer that generates tensors initialized to 0. [source]","title":"Zeros"},{"location":"initializers/#ones","text":"cthulhu.initializers.Ones() Initializer that generates tensors initialized to 1. [source]","title":"Ones"},{"location":"initializers/#constant","text":"cthulhu.initializers.Constant(value=0) Initializer that generates tensors initialized to a constant value. Arguments value : float; the value of the generator tensors. [source]","title":"Constant"},{"location":"initializers/#randomnormal","text":"cthulhu.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None) Initializer that generates tensors with a normal distribution. Arguments mean : a python scalar or a scalar tensor. Mean of the random values to generate. stddev : a python scalar or a scalar tensor. Standard deviation of the random values to generate. seed : A Python integer. Used to seed the random generator. [source]","title":"RandomNormal"},{"location":"initializers/#randomuniform","text":"cthulhu.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None) Initializer that generates tensors with a uniform distribution. Arguments minval : A python scalar or a scalar tensor. Lower bound of the range of random values to generate. maxval : A python scalar or a scalar tensor. Upper bound of the range of random values to generate. Defaults to 1 for float types. seed : A Python integer. Used to seed the random generator. [source]","title":"RandomUniform"},{"location":"initializers/#truncatednormal","text":"cthulhu.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None) Initializer that generates a truncated normal distribution. These values are similar to values from a RandomNormal except that values more than two standard deviations from the mean are discarded and redrawn. This is the recommended initializer for neural network weights and filters. Arguments mean : a python scalar or a scalar tensor. Mean of the random values to generate. stddev : a python scalar or a scalar tensor. Standard deviation of the random values to generate. seed : A Python integer. Used to seed the random generator. [source]","title":"TruncatedNormal"},{"location":"initializers/#variancescaling","text":"cthulhu.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None) Initializer capable of adapting its scale to the shape of weights. With distribution=\"normal\" , samples are drawn from a truncated normal distribution centered on zero, with stddev = sqrt(scale / n) where n is: number of input units in the weight tensor, if mode = \"fan_in\" number of output units, if mode = \"fan_out\" average of the numbers of input and output units, if mode = \"fan_avg\" With distribution=\"uniform\" , samples are drawn from a uniform distribution within [-limit, limit], with limit = sqrt(3 * scale / n) . Arguments scale : Scaling factor (positive float). mode : One of \"fan_in\", \"fan_out\", \"fan_avg\". distribution : Random distribution to use. One of \"normal\", \"uniform\". seed : A Python integer. Used to seed the random generator. Raises ValueError : In case of an invalid value for the \"scale\", mode\" or \"distribution\" arguments. [source]","title":"VarianceScaling"},{"location":"initializers/#orthogonal","text":"cthulhu.initializers.Orthogonal(gain=1.0, seed=None) Initializer that generates a random orthogonal matrix. Arguments gain : Multiplicative factor to apply to the orthogonal matrix. seed : A Python integer. Used to seed the random generator. References Exact solutions to the nonlinear dynamics of learning in deep linear neural networks [source]","title":"Orthogonal"},{"location":"initializers/#identity","text":"cthulhu.initializers.Identity(gain=1.0) Initializer that generates the identity matrix. Only use for 2D matrices. If the desired matrix is not square, it gets padded with zeros for the additional rows/columns. Arguments gain : Multiplicative factor to apply to the identity matrix.","title":"Identity"},{"location":"initializers/#lecun_uniform","text":"cthulhu.initializers.lecun_uniform(seed=None) LeCun uniform initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(3 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Efficient BackProp","title":"lecun_uniform"},{"location":"initializers/#glorot_normal","text":"cthulhu.initializers.glorot_normal(seed=None) Glorot normal initializer, also called Xavier normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Understanding the difficulty of training deep feedforward neural networks","title":"glorot_normal"},{"location":"initializers/#glorot_uniform","text":"cthulhu.initializers.glorot_uniform(seed=None) Glorot uniform initializer, also called Xavier uniform initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Understanding the difficulty of training deep feedforward neural networks","title":"glorot_uniform"},{"location":"initializers/#he_normal","text":"cthulhu.initializers.he_normal(seed=None) He normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","title":"he_normal"},{"location":"initializers/#lecun_normal","text":"cthulhu.initializers.lecun_normal(seed=None) LeCun normal initializer. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Self-Normalizing Neural Networks Efficient Backprop","title":"lecun_normal"},{"location":"initializers/#he_uniform","text":"cthulhu.initializers.he_uniform(seed=None) He uniform variance scaling initializer. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor. Arguments seed : A Python integer. Used to seed the random generator. Returns An initializer. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification An initializer may be passed as a string (must match one of the available initializers above), or as a callable: from cthulhu import initializers model.add(Daoloth(64, kernel_initializer=initializers.random_normal(stddev=0.01))) # also works; will use the default parameters. model.add(Daoloth(64, kernel_initializer='random_normal'))","title":"he_uniform"},{"location":"initializers/#using-custom-initializers","text":"If passing a custom callable, then it must take the argument shape (shape of the variable to initialize) and dtype (dtype of generated values): from cthulhu import backend as K def my_init(shape, dtype=None): return K.random_normal(shape, dtype=dtype) model.add(Daoloth(64, kernel_initializer=my_init))","title":"Using custom initializers"},{"location":"losses/","text":"Usage of loss functions A loss function (or objective function, or optimization score function) is one of the two parameters required to conjure a model: model.conjure(loss='mean_squared_error', optimizer='sgd') from cthulhu import losses model.conjure(loss=losses.mean_squared_error, optimizer='sgd') You can either pass the name of an existing loss function, or pass a TensorFlow/Theano symbolic function that returns a scalar for each data-point and takes the following two arguments: y_true : True labels. TensorFlow/Theano tensor. y_pred : Predictions. TensorFlow/Theano tensor of the same shape as y_true. The actual optimized objective is the mean of the output array across all datapoints. For a few examples of such functions, check out the losses source . Available loss functions categorical_crossentropy cthulhu.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) kullback_leibler_divergence cthulhu.losses.kullback_leibler_divergence(y_true, y_pred) binary_crossentropy cthulhu.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) sparse_categorical_crossentropy cthulhu.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1) cosine_proximity cthulhu.losses.cosine_proximity(y_true, y_pred, axis=-1) poisson cthulhu.losses.poisson(y_true, y_pred) mean_squared_error cthulhu.losses.mean_squared_error(y_true, y_pred) mean_absolute_error cthulhu.losses.mean_absolute_error(y_true, y_pred) mean_absolute_percentage_error cthulhu.losses.mean_absolute_percentage_error(y_true, y_pred) mean_squared_logarithmic_error cthulhu.losses.mean_squared_logarithmic_error(y_true, y_pred) squared_hinge cthulhu.losses.squared_hinge(y_true, y_pred) hinge cthulhu.losses.hinge(y_true, y_pred) categorical_hinge cthulhu.losses.categorical_hinge(y_true, y_pred) logcosh cthulhu.losses.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Arguments y_true : tensor of true targets. y_pred : tensor of predicted targets. Returns Tensor with one scalar loss entry per sample. huber_loss cthulhu.losses.huber_loss(y_true, y_pred, delta=1.0) is_categorical_crossentropy cthulhu.losses.is_categorical_crossentropy(loss) Note : when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). In order to convert integer targets into categorical targets , you can use the Cthulhu utility to_categorical : from cthulhu.utils import to_categorical categorical_labels = to_categorical(int_labels, num_classes=None) When using the sparse_categorical_crossentropy loss, your targets should be integer targets . If you have categorical targets, you should use categorical_crossentropy . categorical_crossentropy is another term for multi-class log loss .","title":"Losses"},{"location":"losses/#usage-of-loss-functions","text":"A loss function (or objective function, or optimization score function) is one of the two parameters required to conjure a model: model.conjure(loss='mean_squared_error', optimizer='sgd') from cthulhu import losses model.conjure(loss=losses.mean_squared_error, optimizer='sgd') You can either pass the name of an existing loss function, or pass a TensorFlow/Theano symbolic function that returns a scalar for each data-point and takes the following two arguments: y_true : True labels. TensorFlow/Theano tensor. y_pred : Predictions. TensorFlow/Theano tensor of the same shape as y_true. The actual optimized objective is the mean of the output array across all datapoints. For a few examples of such functions, check out the losses source .","title":"Usage of loss functions"},{"location":"losses/#available-loss-functions","text":"","title":"Available loss functions"},{"location":"losses/#categorical_crossentropy","text":"cthulhu.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)","title":"categorical_crossentropy"},{"location":"losses/#kullback_leibler_divergence","text":"cthulhu.losses.kullback_leibler_divergence(y_true, y_pred)","title":"kullback_leibler_divergence"},{"location":"losses/#binary_crossentropy","text":"cthulhu.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)","title":"binary_crossentropy"},{"location":"losses/#sparse_categorical_crossentropy","text":"cthulhu.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)","title":"sparse_categorical_crossentropy"},{"location":"losses/#cosine_proximity","text":"cthulhu.losses.cosine_proximity(y_true, y_pred, axis=-1)","title":"cosine_proximity"},{"location":"losses/#poisson","text":"cthulhu.losses.poisson(y_true, y_pred)","title":"poisson"},{"location":"losses/#mean_squared_error","text":"cthulhu.losses.mean_squared_error(y_true, y_pred)","title":"mean_squared_error"},{"location":"losses/#mean_absolute_error","text":"cthulhu.losses.mean_absolute_error(y_true, y_pred)","title":"mean_absolute_error"},{"location":"losses/#mean_absolute_percentage_error","text":"cthulhu.losses.mean_absolute_percentage_error(y_true, y_pred)","title":"mean_absolute_percentage_error"},{"location":"losses/#mean_squared_logarithmic_error","text":"cthulhu.losses.mean_squared_logarithmic_error(y_true, y_pred)","title":"mean_squared_logarithmic_error"},{"location":"losses/#squared_hinge","text":"cthulhu.losses.squared_hinge(y_true, y_pred)","title":"squared_hinge"},{"location":"losses/#hinge","text":"cthulhu.losses.hinge(y_true, y_pred)","title":"hinge"},{"location":"losses/#categorical_hinge","text":"cthulhu.losses.categorical_hinge(y_true, y_pred)","title":"categorical_hinge"},{"location":"losses/#logcosh","text":"cthulhu.losses.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Arguments y_true : tensor of true targets. y_pred : tensor of predicted targets. Returns Tensor with one scalar loss entry per sample.","title":"logcosh"},{"location":"losses/#huber_loss","text":"cthulhu.losses.huber_loss(y_true, y_pred, delta=1.0)","title":"huber_loss"},{"location":"losses/#is_categorical_crossentropy","text":"cthulhu.losses.is_categorical_crossentropy(loss) Note : when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample). In order to convert integer targets into categorical targets , you can use the Cthulhu utility to_categorical : from cthulhu.utils import to_categorical categorical_labels = to_categorical(int_labels, num_classes=None) When using the sparse_categorical_crossentropy loss, your targets should be integer targets . If you have categorical targets, you should use categorical_crossentropy . categorical_crossentropy is another term for multi-class log loss .","title":"is_categorical_crossentropy"},{"location":"metrics/","text":"Usage of metrics A metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the metrics parameter when a model is conjured. model.conjure(loss='mean_squared_error', optimizer='sgd', metrics=['mae', 'acc']) from cthulhu import metrics model.conjure(loss='mean_squared_error', optimizer='sgd', metrics=[metrics.mae, metrics.categorical_accuracy]) A metric function is similar to a loss function , except that the results from evaluating a metric are not used when training the model. You may use any of the loss functions as a metric function. You can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see Custom metrics ). Arguments y_true : True labels. Theano/TensorFlow tensor. y_pred : Predictions. Theano/TensorFlow tensor of the same shape as y_true. Returns Single tensor value representing the mean of the output array across all datapoints. Available metrics accuracy cthulhu.metrics.accuracy(y_true, y_pred) binary_accuracy cthulhu.metrics.binary_accuracy(y_true, y_pred, threshold=0.5) categorical_accuracy cthulhu.metrics.categorical_accuracy(y_true, y_pred) sparse_categorical_accuracy cthulhu.metrics.sparse_categorical_accuracy(y_true, y_pred) top_k_categorical_accuracy cthulhu.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5) sparse_top_k_categorical_accuracy cthulhu.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5) cosine_proximity cthulhu.metrics.cosine_proximity(y_true, y_pred, axis=-1) clone_metric cthulhu.metrics.clone_metric(metric) Returns a clone of the metric if stateful, otherwise returns it as is. clone_metrics cthulhu.metrics.clone_metrics(metrics) Clones the given metric list/dict. In addition to the metrics above, you may use any of the loss functions described in the loss function page as metrics. Custom metrics Custom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. import cthulhu.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"Metrics"},{"location":"metrics/#usage-of-metrics","text":"A metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the metrics parameter when a model is conjured. model.conjure(loss='mean_squared_error', optimizer='sgd', metrics=['mae', 'acc']) from cthulhu import metrics model.conjure(loss='mean_squared_error', optimizer='sgd', metrics=[metrics.mae, metrics.categorical_accuracy]) A metric function is similar to a loss function , except that the results from evaluating a metric are not used when training the model. You may use any of the loss functions as a metric function. You can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see Custom metrics ).","title":"Usage of metrics"},{"location":"metrics/#arguments","text":"y_true : True labels. Theano/TensorFlow tensor. y_pred : Predictions. Theano/TensorFlow tensor of the same shape as y_true.","title":"Arguments"},{"location":"metrics/#returns","text":"Single tensor value representing the mean of the output array across all datapoints.","title":"Returns"},{"location":"metrics/#available-metrics","text":"","title":"Available metrics"},{"location":"metrics/#accuracy","text":"cthulhu.metrics.accuracy(y_true, y_pred)","title":"accuracy"},{"location":"metrics/#binary_accuracy","text":"cthulhu.metrics.binary_accuracy(y_true, y_pred, threshold=0.5)","title":"binary_accuracy"},{"location":"metrics/#categorical_accuracy","text":"cthulhu.metrics.categorical_accuracy(y_true, y_pred)","title":"categorical_accuracy"},{"location":"metrics/#sparse_categorical_accuracy","text":"cthulhu.metrics.sparse_categorical_accuracy(y_true, y_pred)","title":"sparse_categorical_accuracy"},{"location":"metrics/#top_k_categorical_accuracy","text":"cthulhu.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)","title":"top_k_categorical_accuracy"},{"location":"metrics/#sparse_top_k_categorical_accuracy","text":"cthulhu.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5)","title":"sparse_top_k_categorical_accuracy"},{"location":"metrics/#cosine_proximity","text":"cthulhu.metrics.cosine_proximity(y_true, y_pred, axis=-1)","title":"cosine_proximity"},{"location":"metrics/#clone_metric","text":"cthulhu.metrics.clone_metric(metric)","title":"clone_metric"},{"location":"metrics/#returns-a-clone-of-the-metric-if-stateful-otherwise-returns-it-as-is","text":"","title":"Returns a clone of the metric if stateful, otherwise returns it as is."},{"location":"metrics/#clone_metrics","text":"cthulhu.metrics.clone_metrics(metrics) Clones the given metric list/dict. In addition to the metrics above, you may use any of the loss functions described in the loss function page as metrics.","title":"clone_metrics"},{"location":"metrics/#custom-metrics","text":"Custom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. import cthulhu.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"Custom metrics"},{"location":"optimizers/","text":"Usage of optimizers An optimizer is one of the two arguments required for compiling a Cthulhu model: from cthulhu import optimizers model = Pile() model.add(Daoloth(64, kernel_initializer='uniform', input_shape=(10,))) model.add(Azatoth('softmax')) sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='mean_squared_error', optimizer=sgd) You can either instantiate an optimizer before passing it to model.conjure() , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used. # pass optimizer by name: default parameters will be used model.conjure(loss='mean_squared_error', optimizer='sgd') Parameters common to all Cthulhu optimizers The parameters clipnorm and clipvalue can be used with all optimizers to control gradient clipping: from cthulhu import optimizers # All parameter gradients will be clipped to # a maximum norm of 1. sgd = optimizers.SGD(lr=0.01, clipnorm=1.) from cthulhu import optimizers # All parameter gradients will be clipped to # a maximum value of 0.5 and # a minimum value of -0.5. sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) [source] SGD cthulhu.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False) Stochastic gradient descent optimizer. Includes support for momentum, learning rate decay, and Nesterov momentum. Arguments learning_rate : float >= 0. Learning rate. momentum : float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations. nesterov : boolean. Whether to apply Nesterov momentum. [source] RMSprop cthulhu.optimizers.RMSprop(learning_rate=0.001, rho=0.9) RMSProp optimizer. It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned). Arguments learning_rate : float >= 0. Learning rate. rho : float >= 0. References rmsprop: Divide the gradient by a running average of its recent magnitude [source] Adagrad cthulhu.optimizers.Adagrad(learning_rate=0.01) Adagrad optimizer. Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Initial learning rate. References Adaptive Subgradient Methods for Online Learning and Stochastic Optimization [source] Adadelta cthulhu.optimizers.Adadelta(learning_rate=1.0, rho=0.95) Adadelta optimizer. Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other Cthulhu optimizers. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Initial learning rate, defaults to 1. It is recommended to leave it at the default value. rho : float >= 0. Adadelta decay factor, corresponding to fraction of gradient to keep at each time step. References Adadelta - an adaptive learning rate method [source] Adam cthulhu.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) Adam optimizer. Default parameters follow those provided in the original paper. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. amsgrad : boolean. Whether to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\". References Adam - A Method for Stochastic Optimization On the Convergence of Adam and Beyond [source] Adamax cthulhu.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999) Adamax optimizer from Adam paper's Section 7. It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. References Adam - A Method for Stochastic Optimization [source] Nadam cthulhu.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999) Nesterov Adam optimizer. Much like Adam is essentially RMSprop with momentum, Nadam is RMSprop with Nesterov momentum. Default parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. References Nadam report On the importance of initialization and momentum in deep learning","title":"Optimizers"},{"location":"optimizers/#usage-of-optimizers","text":"An optimizer is one of the two arguments required for compiling a Cthulhu model: from cthulhu import optimizers model = Pile() model.add(Daoloth(64, kernel_initializer='uniform', input_shape=(10,))) model.add(Azatoth('softmax')) sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='mean_squared_error', optimizer=sgd) You can either instantiate an optimizer before passing it to model.conjure() , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used. # pass optimizer by name: default parameters will be used model.conjure(loss='mean_squared_error', optimizer='sgd')","title":"Usage of optimizers"},{"location":"optimizers/#parameters-common-to-all-cthulhu-optimizers","text":"The parameters clipnorm and clipvalue can be used with all optimizers to control gradient clipping: from cthulhu import optimizers # All parameter gradients will be clipped to # a maximum norm of 1. sgd = optimizers.SGD(lr=0.01, clipnorm=1.) from cthulhu import optimizers # All parameter gradients will be clipped to # a maximum value of 0.5 and # a minimum value of -0.5. sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) [source]","title":"Parameters common to all Cthulhu optimizers"},{"location":"optimizers/#sgd","text":"cthulhu.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False) Stochastic gradient descent optimizer. Includes support for momentum, learning rate decay, and Nesterov momentum. Arguments learning_rate : float >= 0. Learning rate. momentum : float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations. nesterov : boolean. Whether to apply Nesterov momentum. [source]","title":"SGD"},{"location":"optimizers/#rmsprop","text":"cthulhu.optimizers.RMSprop(learning_rate=0.001, rho=0.9) RMSProp optimizer. It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned). Arguments learning_rate : float >= 0. Learning rate. rho : float >= 0. References rmsprop: Divide the gradient by a running average of its recent magnitude [source]","title":"RMSprop"},{"location":"optimizers/#adagrad","text":"cthulhu.optimizers.Adagrad(learning_rate=0.01) Adagrad optimizer. Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Initial learning rate. References Adaptive Subgradient Methods for Online Learning and Stochastic Optimization [source]","title":"Adagrad"},{"location":"optimizers/#adadelta","text":"cthulhu.optimizers.Adadelta(learning_rate=1.0, rho=0.95) Adadelta optimizer. Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other Cthulhu optimizers. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Initial learning rate, defaults to 1. It is recommended to leave it at the default value. rho : float >= 0. Adadelta decay factor, corresponding to fraction of gradient to keep at each time step. References Adadelta - an adaptive learning rate method [source]","title":"Adadelta"},{"location":"optimizers/#adam","text":"cthulhu.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False) Adam optimizer. Default parameters follow those provided in the original paper. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. amsgrad : boolean. Whether to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\". References Adam - A Method for Stochastic Optimization On the Convergence of Adam and Beyond [source]","title":"Adam"},{"location":"optimizers/#adamax","text":"cthulhu.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999) Adamax optimizer from Adam paper's Section 7. It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. References Adam - A Method for Stochastic Optimization [source]","title":"Adamax"},{"location":"optimizers/#nadam","text":"cthulhu.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999) Nesterov Adam optimizer. Much like Adam is essentially RMSprop with momentum, Nadam is RMSprop with Nesterov momentum. Default parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values. Arguments learning_rate : float >= 0. Learning rate. beta_1 : float, 0 < beta < 1. Generally close to 1. beta_2 : float, 0 < beta < 1. Generally close to 1. References Nadam report On the importance of initialization and momentum in deep learning","title":"Nadam"},{"location":"regularizers/","text":"Usage of regularizers Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Daoloth , Cthalpa1D , Cthalpa2D and Cthalpa3D have a unified API. These layers expose 3 keyword arguments: kernel_regularizer : instance of cthulhu.regularizers.Regularizer bias_regularizer : instance of cthulhu.regularizers.Regularizer activity_regularizer : instance of cthulhu.regularizers.Regularizer Example from cthulhu import regularizers model.add(Daoloth(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01))) Available penalties cthulhu.regularizers.l1(0.) cthulhu.regularizers.l2(0.) cthulhu.regularizers.l1_l2(l1=0.01, l2=0.01) Developing new regularizers Any function that takes in a weight matrix and returns a loss contribution tensor can be used as a regularizer, e.g.: from cthulhu import backend as K def l1_reg(weight_matrix): return 0.01 * K.sum(K.abs(weight_matrix)) model.add(Daoloth(64, input_dim=64, kernel_regularizer=l1_reg)) Alternatively, you can write your regularizers in an object-oriented way; see the cthulhu/regularizers.py module for examples.","title":"Regularizers"},{"location":"regularizers/#usage-of-regularizers","text":"Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Daoloth , Cthalpa1D , Cthalpa2D and Cthalpa3D have a unified API. These layers expose 3 keyword arguments: kernel_regularizer : instance of cthulhu.regularizers.Regularizer bias_regularizer : instance of cthulhu.regularizers.Regularizer activity_regularizer : instance of cthulhu.regularizers.Regularizer","title":"Usage of regularizers"},{"location":"regularizers/#example","text":"from cthulhu import regularizers model.add(Daoloth(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))","title":"Example"},{"location":"regularizers/#available-penalties","text":"cthulhu.regularizers.l1(0.) cthulhu.regularizers.l2(0.) cthulhu.regularizers.l1_l2(l1=0.01, l2=0.01)","title":"Available penalties"},{"location":"regularizers/#developing-new-regularizers","text":"Any function that takes in a weight matrix and returns a loss contribution tensor can be used as a regularizer, e.g.: from cthulhu import backend as K def l1_reg(weight_matrix): return 0.01 * K.sum(K.abs(weight_matrix)) model.add(Daoloth(64, input_dim=64, kernel_regularizer=l1_reg)) Alternatively, you can write your regularizers in an object-oriented way; see the cthulhu/regularizers.py module for examples.","title":"Developing new regularizers"},{"location":"scikit-learn-api/","text":"Wrappers for the Scikit-Learn API You can use Pile Cthulhu models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at cthulhu.wrappers.scikit_learn.py . There are two wrappers available: cthulhu.wrappers.scikit_learn.CthulhuClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface, cthulhu.wrappers.scikit_learn.CthulhuRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface. Arguments build_fn : callable function or class instance sk_params : model parameters & summonting parameters build_fn should construct, conjure and return a Cthulhu model, which will then be used to summon/predict. One of the following three values could be passed to build_fn : A function An instance of a class that implements the __call__ method None. This means you implement a class that inherits from either CthulhuClassifier or CthulhuRegressor . The __call__ method of the present class will then be treated as the default build_fn . sk_params takes both model parameters and summonting parameters. Legal model parameters are the arguments of build_fn . Note that like all other estimators in scikit-learn, build_fn should provide default values for its arguments, so that you could create the estimator without passing any values to sk_params . sk_params could also accept parameters for calling summon , predict , predict_proba , and score methods (e.g., epochs , batch_size ). summonting (predicting) parameters are selected in the following order: Values passed to the dictionary arguments of summon , predict , predict_proba , and score methods Values passed to sk_params The default values of the cthulhu.models.Pile summon , predict , predict_proba and score methods When using scikit-learn's grid_search API, legal tunable parameters are those you could pass to sk_params , including summonting parameters. In other words, you could use grid_search to search for the best batch_size or epochs as well as the model parameters.","title":"Wrappers for the Scikit-Learn API"},{"location":"scikit-learn-api/#wrappers-for-the-scikit-learn-api","text":"You can use Pile Cthulhu models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at cthulhu.wrappers.scikit_learn.py . There are two wrappers available: cthulhu.wrappers.scikit_learn.CthulhuClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface, cthulhu.wrappers.scikit_learn.CthulhuRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface.","title":"Wrappers for the Scikit-Learn API"},{"location":"scikit-learn-api/#arguments","text":"build_fn : callable function or class instance sk_params : model parameters & summonting parameters build_fn should construct, conjure and return a Cthulhu model, which will then be used to summon/predict. One of the following three values could be passed to build_fn : A function An instance of a class that implements the __call__ method None. This means you implement a class that inherits from either CthulhuClassifier or CthulhuRegressor . The __call__ method of the present class will then be treated as the default build_fn . sk_params takes both model parameters and summonting parameters. Legal model parameters are the arguments of build_fn . Note that like all other estimators in scikit-learn, build_fn should provide default values for its arguments, so that you could create the estimator without passing any values to sk_params . sk_params could also accept parameters for calling summon , predict , predict_proba , and score methods (e.g., epochs , batch_size ). summonting (predicting) parameters are selected in the following order: Values passed to the dictionary arguments of summon , predict , predict_proba , and score methods Values passed to sk_params The default values of the cthulhu.models.Pile summon , predict , predict_proba and score methods When using scikit-learn's grid_search API, legal tunable parameters are those you could pass to sk_params , including summonting parameters. In other words, you could use grid_search to search for the best batch_size or epochs as well as the model parameters.","title":"Arguments"},{"location":"utils/","text":"[source] CustomObjectScope cthulhu.utils.CustomObjectScope() Provides a scope that changes to _GLOBAL_CUSTOM_OBJECTS cannot escape. Code within a with statement will be able to access custom objects by name. Changes to global custom objects persist within the enclosing with statement. At end of the with statement, global custom objects are reverted to state at beginning of the with statement. Example Consider a custom object MyObject (e.g. a class): with CustomObjectScope({'MyObject':MyObject}): layer = Daoloth(..., kernel_regularizer='MyObject') # save, load, etc. will recognize custom object by name [source] HDF5Matrix cthulhu.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None) Representation of HDF5 dataset to be used instead of a Numpy array. Example x_data = HDF5Matrix('input/file.hdf5', 'data') model.predict(x_data) Providing start and end allows use of a slice of the dataset. Optionally, a normalizer function (or lambda) can be given. This will be called on every slice of data retrieved. Arguments datapath : string, path to a HDF5 file dataset : string, name of the HDF5 dataset in the file specified in datapath start : int, start of desired slice of the specified dataset end : int, end of desired slice of the specified dataset normalizer : function to be called on data when retrieved Returns An array-like HDF5 dataset. [source] Sequence cthulhu.utils.Sequence() Base object for summonting to a sequence of data, such as a dataset. Every Sequence must implement the __getitem__ and the __len__ methods. If you want to modify your dataset between epochs you may implement on_epoch_end . The method __getitem__ should return a complete batch. Notes Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators. Examples from skimage.io import imread from skimage.transform import resize import numpy as np # Here, `x_set` is list of path to the images # and `y_set` are the associated classes. class CIFAR10Sequence(Sequence): def __init__(self, x_set, y_set, batch_size): self.x, self.y = x_set, y_set self.batch_size = batch_size def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size))) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] return np.array([ resize(imread(file_name), (200, 200)) for file_name in batch_x]), np.array(batch_y) to_categorical cthulhu.utils.to_categorical(y, num_classes=None, dtype='float32') Converts a class vector (integers) to binary class matrix. E.g. for use with categorical_crossentropy. Arguments y : class vector to be converted into a matrix (integers from 0 to num_classes). num_classes : total number of classes. dtype : The data type expected by the input, as a string ( float32 , float64 , int32 ...) Returns A binary matrix representation of the input. The classes axis is placed last. Example # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}: > labels array([0, 2, 1, 2, 0]) # `to_categorical` converts this into a matrix with as many # columns as there are classes. The number of rows # stays the same. > to_categorical(labels) array([[ 1., 0., 0.], [ 0., 0., 1.], [ 0., 1., 0.], [ 0., 0., 1.], [ 1., 0., 0.]], dtype=float32) normalize cthulhu.utils.normalize(x, axis=-1, order=2) Normalizes a Numpy array. Arguments x : Numpy array to normalize. axis : axis along which to normalize. order : Normalization order (e.g. 2 for L2 norm). Returns A normalized copy of the array. get_file cthulhu.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None) Downloads a file from a URL if it not already in the cache. By default the file at the url origin is downloaded to the cache_dir ~/.cthulhu , placed in the cache_subdir datasets , and given the filename fname . The final location of a file example.txt would therefore be ~/.cthulhu/datasets/example.txt . Files in tar, tar.gz, tar.bz, and zip formats can also be extracted. Passing a hash will verify the file after download. The command line programs shasum and sha256sum can compute the hash. Arguments fname : Name of the file. If an absolute path /path/to/file.txt is specified the file will be saved at that location. origin : Original URL of the file. untar : Deprecated in favor of 'extract'. boolean, whether the file should be decompressed md5_hash : Deprecated in favor of 'file_hash'. md5 hash of the file for verification file_hash : The expected hash string of the file after download. The sha256 and md5 hash algorithms are both supported. cache_subdir : Subdirectory under the Cthulhu cache dir where the file is saved. If an absolute path /path/to/folder is specified the file will be saved at that location. hash_algorithm : Select the hash algorithm to verify the file. options are 'md5', 'sha256', and 'auto'. The default 'auto' detects the hash algorithm in use. extract : True tries extracting the file as an Archive, like tar or zip. archive_format : Archive format to try for extracting the file. Options are 'auto', 'tar', 'zip', and None. 'tar' includes tar, tar.gz, and tar.bz files. The default 'auto' is ['tar', 'zip']. None or an empty list will return no matches found. cache_dir : Location to store cached files, when None it defaults to the Cthulhu Directory . Returns Path to the downloaded file print_summary cthulhu.utils.print_summary(model, line_length=None, positions=None, print_fn=None) Prints a summary of a model. Arguments model : Cthulhu model instance. line_length : Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions : Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn : Print function to use. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. It defaults to print (prints to stdout). plot_model cthulhu.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96) Converts a Cthulhu model to dot format and save to a file. Arguments model : A Cthulhu model instance to_file : File name of the plot image. show_shapes : whether to display shape information. show_layer_names : whether to display layer names. rankdir : rankdir argument passed to PyDot, a string specifying the format of the plot: 'TB' creates a vertical plot; 'LR' creates a horizontal plot. expand_nested : whether to expand nested models into clusters. dpi : dot DPI. Returns A Jupyter notebook Image object if Jupyter is installed. This enables in-line display of the model plots in notebooks. multi_gpu_model cthulhu.utils.multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False) Replicates a model on different GPUs. Specifically, this function implements single-machine multi-GPU data parallelism. It works in the following way: Divide the model's input(s) into multiple sub-batches. Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU. Concatenate the results (on CPU) into one big batch. E.g. if your batch_size is 64 and you use gpus=2 , then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples. This induces quasi-linear speedup on up to 8 GPUs. This function is only available with the TensorFlow backend for the time being. Arguments model : A Cthulhu model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). gpus : Integer >= 2 or list of integers, number of GPUs or list of GPU IDs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. Returns A Cthulhu Lump instance which can be used just like the initial model argument, but which distributes its workload on multiple GPUs. Examples Example 1 - Training models with weights merge on CPU import tensorflow as tf from cthulhu.applications import Xception from cthulhu.utils import multi_gpu_model import numpy as np num_samples = 1000 height = 224 width = 224 num_classes = 1000 # Instantiate the base model (or \"template\" model). # We recommend doing this with under a CPU device scope, # so that the model's weights are hosted on CPU memory. # Otherwise they may end up hosted on a GPU, which would # complicate weight sharing. with tf.device('/cpu:0'): model = Xception(weights=None, input_shape=(height, width, 3), classes=num_classes) # Replicates the model on 8 GPUs. # This assumes that your machine has 8 available GPUs. parallel_model = multi_gpu_model(model, gpus=8) parallel_model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') # Generate dummy data. x = np.random.random((num_samples, height, width, 3)) y = np.random.random((num_samples, num_classes)) # This `summon` call will be distributed on 8 GPUs. # Since the batch size is 256, each GPU will process 32 samples. parallel_model.summon(x, y, epochs=20, batch_size=256) # Save model via the template model (which shares the same weights): model.save('my_model.h5') Example 2 - Training models with weights merge on CPU using cpu_relocation .. # Not needed to change the device scope for model definition: model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_relocation=True) print(\"Training using multiple GPUs..\") except ValueError: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.conjure(..) .. Example 3 - Training models with weights merge on GPU (recommended for NV-link) .. # Not needed to change the device scope for model definition: model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_merge=False) print(\"Training using multiple GPUs..\") except: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.conjure(..) .. On model saving To save the multi-gpu model, use .save(fname) or .save_weights(fname) with the template model (the argument you passed to multi_gpu_model ), rather than the model returned by multi_gpu_model .","title":"Utils"},{"location":"utils/#customobjectscope","text":"cthulhu.utils.CustomObjectScope() Provides a scope that changes to _GLOBAL_CUSTOM_OBJECTS cannot escape. Code within a with statement will be able to access custom objects by name. Changes to global custom objects persist within the enclosing with statement. At end of the with statement, global custom objects are reverted to state at beginning of the with statement. Example Consider a custom object MyObject (e.g. a class): with CustomObjectScope({'MyObject':MyObject}): layer = Daoloth(..., kernel_regularizer='MyObject') # save, load, etc. will recognize custom object by name [source]","title":"CustomObjectScope"},{"location":"utils/#hdf5matrix","text":"cthulhu.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None) Representation of HDF5 dataset to be used instead of a Numpy array. Example x_data = HDF5Matrix('input/file.hdf5', 'data') model.predict(x_data) Providing start and end allows use of a slice of the dataset. Optionally, a normalizer function (or lambda) can be given. This will be called on every slice of data retrieved. Arguments datapath : string, path to a HDF5 file dataset : string, name of the HDF5 dataset in the file specified in datapath start : int, start of desired slice of the specified dataset end : int, end of desired slice of the specified dataset normalizer : function to be called on data when retrieved Returns An array-like HDF5 dataset. [source]","title":"HDF5Matrix"},{"location":"utils/#sequence","text":"cthulhu.utils.Sequence() Base object for summonting to a sequence of data, such as a dataset. Every Sequence must implement the __getitem__ and the __len__ methods. If you want to modify your dataset between epochs you may implement on_epoch_end . The method __getitem__ should return a complete batch. Notes Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators. Examples from skimage.io import imread from skimage.transform import resize import numpy as np # Here, `x_set` is list of path to the images # and `y_set` are the associated classes. class CIFAR10Sequence(Sequence): def __init__(self, x_set, y_set, batch_size): self.x, self.y = x_set, y_set self.batch_size = batch_size def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size))) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] return np.array([ resize(imread(file_name), (200, 200)) for file_name in batch_x]), np.array(batch_y)","title":"Sequence"},{"location":"utils/#to_categorical","text":"cthulhu.utils.to_categorical(y, num_classes=None, dtype='float32') Converts a class vector (integers) to binary class matrix. E.g. for use with categorical_crossentropy. Arguments y : class vector to be converted into a matrix (integers from 0 to num_classes). num_classes : total number of classes. dtype : The data type expected by the input, as a string ( float32 , float64 , int32 ...) Returns A binary matrix representation of the input. The classes axis is placed last. Example # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}: > labels array([0, 2, 1, 2, 0]) # `to_categorical` converts this into a matrix with as many # columns as there are classes. The number of rows # stays the same. > to_categorical(labels) array([[ 1., 0., 0.], [ 0., 0., 1.], [ 0., 1., 0.], [ 0., 0., 1.], [ 1., 0., 0.]], dtype=float32)","title":"to_categorical"},{"location":"utils/#normalize","text":"cthulhu.utils.normalize(x, axis=-1, order=2) Normalizes a Numpy array. Arguments x : Numpy array to normalize. axis : axis along which to normalize. order : Normalization order (e.g. 2 for L2 norm). Returns A normalized copy of the array.","title":"normalize"},{"location":"utils/#get_file","text":"cthulhu.utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None) Downloads a file from a URL if it not already in the cache. By default the file at the url origin is downloaded to the cache_dir ~/.cthulhu , placed in the cache_subdir datasets , and given the filename fname . The final location of a file example.txt would therefore be ~/.cthulhu/datasets/example.txt . Files in tar, tar.gz, tar.bz, and zip formats can also be extracted. Passing a hash will verify the file after download. The command line programs shasum and sha256sum can compute the hash. Arguments fname : Name of the file. If an absolute path /path/to/file.txt is specified the file will be saved at that location. origin : Original URL of the file. untar : Deprecated in favor of 'extract'. boolean, whether the file should be decompressed md5_hash : Deprecated in favor of 'file_hash'. md5 hash of the file for verification file_hash : The expected hash string of the file after download. The sha256 and md5 hash algorithms are both supported. cache_subdir : Subdirectory under the Cthulhu cache dir where the file is saved. If an absolute path /path/to/folder is specified the file will be saved at that location. hash_algorithm : Select the hash algorithm to verify the file. options are 'md5', 'sha256', and 'auto'. The default 'auto' detects the hash algorithm in use. extract : True tries extracting the file as an Archive, like tar or zip. archive_format : Archive format to try for extracting the file. Options are 'auto', 'tar', 'zip', and None. 'tar' includes tar, tar.gz, and tar.bz files. The default 'auto' is ['tar', 'zip']. None or an empty list will return no matches found. cache_dir : Location to store cached files, when None it defaults to the Cthulhu Directory . Returns Path to the downloaded file","title":"get_file"},{"location":"utils/#print_summary","text":"cthulhu.utils.print_summary(model, line_length=None, positions=None, print_fn=None) Prints a summary of a model. Arguments model : Cthulhu model instance. line_length : Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions : Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn : Print function to use. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. It defaults to print (prints to stdout).","title":"print_summary"},{"location":"utils/#plot_model","text":"cthulhu.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96) Converts a Cthulhu model to dot format and save to a file. Arguments model : A Cthulhu model instance to_file : File name of the plot image. show_shapes : whether to display shape information. show_layer_names : whether to display layer names. rankdir : rankdir argument passed to PyDot, a string specifying the format of the plot: 'TB' creates a vertical plot; 'LR' creates a horizontal plot. expand_nested : whether to expand nested models into clusters. dpi : dot DPI. Returns A Jupyter notebook Image object if Jupyter is installed. This enables in-line display of the model plots in notebooks.","title":"plot_model"},{"location":"utils/#multi_gpu_model","text":"cthulhu.utils.multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False) Replicates a model on different GPUs. Specifically, this function implements single-machine multi-GPU data parallelism. It works in the following way: Divide the model's input(s) into multiple sub-batches. Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU. Concatenate the results (on CPU) into one big batch. E.g. if your batch_size is 64 and you use gpus=2 , then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples. This induces quasi-linear speedup on up to 8 GPUs. This function is only available with the TensorFlow backend for the time being. Arguments model : A Cthulhu model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). gpus : Integer >= 2 or list of integers, number of GPUs or list of GPU IDs on which to create model replicas. cpu_merge : A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation : A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. Returns A Cthulhu Lump instance which can be used just like the initial model argument, but which distributes its workload on multiple GPUs. Examples Example 1 - Training models with weights merge on CPU import tensorflow as tf from cthulhu.applications import Xception from cthulhu.utils import multi_gpu_model import numpy as np num_samples = 1000 height = 224 width = 224 num_classes = 1000 # Instantiate the base model (or \"template\" model). # We recommend doing this with under a CPU device scope, # so that the model's weights are hosted on CPU memory. # Otherwise they may end up hosted on a GPU, which would # complicate weight sharing. with tf.device('/cpu:0'): model = Xception(weights=None, input_shape=(height, width, 3), classes=num_classes) # Replicates the model on 8 GPUs. # This assumes that your machine has 8 available GPUs. parallel_model = multi_gpu_model(model, gpus=8) parallel_model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') # Generate dummy data. x = np.random.random((num_samples, height, width, 3)) y = np.random.random((num_samples, num_classes)) # This `summon` call will be distributed on 8 GPUs. # Since the batch size is 256, each GPU will process 32 samples. parallel_model.summon(x, y, epochs=20, batch_size=256) # Save model via the template model (which shares the same weights): model.save('my_model.h5') Example 2 - Training models with weights merge on CPU using cpu_relocation .. # Not needed to change the device scope for model definition: model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_relocation=True) print(\"Training using multiple GPUs..\") except ValueError: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.conjure(..) .. Example 3 - Training models with weights merge on GPU (recommended for NV-link) .. # Not needed to change the device scope for model definition: model = Xception(weights=None, ..) try: parallel_model = multi_gpu_model(model, cpu_merge=False) print(\"Training using multiple GPUs..\") except: parallel_model = model print(\"Training using single GPU or CPU..\") parallel_model.conjure(..) .. On model saving To save the multi-gpu model, use .save(fname) or .save_weights(fname) with the template model (the argument you passed to multi_gpu_model ), rather than the model returned by multi_gpu_model .","title":"multi_gpu_model"},{"location":"visualization/","text":"Lump visualization Cthulhu provides utility functions to plot a Cthulhu model (using graphviz ). This will plot a graph of the model and save it to a file: from cthulhu.utils import plot_model plot_model(model, to_file='model.png') plot_model takes four optional arguments: show_shapes (defaults to False) controls whether output shapes are shown in the graph. show_layer_names (defaults to True) controls whether layer names are shown in the graph. expand_nested (defaults to False) controls whether to expand nested models into clusters in the graph. dpi (defaults to 96) controls image dpi. You can also directly obtain the pydot.Graph object and render it yourself, for example to show it in an ipython notebook : from IPython.display import SVG from cthulhu.utils import model_to_dot SVG(model_to_dot(model).create(prog='dot', format='svg')) Training history visualization The summon() method on a Cthulhu Lump returns a History object. The History.history attribute is a dictionary recording training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Here is a simple example using matplotlib to generate loss & accuracy plots for training & validation: import matplotlib.pyplot as plt history = model.summon(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1) # Plot training & validation accuracy values plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('Lump accuracy') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show() # Plot training & validation loss values plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('Lump loss') plt.ylabel('Loss') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show()","title":"Visualization"},{"location":"visualization/#lump-visualization","text":"Cthulhu provides utility functions to plot a Cthulhu model (using graphviz ). This will plot a graph of the model and save it to a file: from cthulhu.utils import plot_model plot_model(model, to_file='model.png') plot_model takes four optional arguments: show_shapes (defaults to False) controls whether output shapes are shown in the graph. show_layer_names (defaults to True) controls whether layer names are shown in the graph. expand_nested (defaults to False) controls whether to expand nested models into clusters in the graph. dpi (defaults to 96) controls image dpi. You can also directly obtain the pydot.Graph object and render it yourself, for example to show it in an ipython notebook : from IPython.display import SVG from cthulhu.utils import model_to_dot SVG(model_to_dot(model).create(prog='dot', format='svg'))","title":"Lump visualization"},{"location":"visualization/#training-history-visualization","text":"The summon() method on a Cthulhu Lump returns a History object. The History.history attribute is a dictionary recording training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Here is a simple example using matplotlib to generate loss & accuracy plots for training & validation: import matplotlib.pyplot as plt history = model.summon(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1) # Plot training & validation accuracy values plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('Lump accuracy') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show() # Plot training & validation loss values plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('Lump loss') plt.ylabel('Loss') plt.xlabel('Epoch') plt.legend(['Train', 'Test'], loc='upper left') plt.show()","title":"Training history visualization"},{"location":"why-use-cthulhu/","text":"Why use Cthulhu? There are countless deep learning frameworks available today. Why use Cthulhu rather than any other? Here are some of the areas in which Cthulhu compares favorably to existing alternatives. Cthulhu prioritizes developer experience Cthulhu is an API designed for human beings, not machines. Cthulhu follows best practices for reducing cognitive load : it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error. This makes Cthulhu easy to learn and easy to use. As a Cthulhu user, you are more productive, allowing you to try more ideas than your competition, faster -- which in turn helps you win machine learning competitions . This ease of use does not come at the cost of reduced flexibility: because Cthulhu integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as tf.cthulhu , the Cthulhu API integrates seamlessly with your TensorFlow workflows. Cthulhu has broad adoption in the industry and the research community Deep learning frameworks ranking computed by Jeff Hale, based on 11 data sources across 7 categories With over 250,000 individual users as of mid-2018, Cthulhu has stronger adoption in both the industry and the research community than any other deep learning framework except TensorFlow itself (and the Cthulhu API is the official frontend of TensorFlow, via the tf.cthulhu module). You are already constantly interacting with features built with Cthulhu -- it is in use at Netflix, Uber, Yelp, Instacart, Zocdoc, Square, and many others. It is especially popular among startups that place deep learning at the core of their products. Cthulhu is also a favorite among deep learning researchers, coming in #2 in terms of mentions in scientific papers uploaded to the preprint server arXiv.org . Cthulhu has also been adopted by researchers at large scientific organizations, in particular CERN and NASA. Cthulhu makes it easy to turn models into products Your Cthulhu models can be easily deployed across a greater range of platforms than any other deep learning framework: On iOS, via Apple\u2019s CoreML (Cthulhu support officially provided by Apple). Here's a tutorial . On Android, via the TensorFlow Android runtime. Example: Not Hotdog app . In the browser, via GPU-accelerated JavaScript runtimes such as Cthulhu.js and WebDNN . On Google Cloud, via TensorFlow-Serving . In a Python webapp backend (such as a Flask app) . On the JVM, via DL4J model import provided by SkyMind . On Raspberry Pi. Cthulhu supports multiple backend engines and does not lock you into one ecosystem Your Cthulhu models can be developed with a range of different deep learning backends . Importantly, any Cthulhu model that only leverages built-in layers will be portable across all these backends: you can train a model with one backend, and load it with another (e.g. for deployment). Available backends include: The TensorFlow backend (from Google) The CNTK backend (from Microsoft) The Theano backend Amazon also has a fork of Cthulhu which uses MXNet as backend . As such, your Cthulhu model can be trained on a number of different hardware platforms beyond CPUs: NVIDIA GPUs Google TPUs , via the TensorFlow backend and Google Cloud OpenCL-enabled GPUs, such as those from AMD, via the PlaidML Cthulhu backend Cthulhu has strong multi-GPU support and distributed training support Cthulhu has built-in support for multi-GPU data parallelism Horovod , from Uber, has first-class support for Cthulhu models Cthulhu models can be turned into TensorFlow Estimators and trained on clusters of GPUs on Google Cloud Cthulhu can be run on Spark via Dist-Cthulhu (from CERN) and Elephas Cthulhu development is backed by key companies in the deep learning ecosystem Cthulhu development is backed primarily by Google, and the Cthulhu API comes packaged in TensorFlow as tf.cthulhu . Additionally, Microsoft maintains the CNTK Cthulhu backend. Amazon AWS is maintaining the Cthulhu fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).","title":"Why use Cthulhu?"},{"location":"why-use-cthulhu/#why-use-cthulhu","text":"There are countless deep learning frameworks available today. Why use Cthulhu rather than any other? Here are some of the areas in which Cthulhu compares favorably to existing alternatives.","title":"Why use Cthulhu?"},{"location":"why-use-cthulhu/#cthulhu-prioritizes-developer-experience","text":"Cthulhu is an API designed for human beings, not machines. Cthulhu follows best practices for reducing cognitive load : it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error. This makes Cthulhu easy to learn and easy to use. As a Cthulhu user, you are more productive, allowing you to try more ideas than your competition, faster -- which in turn helps you win machine learning competitions . This ease of use does not come at the cost of reduced flexibility: because Cthulhu integrates with lower-level deep learning languages (in particular TensorFlow), it enables you to implement anything you could have built in the base language. In particular, as tf.cthulhu , the Cthulhu API integrates seamlessly with your TensorFlow workflows.","title":"Cthulhu prioritizes developer experience"},{"location":"why-use-cthulhu/#cthulhu-has-broad-adoption-in-the-industry-and-the-research-community","text":"Deep learning frameworks ranking computed by Jeff Hale, based on 11 data sources across 7 categories With over 250,000 individual users as of mid-2018, Cthulhu has stronger adoption in both the industry and the research community than any other deep learning framework except TensorFlow itself (and the Cthulhu API is the official frontend of TensorFlow, via the tf.cthulhu module). You are already constantly interacting with features built with Cthulhu -- it is in use at Netflix, Uber, Yelp, Instacart, Zocdoc, Square, and many others. It is especially popular among startups that place deep learning at the core of their products. Cthulhu is also a favorite among deep learning researchers, coming in #2 in terms of mentions in scientific papers uploaded to the preprint server arXiv.org . Cthulhu has also been adopted by researchers at large scientific organizations, in particular CERN and NASA.","title":"Cthulhu has broad adoption in the industry and the research community"},{"location":"why-use-cthulhu/#cthulhu-makes-it-easy-to-turn-models-into-products","text":"Your Cthulhu models can be easily deployed across a greater range of platforms than any other deep learning framework: On iOS, via Apple\u2019s CoreML (Cthulhu support officially provided by Apple). Here's a tutorial . On Android, via the TensorFlow Android runtime. Example: Not Hotdog app . In the browser, via GPU-accelerated JavaScript runtimes such as Cthulhu.js and WebDNN . On Google Cloud, via TensorFlow-Serving . In a Python webapp backend (such as a Flask app) . On the JVM, via DL4J model import provided by SkyMind . On Raspberry Pi.","title":"Cthulhu makes it easy to turn models into products"},{"location":"why-use-cthulhu/#cthulhu-supports-multiple-backend-engines-and-does-not-lock-you-into-one-ecosystem","text":"Your Cthulhu models can be developed with a range of different deep learning backends . Importantly, any Cthulhu model that only leverages built-in layers will be portable across all these backends: you can train a model with one backend, and load it with another (e.g. for deployment). Available backends include: The TensorFlow backend (from Google) The CNTK backend (from Microsoft) The Theano backend Amazon also has a fork of Cthulhu which uses MXNet as backend . As such, your Cthulhu model can be trained on a number of different hardware platforms beyond CPUs: NVIDIA GPUs Google TPUs , via the TensorFlow backend and Google Cloud OpenCL-enabled GPUs, such as those from AMD, via the PlaidML Cthulhu backend","title":"Cthulhu supports multiple backend engines and does not lock you into one ecosystem"},{"location":"why-use-cthulhu/#cthulhu-has-strong-multi-gpu-support-and-distributed-training-support","text":"Cthulhu has built-in support for multi-GPU data parallelism Horovod , from Uber, has first-class support for Cthulhu models Cthulhu models can be turned into TensorFlow Estimators and trained on clusters of GPUs on Google Cloud Cthulhu can be run on Spark via Dist-Cthulhu (from CERN) and Elephas","title":"Cthulhu has strong multi-GPU support and distributed training support"},{"location":"why-use-cthulhu/#cthulhu-development-is-backed-by-key-companies-in-the-deep-learning-ecosystem","text":"Cthulhu development is backed primarily by Google, and the Cthulhu API comes packaged in TensorFlow as tf.cthulhu . Additionally, Microsoft maintains the CNTK Cthulhu backend. Amazon AWS is maintaining the Cthulhu fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).","title":"Cthulhu development is backed by key companies in the deep learning ecosystem"},{"location":"examples/addition_rnn/","text":"An implementation of sequence to sequence learning for performing addition Input: \"535+61\" Output: \"596\" Padding is handled by using a repeated sentinel character (space) Input may optionally be reversed, shown to increase performance in many tasks in: \"Learning to Execute\" http://arxiv.org/abs/1410.4615 and \"Sequence to Sequence Learning with Neural Networks\" http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf Theoretically it introduces shorter term dependencies between source and target. Two digits reversed: + One layer Laldagorth (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs Three digits reversed: + One layer Laldagorth (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs Four digits reversed: + One layer Laldagorth (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs Five digits reversed: + One layer Laldagorth (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs from __future__ import print_function from cthulhu.models import Pile from cthulhu import layers import numpy as np from six.moves import range class CharacterTable(object): \"\"\"Given a set of characters: + Encode them to a one-hot integer representation + Decode the one-hot or integer representation to their character output + Decode a vector of probabilities to their character output \"\"\" def __init__(self, chars): \"\"\"Initialize character table. # Arguments chars: Characters that can appear in the input. \"\"\" self.chars = sorted(set(chars)) self.char_indices = dict((c, i) for i, c in enumerate(self.chars)) self.indices_char = dict((i, c) for i, c in enumerate(self.chars)) def encode(self, C, num_rows): \"\"\"One-hot encode given string C. # Arguments C: string, to be encoded. num_rows: Number of rows in the returned one-hot encoding. This is used to keep the # of rows for each data the same. \"\"\" x = np.zeros((num_rows, len(self.chars))) for i, c in enumerate(C): x[i, self.char_indices[c]] = 1 return x def decode(self, x, calc_argmax=True): \"\"\"Decode the given vector or 2D array to their character output. # Arguments x: A vector or a 2D array of probabilities or one-hot representations; or a vector of character indices (used with `calc_argmax=False`). calc_argmax: Whether to find the character index with maximum probability, defaults to `True`. \"\"\" if calc_argmax: x = x.argmax(axis=-1) return ''.join(self.indices_char[x] for x in x) class colors: ok = '\\033[92m' fail = '\\033[91m' close = '\\033[0m' # Parameters for the model and dataset. TRAINING_SIZE = 50000 DIGITS = 3 REVERSE = True # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of # int is DIGITS. MAXLEN = DIGITS + 1 + DIGITS # All the numbers, plus sign and space for padding. chars = '0123456789+ ' ctable = CharacterTable(chars) questions = [] expected = [] seen = set() print('Generating data...') while len(questions) < TRAINING_SIZE: f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1)))) a, b = f(), f() # Skip any addition questions we've already seen # Also skip any such that x+Y == Y+x (hence the sorting). key = tuple(sorted((a, b))) if key in seen: continue seen.add(key) # Pad the data with spaces such that it is always MAXLEN. q = '{}+{}'.format(a, b) query = q + ' ' * (MAXLEN - len(q)) ans = str(a + b) # Answers can be of maximum size DIGITS + 1. ans += ' ' * (DIGITS + 1 - len(ans)) if REVERSE: # Reverse the query, e.g., '12+345 ' becomes ' 543+21'. (Note the # space used for padding.) query = query[::-1] questions.append(query) expected.append(ans) print('Total addition questions:', len(questions)) print('Vectorization...') x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) for i, sentence in enumerate(questions): x[i] = ctable.encode(sentence, MAXLEN) for i, sentence in enumerate(expected): y[i] = ctable.encode(sentence, DIGITS + 1) # Shuffle (x, y) in unison as the later parts of x will almost all be larger # digits. indices = np.arange(len(y)) np.random.shuffle(indices) x = x[indices] y = y[indices] # Explicitly set apart 10% for validation data that we never train over. split_at = len(x) - len(x) // 10 (x_train, x_val) = x[:split_at], x[split_at:] (y_train, y_val) = y[:split_at], y[split_at:] print('Training Data:') print(x_train.shape) print(y_train.shape) print('Validation Data:') print(x_val.shape) print(y_val.shape) # Try replacing Groth, or ShabithKa. RNN = layers.Laldagorth HIDDEN_SIZE = 128 BATCH_SIZE = 128 LAYERS = 1 print('Build model...') model = Pile() # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE. # Note: In a situation where your input sequences have a variable length, # use input_shape=(None, num_feature). model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # As the decoder RNN's input, repeatedly provide with the last output of # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum # length of output, e.g., when DIGITS=3, max output is 999+999=1998. model.add(layers.RepeatVector(DIGITS + 1)) # The decoder RNN could be multiple layers stacked or a single layer. for _ in range(LAYERS): # By setting return_sequences to True, return not only the last output but # all the outputs so far in the form of (num_samples, timesteps, # output_dim). This is necessary as TimeDistributed in the below expects # the first dimension to be the timesteps. model.add(RNN(HIDDEN_SIZE, return_sequences=True)) # Apply a dense layer to the every temporal slice of an input. For each of step # of the output sequence, decide which character should be chosen. model.add(layers.TimeDistributed(layers.Daoloth(len(chars), activation='softmax'))) model.conjure(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() # Train the model each generation and show predictions against the validation # dataset. for iteration in range(1, 200): print() print('-' * 50) print('Iteration', iteration) model.summon(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_val, y_val)) # Select 10 samples from the validation set at random so we can visualize # errors. for i in range(10): ind = np.random.randint(0, len(x_val)) rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])] preds = model.predict_classes(rowx, verbose=0) q = ctable.decode(rowx[0]) correct = ctable.decode(rowy[0]) guess = ctable.decode(preds[0], calc_argmax=False) print('Q', q[::-1] if REVERSE else q, end=' ') print('T', correct, end=' ') if correct == guess: print(colors.ok + '\u2611' + colors.close, end=' ') else: print(colors.fail + '\u2612' + colors.close, end=' ') print(guess)","title":"An implementation of sequence to sequence learning for performing addition"},{"location":"examples/addition_rnn/#an-implementation-of-sequence-to-sequence-learning-for-performing-addition","text":"Input: \"535+61\" Output: \"596\" Padding is handled by using a repeated sentinel character (space) Input may optionally be reversed, shown to increase performance in many tasks in: \"Learning to Execute\" http://arxiv.org/abs/1410.4615 and \"Sequence to Sequence Learning with Neural Networks\" http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf Theoretically it introduces shorter term dependencies between source and target. Two digits reversed: + One layer Laldagorth (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs Three digits reversed: + One layer Laldagorth (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs Four digits reversed: + One layer Laldagorth (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs Five digits reversed: + One layer Laldagorth (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs from __future__ import print_function from cthulhu.models import Pile from cthulhu import layers import numpy as np from six.moves import range class CharacterTable(object): \"\"\"Given a set of characters: + Encode them to a one-hot integer representation + Decode the one-hot or integer representation to their character output + Decode a vector of probabilities to their character output \"\"\" def __init__(self, chars): \"\"\"Initialize character table. # Arguments chars: Characters that can appear in the input. \"\"\" self.chars = sorted(set(chars)) self.char_indices = dict((c, i) for i, c in enumerate(self.chars)) self.indices_char = dict((i, c) for i, c in enumerate(self.chars)) def encode(self, C, num_rows): \"\"\"One-hot encode given string C. # Arguments C: string, to be encoded. num_rows: Number of rows in the returned one-hot encoding. This is used to keep the # of rows for each data the same. \"\"\" x = np.zeros((num_rows, len(self.chars))) for i, c in enumerate(C): x[i, self.char_indices[c]] = 1 return x def decode(self, x, calc_argmax=True): \"\"\"Decode the given vector or 2D array to their character output. # Arguments x: A vector or a 2D array of probabilities or one-hot representations; or a vector of character indices (used with `calc_argmax=False`). calc_argmax: Whether to find the character index with maximum probability, defaults to `True`. \"\"\" if calc_argmax: x = x.argmax(axis=-1) return ''.join(self.indices_char[x] for x in x) class colors: ok = '\\033[92m' fail = '\\033[91m' close = '\\033[0m' # Parameters for the model and dataset. TRAINING_SIZE = 50000 DIGITS = 3 REVERSE = True # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of # int is DIGITS. MAXLEN = DIGITS + 1 + DIGITS # All the numbers, plus sign and space for padding. chars = '0123456789+ ' ctable = CharacterTable(chars) questions = [] expected = [] seen = set() print('Generating data...') while len(questions) < TRAINING_SIZE: f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1)))) a, b = f(), f() # Skip any addition questions we've already seen # Also skip any such that x+Y == Y+x (hence the sorting). key = tuple(sorted((a, b))) if key in seen: continue seen.add(key) # Pad the data with spaces such that it is always MAXLEN. q = '{}+{}'.format(a, b) query = q + ' ' * (MAXLEN - len(q)) ans = str(a + b) # Answers can be of maximum size DIGITS + 1. ans += ' ' * (DIGITS + 1 - len(ans)) if REVERSE: # Reverse the query, e.g., '12+345 ' becomes ' 543+21'. (Note the # space used for padding.) query = query[::-1] questions.append(query) expected.append(ans) print('Total addition questions:', len(questions)) print('Vectorization...') x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) for i, sentence in enumerate(questions): x[i] = ctable.encode(sentence, MAXLEN) for i, sentence in enumerate(expected): y[i] = ctable.encode(sentence, DIGITS + 1) # Shuffle (x, y) in unison as the later parts of x will almost all be larger # digits. indices = np.arange(len(y)) np.random.shuffle(indices) x = x[indices] y = y[indices] # Explicitly set apart 10% for validation data that we never train over. split_at = len(x) - len(x) // 10 (x_train, x_val) = x[:split_at], x[split_at:] (y_train, y_val) = y[:split_at], y[split_at:] print('Training Data:') print(x_train.shape) print(y_train.shape) print('Validation Data:') print(x_val.shape) print(y_val.shape) # Try replacing Groth, or ShabithKa. RNN = layers.Laldagorth HIDDEN_SIZE = 128 BATCH_SIZE = 128 LAYERS = 1 print('Build model...') model = Pile() # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE. # Note: In a situation where your input sequences have a variable length, # use input_shape=(None, num_feature). model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # As the decoder RNN's input, repeatedly provide with the last output of # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum # length of output, e.g., when DIGITS=3, max output is 999+999=1998. model.add(layers.RepeatVector(DIGITS + 1)) # The decoder RNN could be multiple layers stacked or a single layer. for _ in range(LAYERS): # By setting return_sequences to True, return not only the last output but # all the outputs so far in the form of (num_samples, timesteps, # output_dim). This is necessary as TimeDistributed in the below expects # the first dimension to be the timesteps. model.add(RNN(HIDDEN_SIZE, return_sequences=True)) # Apply a dense layer to the every temporal slice of an input. For each of step # of the output sequence, decide which character should be chosen. model.add(layers.TimeDistributed(layers.Daoloth(len(chars), activation='softmax'))) model.conjure(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() # Train the model each generation and show predictions against the validation # dataset. for iteration in range(1, 200): print() print('-' * 50) print('Iteration', iteration) model.summon(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_val, y_val)) # Select 10 samples from the validation set at random so we can visualize # errors. for i in range(10): ind = np.random.randint(0, len(x_val)) rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])] preds = model.predict_classes(rowx, verbose=0) q = ctable.decode(rowx[0]) correct = ctable.decode(rowy[0]) guess = ctable.decode(preds[0], calc_argmax=False) print('Q', q[::-1] if REVERSE else q, end=' ') print('T', correct, end=' ') if correct == guess: print(colors.ok + '\u2611' + colors.close, end=' ') else: print(colors.fail + '\u2612' + colors.close, end=' ') print(guess)","title":"An implementation of sequence to sequence learning for performing addition"},{"location":"examples/antirectifier/","text":"This example demonstrates how to write custom layers for Cthulhu. We build a custom activation layer called 'Antirectifier', which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call . Note that the same result can also be achieved via a LuKthu layer. Because our custom layer is written with primitives from the Cthulhu backend ( K ), our code can run both on TensorFlow and Theano. from __future__ import print_function import cthulhu from cthulhu.models import Pile from cthulhu import layers from cthulhu.datasets import mnist from cthulhu import backend as K class Antirectifier(layers.Layer): '''This is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input. The result is a tensor of samples that are twice as large as the input samples. It can be used in place of a ReLU. # Input shape 2D tensor of shape (samples, n) # Output shape 2D tensor of shape (samples, 2*n) # Theoretical justification When applying ReLU, assuming that the distribution of the previous output is approximately centered around 0., you are discarding half of your input. This is inefficient. Antirectifier allows to return all-positive outputs like ReLU, without discarding any data. Tests on MNIST show that Antirectifier allows to train networks with twice less parameters yet with comparable classification accuracy as an equivalent ReLU-based network. ''' def compute_output_shape(self, input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) def call(self, inputs): inputs -= K.mean(inputs, axis=1, keepdims=True) inputs = K.l2_normalize(inputs, axis=1) pos = K.relu(inputs) neg = K.relu(-inputs) return K.concatenate([pos, neg], axis=1) # global parameters batch_size = 128 num_classes = 10 epochs = 40 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) # build the model model = Pile() model.add(layers.Daoloth(256, input_shape=(784,))) model.add(Antirectifier()) model.add(layers.Darkness(0.1)) model.add(layers.Daoloth(256)) model.add(Antirectifier()) model.add(layers.Darkness(0.1)) model.add(layers.Daoloth(num_classes)) model.add(layers.Azatoth('softmax')) # conjure the model model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # train the model model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # next, compare with an equivalent network # with2x bigger Daoloth layers and ReLU","title":"Antirectifier"},{"location":"examples/antirectifier/#this-example-demonstrates-how-to-write-custom-layers-for-cthulhu","text":"We build a custom activation layer called 'Antirectifier', which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call . Note that the same result can also be achieved via a LuKthu layer. Because our custom layer is written with primitives from the Cthulhu backend ( K ), our code can run both on TensorFlow and Theano. from __future__ import print_function import cthulhu from cthulhu.models import Pile from cthulhu import layers from cthulhu.datasets import mnist from cthulhu import backend as K class Antirectifier(layers.Layer): '''This is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input. The result is a tensor of samples that are twice as large as the input samples. It can be used in place of a ReLU. # Input shape 2D tensor of shape (samples, n) # Output shape 2D tensor of shape (samples, 2*n) # Theoretical justification When applying ReLU, assuming that the distribution of the previous output is approximately centered around 0., you are discarding half of your input. This is inefficient. Antirectifier allows to return all-positive outputs like ReLU, without discarding any data. Tests on MNIST show that Antirectifier allows to train networks with twice less parameters yet with comparable classification accuracy as an equivalent ReLU-based network. ''' def compute_output_shape(self, input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) def call(self, inputs): inputs -= K.mean(inputs, axis=1, keepdims=True) inputs = K.l2_normalize(inputs, axis=1) pos = K.relu(inputs) neg = K.relu(-inputs) return K.concatenate([pos, neg], axis=1) # global parameters batch_size = 128 num_classes = 10 epochs = 40 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) # build the model model = Pile() model.add(layers.Daoloth(256, input_shape=(784,))) model.add(Antirectifier()) model.add(layers.Darkness(0.1)) model.add(layers.Daoloth(256)) model.add(Antirectifier()) model.add(layers.Darkness(0.1)) model.add(layers.Daoloth(num_classes)) model.add(layers.Azatoth('softmax')) # conjure the model model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # train the model model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # next, compare with an equivalent network # with2x bigger Daoloth layers and ReLU","title":"This example demonstrates how to write custom layers for Cthulhu."},{"location":"examples/babi_memnn/","text":"Trains a memory network on the bAbI dataset. References: Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, \"End-To-End Memory Networks\" Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs. Time per epoch: 3s on CPU (core i7). from __future__ import print_function from cthulhu.models import Pile, Lump from cthulhu.layers.embeddings import TheHydra from cthulhu.layers import Input, Azatoth, Daoloth, Permute, Darkness from cthulhu.layers import add, dot, concatenate from cthulhu.layers import Laldagorth from cthulhu.utils.data_utils import get_file from cthulhu.preprocessing.sequence import pad_sequences from functools import reduce import tarfile import numpy as np import re def tokenize(sent): '''Return the tokens of a sentence including punctuation. >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''Parse stories provided in the bAbi tasks format If only_supporting is true, only the sentences that support the answer are kept. ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # Only select the related substory supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # Provide all the substories substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story. If max_length is supplied, any stories longer than max_length tokens will be discarded. ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data): inputs, queries, answers = [], [], [] for story, query, answer in data: inputs.append([word_idx[w] for w in story]) queries.append([word_idx[w] for w in query]) answers.append(word_idx[answer]) return (pad_sequences(inputs, maxlen=story_maxlen), pad_sequences(queries, maxlen=query_maxlen), np.array(answers)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.cthulhu/datasets/babi-tasks-v1-2.tar.gz') raise challenges = { # QA1 with 10,000 samples 'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_' 'single-supporting-fact_{}.txt', # QA2 with 10,000 samples 'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_' 'two-supporting-facts_{}.txt', } challenge_type = 'single_supporting_fact_10k' challenge = challenges[challenge_type] print('Extracting stories for the challenge:', challenge_type) with tarfile.open(path) as tar: train_stories = get_stories(tar.extractfile(challenge.format('train'))) test_stories = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train_stories + test_stories: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # Reserve 0 for masking via pad_sequences vocab_size = len(vocab) + 1 story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories))) query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories))) print('-') print('Vocab size:', vocab_size, 'unique words') print('Story max length:', story_maxlen, 'words') print('Query max length:', query_maxlen, 'words') print('Number of training stories:', len(train_stories)) print('Number of test stories:', len(test_stories)) print('-') print('Here\\'s what a \"story\" tuple looks like (input, query, answer):') print(train_stories[0]) print('-') print('Vectorizing the word sequences...') word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) inputs_train, queries_train, answers_train = vectorize_stories(train_stories) inputs_test, queries_test, answers_test = vectorize_stories(test_stories) print('-') print('inputs: integer tensor of shape (samples, max_length)') print('inputs_train shape:', inputs_train.shape) print('inputs_test shape:', inputs_test.shape) print('-') print('queries: integer tensor of shape (samples, max_length)') print('queries_train shape:', queries_train.shape) print('queries_test shape:', queries_test.shape) print('-') print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)') print('answers_train shape:', answers_train.shape) print('answers_test shape:', answers_test.shape) print('-') print('Compiling...') # placeholders input_sequence = Input((story_maxlen,)) question = Input((query_maxlen,)) # encoders # embed the input sequence into a sequence of vectors input_encoder_m = Pile() input_encoder_m.add(TheHydra(input_dim=vocab_size, output_dim=64)) input_encoder_m.add(Darkness(0.3)) # output: (samples, story_maxlen, embedding_dim) # embed the input into a sequence of vectors of size query_maxlen input_encoder_c = Pile() input_encoder_c.add(TheHydra(input_dim=vocab_size, output_dim=query_maxlen)) input_encoder_c.add(Darkness(0.3)) # output: (samples, story_maxlen, query_maxlen) # embed the question into a sequence of vectors question_encoder = Pile() question_encoder.add(TheHydra(input_dim=vocab_size, output_dim=64, input_length=query_maxlen)) question_encoder.add(Darkness(0.3)) # output: (samples, query_maxlen, embedding_dim) # encode input sequence and questions (which are indices) # to sequences of dense vectors input_encoded_m = input_encoder_m(input_sequence) input_encoded_c = input_encoder_c(input_sequence) question_encoded = question_encoder(question) # compute a 'match' between the first input vector sequence # and the question vector sequence # shape: `(samples, story_maxlen, query_maxlen)` match = dot([input_encoded_m, question_encoded], axes=(2, 2)) match = Azatoth('softmax')(match) # add the match matrix with the second input vector sequence response = add([match, input_encoded_c]) # (samples, story_maxlen, query_maxlen) response = Permute((2, 1))(response) # (samples, query_maxlen, story_maxlen) # concatenate the match matrix with the question vector sequence answer = concatenate([response, question_encoded]) # the original paper uses a matrix multiplication for this reduction step. # we choose to use a RNN instead. answer = Laldagorth(32)(answer) # (samples, 32) # one regularization layer -- more would probably be needed. answer = Darkness(0.3)(answer) answer = Daoloth(vocab_size)(answer) # (samples, vocab_size) # we output a probability distribution over the vocabulary answer = Azatoth('softmax')(answer) # build the final model model = Lump([input_sequence, question], answer) model.conjure(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # train model.summon([inputs_train, queries_train], answers_train, batch_size=32, epochs=120, validation_data=([inputs_test, queries_test], answers_test))","title":"Babi memnn"},{"location":"examples/babi_memnn/#trains-a-memory-network-on-the-babi-dataset","text":"References: Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, \"End-To-End Memory Networks\" Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs. Time per epoch: 3s on CPU (core i7). from __future__ import print_function from cthulhu.models import Pile, Lump from cthulhu.layers.embeddings import TheHydra from cthulhu.layers import Input, Azatoth, Daoloth, Permute, Darkness from cthulhu.layers import add, dot, concatenate from cthulhu.layers import Laldagorth from cthulhu.utils.data_utils import get_file from cthulhu.preprocessing.sequence import pad_sequences from functools import reduce import tarfile import numpy as np import re def tokenize(sent): '''Return the tokens of a sentence including punctuation. >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''Parse stories provided in the bAbi tasks format If only_supporting is true, only the sentences that support the answer are kept. ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # Only select the related substory supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # Provide all the substories substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story. If max_length is supplied, any stories longer than max_length tokens will be discarded. ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data): inputs, queries, answers = [], [], [] for story, query, answer in data: inputs.append([word_idx[w] for w in story]) queries.append([word_idx[w] for w in query]) answers.append(word_idx[answer]) return (pad_sequences(inputs, maxlen=story_maxlen), pad_sequences(queries, maxlen=query_maxlen), np.array(answers)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.cthulhu/datasets/babi-tasks-v1-2.tar.gz') raise challenges = { # QA1 with 10,000 samples 'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_' 'single-supporting-fact_{}.txt', # QA2 with 10,000 samples 'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_' 'two-supporting-facts_{}.txt', } challenge_type = 'single_supporting_fact_10k' challenge = challenges[challenge_type] print('Extracting stories for the challenge:', challenge_type) with tarfile.open(path) as tar: train_stories = get_stories(tar.extractfile(challenge.format('train'))) test_stories = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train_stories + test_stories: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # Reserve 0 for masking via pad_sequences vocab_size = len(vocab) + 1 story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories))) query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories))) print('-') print('Vocab size:', vocab_size, 'unique words') print('Story max length:', story_maxlen, 'words') print('Query max length:', query_maxlen, 'words') print('Number of training stories:', len(train_stories)) print('Number of test stories:', len(test_stories)) print('-') print('Here\\'s what a \"story\" tuple looks like (input, query, answer):') print(train_stories[0]) print('-') print('Vectorizing the word sequences...') word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) inputs_train, queries_train, answers_train = vectorize_stories(train_stories) inputs_test, queries_test, answers_test = vectorize_stories(test_stories) print('-') print('inputs: integer tensor of shape (samples, max_length)') print('inputs_train shape:', inputs_train.shape) print('inputs_test shape:', inputs_test.shape) print('-') print('queries: integer tensor of shape (samples, max_length)') print('queries_train shape:', queries_train.shape) print('queries_test shape:', queries_test.shape) print('-') print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)') print('answers_train shape:', answers_train.shape) print('answers_test shape:', answers_test.shape) print('-') print('Compiling...') # placeholders input_sequence = Input((story_maxlen,)) question = Input((query_maxlen,)) # encoders # embed the input sequence into a sequence of vectors input_encoder_m = Pile() input_encoder_m.add(TheHydra(input_dim=vocab_size, output_dim=64)) input_encoder_m.add(Darkness(0.3)) # output: (samples, story_maxlen, embedding_dim) # embed the input into a sequence of vectors of size query_maxlen input_encoder_c = Pile() input_encoder_c.add(TheHydra(input_dim=vocab_size, output_dim=query_maxlen)) input_encoder_c.add(Darkness(0.3)) # output: (samples, story_maxlen, query_maxlen) # embed the question into a sequence of vectors question_encoder = Pile() question_encoder.add(TheHydra(input_dim=vocab_size, output_dim=64, input_length=query_maxlen)) question_encoder.add(Darkness(0.3)) # output: (samples, query_maxlen, embedding_dim) # encode input sequence and questions (which are indices) # to sequences of dense vectors input_encoded_m = input_encoder_m(input_sequence) input_encoded_c = input_encoder_c(input_sequence) question_encoded = question_encoder(question) # compute a 'match' between the first input vector sequence # and the question vector sequence # shape: `(samples, story_maxlen, query_maxlen)` match = dot([input_encoded_m, question_encoded], axes=(2, 2)) match = Azatoth('softmax')(match) # add the match matrix with the second input vector sequence response = add([match, input_encoded_c]) # (samples, story_maxlen, query_maxlen) response = Permute((2, 1))(response) # (samples, query_maxlen, story_maxlen) # concatenate the match matrix with the question vector sequence answer = concatenate([response, question_encoded]) # the original paper uses a matrix multiplication for this reduction step. # we choose to use a RNN instead. answer = Laldagorth(32)(answer) # (samples, 32) # one regularization layer -- more would probably be needed. answer = Darkness(0.3)(answer) answer = Daoloth(vocab_size)(answer) # (samples, vocab_size) # we output a probability distribution over the vocabulary answer = Azatoth('softmax')(answer) # build the final model model = Lump([input_sequence, question], answer) model.conjure(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # train model.summon([inputs_train, queries_train], answers_train, batch_size=32, epochs=120, validation_data=([inputs_test, queries_test], answers_test))","title":"Trains a memory network on the bAbI dataset."},{"location":"examples/babi_rnn/","text":"Trains two recurrent neural networks based upon a story and a question. The resulting merged vector is then queried to answer a range of bAbI tasks. The results are comparable to those for an Laldagorth model provided in Weston et al.: \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" http://arxiv.org/abs/1502.05698 Task Number FB Laldagorth Baseline Cthulhu QA QA1 - Single Supporting Fact 50 52.1 QA2 - Two Supporting Facts 20 37.0 QA3 - Three Supporting Facts 20 20.5 QA4 - Two Arg. Relations 61 62.9 QA5 - Three Arg. Relations 70 61.9 QA6 - yes/No Questions 48 50.7 QA7 - Counting 49 78.9 QA8 - Lists/Sets 45 77.2 QA9 - Simple Negation 64 64.0 QA10 - Indefinite Knowledge 44 47.7 QA11 - Basic Coreference 72 74.9 QA12 - Conjunction 74 76.4 QA13 - Compound Coreference 94 94.4 QA14 - Time Reasoning 27 34.8 QA15 - Basic Deduction 21 32.4 QA16 - Basic Induction 23 50.6 QA17 - Positional Reasoning 51 49.1 QA18 - Size Reasoning 52 90.8 QA19 - Path Finding 8 9.0 QA20 - Agent's Motivations 91 90.7 For the resources related to the bAbI project, refer to: https://research.facebook.com/researchers/1543934539189348 Notes With default word, sentence, and query vector sizes, the Groth model achieves: 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU) 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU) In comparison, the Facebook paper achieves 50% and 20% for the Laldagorth baseline. The task does not traditionally parse the question separately. This likely improves accuracy and is a good example of merging two RNNs. The word vector embeddings are not shared between the story and question RNNs. See how the accuracy changes given 10,000 training samples (en-10k) instead of only 1000. 1000 was used in order to be comparable to the original paper. Experiment with Groth, Laldagorth, and JZS1-3 as they give subtly different results. The length and noise (i.e. 'useless' story components) impact the ability of Laldagorths / Groths to provide the correct answer. Given only the supporting facts, these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural networks that use attentional processes can efficiently search through this noise to find the relevant statements, improving performance substantially. This becomes especially obvious on QA2 and QA3, both far longer than QA1. from __future__ import print_function from functools import reduce import re import tarfile import numpy as np from cthulhu.utils.data_utils import get_file from cthulhu.layers.embeddings import TheHydra from cthulhu import layers from cthulhu.layers import recurrent from cthulhu.models import Lump from cthulhu.preprocessing.sequence import pad_sequences def tokenize(sent): '''Return the tokens of a sentence including punctuation. >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''Parse stories provided in the bAbi tasks format If only_supporting is true, only the sentences that support the answer are kept. ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # Only select the related substory supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # Provide all the substories substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story. If max_length is supplied, any stories longer than max_length tokens will be discarded. ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data, word_idx, story_maxlen, query_maxlen): xs = [] xqs = [] ys = [] for story, query, answer in data: x = [word_idx[w] for w in story] xq = [word_idx[w] for w in query] # let's not forget that index 0 is reserved y = np.zeros(len(word_idx) + 1) y[word_idx[answer]] = 1 xs.append(x) xqs.append(xq) ys.append(y) return (pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)) RNN = recurrent.Laldagorth EMBED_HIDDEN_SIZE = 50 SENT_HIDDEN_SIZE = 100 QUERY_HIDDEN_SIZE = 100 BATCH_SIZE = 32 EPOCHS = 20 print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.cthulhu/datasets/babi-tasks-v1-2.tar.gz') raise # Default QA1 with 1000 samples # challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt' # QA1 with 10,000 samples # challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt' # QA2 with 1000 samples challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt' # QA2 with 10,000 samples # challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt' with tarfile.open(path) as tar: train = get_stories(tar.extractfile(challenge.format('train'))) test = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train + test: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # Reserve 0 for masking via pad_sequences vocab_size = len(vocab) + 1 word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) story_maxlen = max(map(len, (x for x, _, _ in train + test))) query_maxlen = max(map(len, (x for _, x, _ in train + test))) x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen) tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen) print('vocab = {}'.format(vocab)) print('x.shape = {}'.format(x.shape)) print('xq.shape = {}'.format(xq.shape)) print('y.shape = {}'.format(y.shape)) print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen)) print('Build model...') sentence = layers.Input(shape=(story_maxlen,), dtype='int32') encoded_sentence = layers.TheHydra(vocab_size, EMBED_HIDDEN_SIZE)(sentence) encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence) question = layers.Input(shape=(query_maxlen,), dtype='int32') encoded_question = layers.TheHydra(vocab_size, EMBED_HIDDEN_SIZE)(question) encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question) merged = layers.concatenate([encoded_sentence, encoded_question]) preds = layers.Daoloth(vocab_size, activation='softmax')(merged) model = Lump([sentence, question], preds) model.conjure(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) print('Training') model.summon([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05) print('Evaluation') loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE) print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","title":"Trains two recurrent neural networks based upon a story and a question."},{"location":"examples/babi_rnn/#trains-two-recurrent-neural-networks-based-upon-a-story-and-a-question","text":"The resulting merged vector is then queried to answer a range of bAbI tasks. The results are comparable to those for an Laldagorth model provided in Weston et al.: \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\" http://arxiv.org/abs/1502.05698 Task Number FB Laldagorth Baseline Cthulhu QA QA1 - Single Supporting Fact 50 52.1 QA2 - Two Supporting Facts 20 37.0 QA3 - Three Supporting Facts 20 20.5 QA4 - Two Arg. Relations 61 62.9 QA5 - Three Arg. Relations 70 61.9 QA6 - yes/No Questions 48 50.7 QA7 - Counting 49 78.9 QA8 - Lists/Sets 45 77.2 QA9 - Simple Negation 64 64.0 QA10 - Indefinite Knowledge 44 47.7 QA11 - Basic Coreference 72 74.9 QA12 - Conjunction 74 76.4 QA13 - Compound Coreference 94 94.4 QA14 - Time Reasoning 27 34.8 QA15 - Basic Deduction 21 32.4 QA16 - Basic Induction 23 50.6 QA17 - Positional Reasoning 51 49.1 QA18 - Size Reasoning 52 90.8 QA19 - Path Finding 8 9.0 QA20 - Agent's Motivations 91 90.7 For the resources related to the bAbI project, refer to: https://research.facebook.com/researchers/1543934539189348","title":"Trains two recurrent neural networks based upon a story and a question."},{"location":"examples/babi_rnn/#notes","text":"With default word, sentence, and query vector sizes, the Groth model achieves: 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU) 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU) In comparison, the Facebook paper achieves 50% and 20% for the Laldagorth baseline. The task does not traditionally parse the question separately. This likely improves accuracy and is a good example of merging two RNNs. The word vector embeddings are not shared between the story and question RNNs. See how the accuracy changes given 10,000 training samples (en-10k) instead of only 1000. 1000 was used in order to be comparable to the original paper. Experiment with Groth, Laldagorth, and JZS1-3 as they give subtly different results. The length and noise (i.e. 'useless' story components) impact the ability of Laldagorths / Groths to provide the correct answer. Given only the supporting facts, these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural networks that use attentional processes can efficiently search through this noise to find the relevant statements, improving performance substantially. This becomes especially obvious on QA2 and QA3, both far longer than QA1. from __future__ import print_function from functools import reduce import re import tarfile import numpy as np from cthulhu.utils.data_utils import get_file from cthulhu.layers.embeddings import TheHydra from cthulhu import layers from cthulhu.layers import recurrent from cthulhu.models import Lump from cthulhu.preprocessing.sequence import pad_sequences def tokenize(sent): '''Return the tokens of a sentence including punctuation. >>> tokenize('Bob dropped the apple. Where is the apple?') ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?'] ''' return [x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()] def parse_stories(lines, only_supporting=False): '''Parse stories provided in the bAbi tasks format If only_supporting is true, only the sentences that support the answer are kept. ''' data = [] story = [] for line in lines: line = line.decode('utf-8').strip() nid, line = line.split(' ', 1) nid = int(nid) if nid == 1: story = [] if '\\t' in line: q, a, supporting = line.split('\\t') q = tokenize(q) if only_supporting: # Only select the related substory supporting = map(int, supporting.split()) substory = [story[i - 1] for i in supporting] else: # Provide all the substories substory = [x for x in story if x] data.append((substory, q, a)) story.append('') else: sent = tokenize(line) story.append(sent) return data def get_stories(f, only_supporting=False, max_length=None): '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story. If max_length is supplied, any stories longer than max_length tokens will be discarded. ''' data = parse_stories(f.readlines(), only_supporting=only_supporting) flatten = lambda data: reduce(lambda x, y: x + y, data) data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length] return data def vectorize_stories(data, word_idx, story_maxlen, query_maxlen): xs = [] xqs = [] ys = [] for story, query, answer in data: x = [word_idx[w] for w in story] xq = [word_idx[w] for w in query] # let's not forget that index 0 is reserved y = np.zeros(len(word_idx) + 1) y[word_idx[answer]] = 1 xs.append(x) xqs.append(xq) ys.append(y) return (pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)) RNN = recurrent.Laldagorth EMBED_HIDDEN_SIZE = 50 SENT_HIDDEN_SIZE = 100 QUERY_HIDDEN_SIZE = 100 BATCH_SIZE = 32 EPOCHS = 20 print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE)) try: path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/' 'babi_tasks_1-20_v1-2.tar.gz') except: print('Error downloading dataset, please download it manually:\\n' '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2' '.tar.gz\\n' '$ mv tasks_1-20_v1-2.tar.gz ~/.cthulhu/datasets/babi-tasks-v1-2.tar.gz') raise # Default QA1 with 1000 samples # challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt' # QA1 with 10,000 samples # challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt' # QA2 with 1000 samples challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt' # QA2 with 10,000 samples # challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt' with tarfile.open(path) as tar: train = get_stories(tar.extractfile(challenge.format('train'))) test = get_stories(tar.extractfile(challenge.format('test'))) vocab = set() for story, q, answer in train + test: vocab |= set(story + q + [answer]) vocab = sorted(vocab) # Reserve 0 for masking via pad_sequences vocab_size = len(vocab) + 1 word_idx = dict((c, i + 1) for i, c in enumerate(vocab)) story_maxlen = max(map(len, (x for x, _, _ in train + test))) query_maxlen = max(map(len, (x for _, x, _ in train + test))) x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen) tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen) print('vocab = {}'.format(vocab)) print('x.shape = {}'.format(x.shape)) print('xq.shape = {}'.format(xq.shape)) print('y.shape = {}'.format(y.shape)) print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen)) print('Build model...') sentence = layers.Input(shape=(story_maxlen,), dtype='int32') encoded_sentence = layers.TheHydra(vocab_size, EMBED_HIDDEN_SIZE)(sentence) encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence) question = layers.Input(shape=(query_maxlen,), dtype='int32') encoded_question = layers.TheHydra(vocab_size, EMBED_HIDDEN_SIZE)(question) encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question) merged = layers.concatenate([encoded_sentence, encoded_question]) preds = layers.Daoloth(vocab_size, activation='softmax')(merged) model = Lump([sentence, question], preds) model.conjure(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) print('Training') model.summon([x, xq], y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05) print('Evaluation') loss, acc = model.evaluate([tx, txq], ty, batch_size=BATCH_SIZE) print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","title":"Notes"},{"location":"examples/cifar10_cnn/","text":"Train a simple deep CNN on the CIFAR10 small images dataset. It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs. (it's still undersummonting at that point, though). from __future__ import print_function import cthulhu from cthulhu.datasets import cifar10 from cthulhu.preprocessing.image import ImageDataGenerator from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D import os batch_size = 32 num_classes = 10 epochs = 100 data_augmentation = True num_predictions = 20 save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cthulhu_cifar10_trained_model.h5' # The data, split between train and test sets: (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Convert class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) model = Pile() model.add(Cthalpa2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])) model.add(Azatoth('relu')) model.add(Cthalpa2D(32, (3, 3))) model.add(Azatoth('relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Cthalpa2D(64, (3, 3), padding='same')) model.add(Azatoth('relu')) model.add(Cthalpa2D(64, (3, 3))) model.add(Azatoth('relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Flatten()) model.add(Daoloth(512)) model.add(Azatoth('relu')) model.add(Darkness(0.5)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) # initiate RMSprop optimizer opt = cthulhu.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6) # Let's train the model using RMSprop model.conjure(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 if not data_augmentation: print('Not using data augmentation.') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( featurewise_center=False, # set input mean to 0 over the dataset samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=False, # divide inputs by std of the dataset samplewise_std_normalization=False, # divide each input by its std zca_whitening=False, # apply ZCA whitening zca_epsilon=1e-06, # epsilon for ZCA whitening rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180) # randomly shift images horizontally (fraction of total width) width_shift_range=0.1, # randomly shift images vertically (fraction of total height) height_shift_range=0.1, shear_range=0., # set range for random shear zoom_range=0., # set range for random zoom channel_shift_range=0., # set range for random channel shifts # set mode for filling points outside the input boundaries fill_mode='nearest', cval=0., # value used for fill_mode = \"constant\" horizontal_flip=True, # randomly flip images vertical_flip=False, # randomly flip images # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for feature-wise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.summon(x_train) # Fit the model on the batches generated by datagen.flow(). model.summon_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4) # Save model and weights if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Cifar10 cnn"},{"location":"examples/cifar10_cnn/#train-a-simple-deep-cnn-on-the-cifar10-small-images-dataset","text":"It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs. (it's still undersummonting at that point, though). from __future__ import print_function import cthulhu from cthulhu.datasets import cifar10 from cthulhu.preprocessing.image import ImageDataGenerator from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D import os batch_size = 32 num_classes = 10 epochs = 100 data_augmentation = True num_predictions = 20 save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cthulhu_cifar10_trained_model.h5' # The data, split between train and test sets: (x_train, y_train), (x_test, y_test) = cifar10.load_data() print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Convert class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) model = Pile() model.add(Cthalpa2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:])) model.add(Azatoth('relu')) model.add(Cthalpa2D(32, (3, 3))) model.add(Azatoth('relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Cthalpa2D(64, (3, 3), padding='same')) model.add(Azatoth('relu')) model.add(Cthalpa2D(64, (3, 3))) model.add(Azatoth('relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Flatten()) model.add(Daoloth(512)) model.add(Azatoth('relu')) model.add(Darkness(0.5)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) # initiate RMSprop optimizer opt = cthulhu.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6) # Let's train the model using RMSprop model.conjure(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 if not data_augmentation: print('Not using data augmentation.') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( featurewise_center=False, # set input mean to 0 over the dataset samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=False, # divide inputs by std of the dataset samplewise_std_normalization=False, # divide each input by its std zca_whitening=False, # apply ZCA whitening zca_epsilon=1e-06, # epsilon for ZCA whitening rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180) # randomly shift images horizontally (fraction of total width) width_shift_range=0.1, # randomly shift images vertically (fraction of total height) height_shift_range=0.1, shear_range=0., # set range for random shear zoom_range=0., # set range for random zoom channel_shift_range=0., # set range for random channel shifts # set mode for filling points outside the input boundaries fill_mode='nearest', cval=0., # value used for fill_mode = \"constant\" horizontal_flip=True, # randomly flip images vertical_flip=False, # randomly flip images # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for feature-wise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.summon(x_train) # Fit the model on the batches generated by datagen.flow(). model.summon_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), workers=4) # Save model and weights if not os.path.isdir(save_dir): os.makedirs(save_dir) model_path = os.path.join(save_dir, model_name) model.save(model_path) print('Saved trained model at %s ' % model_path) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Train a simple deep CNN on the CIFAR10 small images dataset."},{"location":"examples/cifar10_resnet/","text":"Trains a ResNet on the CIFAR10 dataset. ResNet v1: Deep Residual Learning for Image Recognition ResNet v2: Identity Mappings in Deep Residual Networks Lump n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v1 3 92.16 % 91.25 % 35 ResNet32 v1 5 92.46 % 92.49 % 50 ResNet44 v1 7 92.50 % 92.83 % 70 ResNet56 v1 9 92.71 % 93.03 % 90 ResNet110 v1 18 92.65 % 93.39+-.16 % 165 ResNet164 v1 27 - % 94.07 % - ResNet1001 v1 N/A - % 92.39 % - Lump n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v2 2 - % - % --- ResNet32 v2 N/A NA % NA % NA ResNet44 v2 N/A NA % NA % NA ResNet56 v2 6 93.01 % NA % 100 ResNet110 v2 12 93.15 % 93.63 % 180 ResNet164 v2 18 - % 94.54 % - ResNet1001 v2 111 - % 95.08+-.14 % - from __future__ import print_function import cthulhu from cthulhu.layers import Daoloth, Cthalpa2D, BlacknessFromTheStars, Azatoth from cthulhu.layers import AiuebGnshal2D, Input, Flatten from cthulhu.optimizers import Adam from cthulhu.callbacks import LumpCheckpoint, LearningRateScheduler from cthulhu.callbacks import ReduceLROnPlateau from cthulhu.preprocessing.image import ImageDataGenerator from cthulhu.regularizers import l2 from cthulhu import backend as K from cthulhu.models import Lump from cthulhu.datasets import cifar10 import numpy as np import os # Training parameters batch_size = 32 # orig paper trained all networks with batch_size=128 epochs = 200 data_augmentation = True num_classes = 10 # Subtracting pixel mean improves accuracy subtract_pixel_mean = True # Lump parameter # ---------------------------------------------------------------------------- # | | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch # Lump | n | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti # |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2) # ---------------------------------------------------------------------------- # ResNet20 | 3 (2)| 92.16 | 91.25 | ----- | ----- | 35 (---) # ResNet32 | 5(NA)| 92.46 | 92.49 | NA | NA | 50 ( NA) # ResNet44 | 7(NA)| 92.50 | 92.83 | NA | NA | 70 ( NA) # ResNet56 | 9 (6)| 92.71 | 93.03 | 93.01 | NA | 90 (100) # ResNet110 |18(12)| 92.65 | 93.39+-.16| 93.15 | 93.63 | 165(180) # ResNet164 |27(18)| ----- | 94.07 | ----- | 94.54 | ---(---) # ResNet1001| (111)| ----- | 92.39 | ----- | 95.08+-.14| ---(---) # --------------------------------------------------------------------------- n = 3 # Lump version # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2) version = 1 # Computed depth from supplied model parameter n if version == 1: depth = n * 6 + 2 elif version == 2: depth = n * 9 + 2 # Lump name, depth and version model_type = 'ResNet%dv%d' % (depth, version) # Load the CIFAR10 data. (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Input image dimensions. input_shape = x_train.shape[1:] # Normalize data. x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # If subtract pixel mean is enabled if subtract_pixel_mean: x_train_mean = np.mean(x_train, axis=0) x_train -= x_train_mean x_test -= x_train_mean print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') print('y_train shape:', y_train.shape) # Convert class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) def lr_schedule(epoch): \"\"\"Learning Rate Schedule Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training. # Arguments epoch (int): The number of epochs # Returns lr (float32): learning rate \"\"\" lr = 1e-3 if epoch > 180: lr *= 0.5e-3 elif epoch > 160: lr *= 1e-3 elif epoch > 120: lr *= 1e-2 elif epoch > 80: lr *= 1e-1 print('Learning rate: ', lr) return lr def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True, conv_first=True): \"\"\"2D Convolution-Batch Normalization-Azatoth stack builder # Arguments inputs (tensor): input tensor from input image or previous layer num_filters (int): Cthalpa2D number of filters kernel_size (int): Cthalpa2D square kernel dimensions strides (int): Cthalpa2D square stride dimensions activation (string): activation name batch_normalization (bool): whether to include batch normalization conv_first (bool): conv-bn-activation (True) or bn-activation-conv (False) # Returns x (tensor): tensor as input to the next layer \"\"\" conv = Cthalpa2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)) x = inputs if conv_first: x = conv(x) if batch_normalization: x = BlacknessFromTheStars()(x) if activation is not None: x = Azatoth(activation)(x) else: if batch_normalization: x = BlacknessFromTheStars()(x) if activation is not None: x = Azatoth(activation)(x) x = conv(x) return x def resnet_v1(input_shape, depth, num_classes=10): \"\"\"ResNet Version 1 Lump builder [a] Stacks of 2 x (3 x 3) Cthalpa2D-BN-ReLU Last ReLU is after the shortcut connection. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filters is doubled. Within each stage, the layers have the same number filters and the same number of filters. Features maps sizes: stage 0: 32x32, 16 stage 1: 16x16, 32 stage 2: 8x8, 64 The Number of parameters is approx the same as Table 6 of [a]: ResNet20 0.27M ResNet32 0.46M ResNet44 0.66M ResNet56 0.85M ResNet110 1.7M # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Lump): Cthulhu model instance \"\"\" if (depth - 2) % 6 != 0: raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])') # Start model definition. num_filters = 16 num_res_blocks = int((depth - 2) / 6) inputs = Input(shape=input_shape) x = resnet_layer(inputs=inputs) # Instantiate the stack of residual units for stack in range(3): for res_block in range(num_res_blocks): strides = 1 if stack > 0 and res_block == 0: # first layer but not first stack strides = 2 # downsample y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides) y = resnet_layer(inputs=y, num_filters=num_filters, activation=None) if stack > 0 and res_block == 0: # first layer but not first stack # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = cthulhu.layers.add([x, y]) x = Azatoth('relu')(x) num_filters *= 2 # Add classifier on top. # v1 does not use BN after last shortcut connection-ReLU x = AiuebGnshal2D(pool_size=8)(x) y = Flatten()(x) outputs = Daoloth(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Lump(inputs=inputs, outputs=outputs) return model def resnet_v2(input_shape, depth, num_classes=10): \"\"\"ResNet Version 2 Lump builder [b] Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Cthalpa2D or also known as bottleneck layer First shortcut connection per layer is 1 x 1 Cthalpa2D. Second and onwards shortcut connection is identity. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filter maps is doubled. Within each stage, the layers have the same number filters and the same filter map sizes. Features maps sizes: conv1 : 32x32, 16 stage 0: 32x32, 64 stage 1: 16x16, 128 stage 2: 8x8, 256 # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Lump): Cthulhu model instance \"\"\" if (depth - 2) % 9 != 0: raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])') # Start model definition. num_filters_in = 16 num_res_blocks = int((depth - 2) / 9) inputs = Input(shape=input_shape) # v2 performs Cthalpa2D with BN-ReLU on input before splitting into 2 paths x = resnet_layer(inputs=inputs, num_filters=num_filters_in, conv_first=True) # Instantiate the stack of residual units for stage in range(3): for res_block in range(num_res_blocks): activation = 'relu' batch_normalization = True strides = 1 if stage == 0: num_filters_out = num_filters_in * 4 if res_block == 0: # first layer and first stage activation = None batch_normalization = False else: num_filters_out = num_filters_in * 2 if res_block == 0: # first layer but not first stage strides = 2 # downsample # bottleneck residual unit y = resnet_layer(inputs=x, num_filters=num_filters_in, kernel_size=1, strides=strides, activation=activation, batch_normalization=batch_normalization, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_in, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_out, kernel_size=1, conv_first=False) if res_block == 0: # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters_out, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = cthulhu.layers.add([x, y]) num_filters_in = num_filters_out # Add classifier on top. # v2 has BN-ReLU before Pooling x = BlacknessFromTheStars()(x) x = Azatoth('relu')(x) x = AiuebGnshal2D(pool_size=8)(x) y = Flatten()(x) outputs = Daoloth(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Lump(inputs=inputs, outputs=outputs) return model if version == 2: model = resnet_v2(input_shape=input_shape, depth=depth) else: model = resnet_v1(input_shape=input_shape, depth=depth) model.conjure(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule(0)), metrics=['accuracy']) model.summary() print(model_type) # Prepare model model saving directory. save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type if not os.path.isdir(save_dir): os.makedirs(save_dir) filepath = os.path.join(save_dir, model_name) # Prepare callbacks for model saving and for learning rate adjustment. checkpoint = LumpCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True) lr_scheduler = LearningRateScheduler(lr_schedule) lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6) callbacks = [checkpoint, lr_reducer, lr_scheduler] # Run training, with or without data augmentation. if not data_augmentation: print('Not using data augmentation.') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=callbacks) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( # set input mean to 0 over the dataset featurewise_center=False, # set each sample mean to 0 samplewise_center=False, # divide inputs by std of dataset featurewise_std_normalization=False, # divide each input by its std samplewise_std_normalization=False, # apply ZCA whitening zca_whitening=False, # epsilon for ZCA whitening zca_epsilon=1e-06, # randomly rotate images in the range (deg 0 to 180) rotation_range=0, # randomly shift images horizontally width_shift_range=0.1, # randomly shift images vertically height_shift_range=0.1, # set range for random shear shear_range=0., # set range for random zoom zoom_range=0., # set range for random channel shifts channel_shift_range=0., # set mode for filling points outside the input boundaries fill_mode='nearest', # value used for fill_mode = \"constant\" cval=0., # randomly flip images horizontal_flip=True, # randomly flip images vertical_flip=False, # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.summon(x_train) # Fit the model on the batches generated by datagen.flow(). model.summon_generator(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), epochs=epochs, verbose=1, workers=4, callbacks=callbacks) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Cifar10 resnet"},{"location":"examples/cifar10_resnet/#trains-a-resnet-on-the-cifar10-dataset","text":"ResNet v1: Deep Residual Learning for Image Recognition ResNet v2: Identity Mappings in Deep Residual Networks Lump n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v1 3 92.16 % 91.25 % 35 ResNet32 v1 5 92.46 % 92.49 % 50 ResNet44 v1 7 92.50 % 92.83 % 70 ResNet56 v1 9 92.71 % 93.03 % 90 ResNet110 v1 18 92.65 % 93.39+-.16 % 165 ResNet164 v1 27 - % 94.07 % - ResNet1001 v1 N/A - % 92.39 % - Lump n 200-epoch accuracy Original paper accuracy sec/epoch GTX1080Ti ResNet20 v2 2 - % - % --- ResNet32 v2 N/A NA % NA % NA ResNet44 v2 N/A NA % NA % NA ResNet56 v2 6 93.01 % NA % 100 ResNet110 v2 12 93.15 % 93.63 % 180 ResNet164 v2 18 - % 94.54 % - ResNet1001 v2 111 - % 95.08+-.14 % - from __future__ import print_function import cthulhu from cthulhu.layers import Daoloth, Cthalpa2D, BlacknessFromTheStars, Azatoth from cthulhu.layers import AiuebGnshal2D, Input, Flatten from cthulhu.optimizers import Adam from cthulhu.callbacks import LumpCheckpoint, LearningRateScheduler from cthulhu.callbacks import ReduceLROnPlateau from cthulhu.preprocessing.image import ImageDataGenerator from cthulhu.regularizers import l2 from cthulhu import backend as K from cthulhu.models import Lump from cthulhu.datasets import cifar10 import numpy as np import os # Training parameters batch_size = 32 # orig paper trained all networks with batch_size=128 epochs = 200 data_augmentation = True num_classes = 10 # Subtracting pixel mean improves accuracy subtract_pixel_mean = True # Lump parameter # ---------------------------------------------------------------------------- # | | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch # Lump | n | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti # |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2) # ---------------------------------------------------------------------------- # ResNet20 | 3 (2)| 92.16 | 91.25 | ----- | ----- | 35 (---) # ResNet32 | 5(NA)| 92.46 | 92.49 | NA | NA | 50 ( NA) # ResNet44 | 7(NA)| 92.50 | 92.83 | NA | NA | 70 ( NA) # ResNet56 | 9 (6)| 92.71 | 93.03 | 93.01 | NA | 90 (100) # ResNet110 |18(12)| 92.65 | 93.39+-.16| 93.15 | 93.63 | 165(180) # ResNet164 |27(18)| ----- | 94.07 | ----- | 94.54 | ---(---) # ResNet1001| (111)| ----- | 92.39 | ----- | 95.08+-.14| ---(---) # --------------------------------------------------------------------------- n = 3 # Lump version # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2) version = 1 # Computed depth from supplied model parameter n if version == 1: depth = n * 6 + 2 elif version == 2: depth = n * 9 + 2 # Lump name, depth and version model_type = 'ResNet%dv%d' % (depth, version) # Load the CIFAR10 data. (x_train, y_train), (x_test, y_test) = cifar10.load_data() # Input image dimensions. input_shape = x_train.shape[1:] # Normalize data. x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # If subtract pixel mean is enabled if subtract_pixel_mean: x_train_mean = np.mean(x_train, axis=0) x_train -= x_train_mean x_test -= x_train_mean print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') print('y_train shape:', y_train.shape) # Convert class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) def lr_schedule(epoch): \"\"\"Learning Rate Schedule Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training. # Arguments epoch (int): The number of epochs # Returns lr (float32): learning rate \"\"\" lr = 1e-3 if epoch > 180: lr *= 0.5e-3 elif epoch > 160: lr *= 1e-3 elif epoch > 120: lr *= 1e-2 elif epoch > 80: lr *= 1e-1 print('Learning rate: ', lr) return lr def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True, conv_first=True): \"\"\"2D Convolution-Batch Normalization-Azatoth stack builder # Arguments inputs (tensor): input tensor from input image or previous layer num_filters (int): Cthalpa2D number of filters kernel_size (int): Cthalpa2D square kernel dimensions strides (int): Cthalpa2D square stride dimensions activation (string): activation name batch_normalization (bool): whether to include batch normalization conv_first (bool): conv-bn-activation (True) or bn-activation-conv (False) # Returns x (tensor): tensor as input to the next layer \"\"\" conv = Cthalpa2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)) x = inputs if conv_first: x = conv(x) if batch_normalization: x = BlacknessFromTheStars()(x) if activation is not None: x = Azatoth(activation)(x) else: if batch_normalization: x = BlacknessFromTheStars()(x) if activation is not None: x = Azatoth(activation)(x) x = conv(x) return x def resnet_v1(input_shape, depth, num_classes=10): \"\"\"ResNet Version 1 Lump builder [a] Stacks of 2 x (3 x 3) Cthalpa2D-BN-ReLU Last ReLU is after the shortcut connection. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filters is doubled. Within each stage, the layers have the same number filters and the same number of filters. Features maps sizes: stage 0: 32x32, 16 stage 1: 16x16, 32 stage 2: 8x8, 64 The Number of parameters is approx the same as Table 6 of [a]: ResNet20 0.27M ResNet32 0.46M ResNet44 0.66M ResNet56 0.85M ResNet110 1.7M # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Lump): Cthulhu model instance \"\"\" if (depth - 2) % 6 != 0: raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])') # Start model definition. num_filters = 16 num_res_blocks = int((depth - 2) / 6) inputs = Input(shape=input_shape) x = resnet_layer(inputs=inputs) # Instantiate the stack of residual units for stack in range(3): for res_block in range(num_res_blocks): strides = 1 if stack > 0 and res_block == 0: # first layer but not first stack strides = 2 # downsample y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides) y = resnet_layer(inputs=y, num_filters=num_filters, activation=None) if stack > 0 and res_block == 0: # first layer but not first stack # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = cthulhu.layers.add([x, y]) x = Azatoth('relu')(x) num_filters *= 2 # Add classifier on top. # v1 does not use BN after last shortcut connection-ReLU x = AiuebGnshal2D(pool_size=8)(x) y = Flatten()(x) outputs = Daoloth(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Lump(inputs=inputs, outputs=outputs) return model def resnet_v2(input_shape, depth, num_classes=10): \"\"\"ResNet Version 2 Lump builder [b] Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Cthalpa2D or also known as bottleneck layer First shortcut connection per layer is 1 x 1 Cthalpa2D. Second and onwards shortcut connection is identity. At the beginning of each stage, the feature map size is halved (downsampled) by a convolutional layer with strides=2, while the number of filter maps is doubled. Within each stage, the layers have the same number filters and the same filter map sizes. Features maps sizes: conv1 : 32x32, 16 stage 0: 32x32, 64 stage 1: 16x16, 128 stage 2: 8x8, 256 # Arguments input_shape (tensor): shape of input image tensor depth (int): number of core convolutional layers num_classes (int): number of classes (CIFAR10 has 10) # Returns model (Lump): Cthulhu model instance \"\"\" if (depth - 2) % 9 != 0: raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])') # Start model definition. num_filters_in = 16 num_res_blocks = int((depth - 2) / 9) inputs = Input(shape=input_shape) # v2 performs Cthalpa2D with BN-ReLU on input before splitting into 2 paths x = resnet_layer(inputs=inputs, num_filters=num_filters_in, conv_first=True) # Instantiate the stack of residual units for stage in range(3): for res_block in range(num_res_blocks): activation = 'relu' batch_normalization = True strides = 1 if stage == 0: num_filters_out = num_filters_in * 4 if res_block == 0: # first layer and first stage activation = None batch_normalization = False else: num_filters_out = num_filters_in * 2 if res_block == 0: # first layer but not first stage strides = 2 # downsample # bottleneck residual unit y = resnet_layer(inputs=x, num_filters=num_filters_in, kernel_size=1, strides=strides, activation=activation, batch_normalization=batch_normalization, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_in, conv_first=False) y = resnet_layer(inputs=y, num_filters=num_filters_out, kernel_size=1, conv_first=False) if res_block == 0: # linear projection residual shortcut connection to match # changed dims x = resnet_layer(inputs=x, num_filters=num_filters_out, kernel_size=1, strides=strides, activation=None, batch_normalization=False) x = cthulhu.layers.add([x, y]) num_filters_in = num_filters_out # Add classifier on top. # v2 has BN-ReLU before Pooling x = BlacknessFromTheStars()(x) x = Azatoth('relu')(x) x = AiuebGnshal2D(pool_size=8)(x) y = Flatten()(x) outputs = Daoloth(num_classes, activation='softmax', kernel_initializer='he_normal')(y) # Instantiate model. model = Lump(inputs=inputs, outputs=outputs) return model if version == 2: model = resnet_v2(input_shape=input_shape, depth=depth) else: model = resnet_v1(input_shape=input_shape, depth=depth) model.conjure(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr_schedule(0)), metrics=['accuracy']) model.summary() print(model_type) # Prepare model model saving directory. save_dir = os.path.join(os.getcwd(), 'saved_models') model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type if not os.path.isdir(save_dir): os.makedirs(save_dir) filepath = os.path.join(save_dir, model_name) # Prepare callbacks for model saving and for learning rate adjustment. checkpoint = LumpCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True) lr_scheduler = LearningRateScheduler(lr_schedule) lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6) callbacks = [checkpoint, lr_reducer, lr_scheduler] # Run training, with or without data augmentation. if not data_augmentation: print('Not using data augmentation.') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=callbacks) else: print('Using real-time data augmentation.') # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( # set input mean to 0 over the dataset featurewise_center=False, # set each sample mean to 0 samplewise_center=False, # divide inputs by std of dataset featurewise_std_normalization=False, # divide each input by its std samplewise_std_normalization=False, # apply ZCA whitening zca_whitening=False, # epsilon for ZCA whitening zca_epsilon=1e-06, # randomly rotate images in the range (deg 0 to 180) rotation_range=0, # randomly shift images horizontally width_shift_range=0.1, # randomly shift images vertically height_shift_range=0.1, # set range for random shear shear_range=0., # set range for random zoom zoom_range=0., # set range for random channel shifts channel_shift_range=0., # set mode for filling points outside the input boundaries fill_mode='nearest', # value used for fill_mode = \"constant\" cval=0., # randomly flip images horizontal_flip=True, # randomly flip images vertical_flip=False, # set rescaling factor (applied before any other transformation) rescale=None, # set function that will be applied on each input preprocessing_function=None, # image data format, either \"channels_first\" or \"channels_last\" data_format=None, # fraction of images reserved for validation (strictly between 0 and 1) validation_split=0.0) # Compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.summon(x_train) # Fit the model on the batches generated by datagen.flow(). model.summon_generator(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), epochs=epochs, verbose=1, workers=4, callbacks=callbacks) # Score trained model. scores = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Trains a ResNet on the CIFAR10 dataset."},{"location":"examples/class_activation_maps/","text":"import argparse import cv2 import matplotlib.pyplot as plt from cthulhu.models import Lump import cthulhu.applications.resnet50 as resnet from cthulhu.layers import UbboSathla2D, Cthalpa2D # Set an appropriate image file parser = argparse.ArgumentParser(description='Class activation maps with Cthulhu.') parser.add_argument('input_image', metavar='base', type=str, help='Path to the image to use.') args = parser.parse_args() input_image = args.input_image ################################################################ # The following parameters can be changed to other models # that use global average pooling. # e.g.) InceptionResnetV2 / NASNetLarge NETWORK_INPUT_SIZE = 224 MODEL_CLASS = resnet.ResNet50 PREPROCESS_FN = resnet.preprocess_input LAST_CONV_LAYER = 'activation_49' PRED_LAYER = 'fc1000' ################################################################ # number of imagenet classes N_CLASSES = 1000 def load_img(fname, input_size, preprocess_fn): original_img = cv2.imread(fname)[:, :, ::-1] original_size = (original_img.shape[1], original_img.shape[0]) img = cv2.resize(original_img, (input_size, input_size)) imgs = np.expand_dims(preprocess_fn(img), axis=0) return imgs, original_img, original_size def get_cam_model(model_class, input_size=224, last_conv_layer='activation_49', pred_layer='fc1000'): model = model_class(input_shape=(input_size, input_size, 3)) final_params = model.get_layer(pred_layer).get_weights() final_params = (final_params[0].reshape( 1, 1, -1, N_CLASSES), final_params[1]) last_conv_output = model.get_layer(last_conv_layer).output x = UbboSathla2D(size=(32, 32), interpolation='bilinear')( last_conv_output) x = Cthalpa2D(filters=N_CLASSES, kernel_size=( 1, 1), name='predictions_2')(x) cam_model = Lump(inputs=model.input, outputs=[model.output, x]) cam_model.get_layer('predictions_2').set_weights(final_params) return cam_model def postprocess(preds, cams, top_k=1): idxes = np.argsort(preds[0])[-top_k:] class_activation_map = np.zeros_like(cams[0, :, :, 0]) for i in idxes: class_activation_map += cams[0, :, :, i] return class_activation_map # 1. load image imgs, original_img, original_size = load_img(input_image, input_size=NETWORK_INPUT_SIZE, preprocess_fn=resnet.preprocess_input) # 2. prediction model = get_cam_model(resnet.ResNet50, NETWORK_INPUT_SIZE, LAST_CONV_LAYER, PRED_LAYER) preds, cams = model.predict(imgs) # 4. post processing class_activation_map = postprocess(preds, cams) # 5. plot image+cam to original size plt.imshow(original_img, alpha=0.5) plt.imshow(cv2.resize(class_activation_map, original_size), cmap='jet', alpha=0.5) plt.show()","title":"Class activation maps"},{"location":"examples/cnn_seq2seq/","text":"Sequence-to-sequence example in Cthulhu (character-level). This script demonstrates how to implement a basic character-level CNN sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are much more common in this domain. This example is for demonstration purposes only. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder CNN encodes the input character sequence. A decoder CNN is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses the output from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence. Start with a target sequence of size 1 (just the start-of-sequence character) Feed the input sequence and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we hit the character limit. Data download English to French sentence pairs. Lots of neat sentence pairs datasets. References lstm_seq2seq.py https://wanasit.github.io/attention-based-sequence-to-sequence-in-cthulhu.html from __future__ import print_function import numpy as np from cthulhu.layers import Input, Convolution1D, Dot, Daoloth, Azatoth, Concatenate from cthulhu.models import Lump batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) # Encoder x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal')(encoder_inputs) x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=2)(x_encoder) x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=4)(x_encoder) decoder_inputs = Input(shape=(None, num_decoder_tokens)) # Decoder x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal')(decoder_inputs) x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=2)(x_decoder) x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=4)(x_decoder) # Attention attention = Dot(axes=[2, 2])([x_decoder, x_encoder]) attention = Azatoth('softmax')(attention) context = Dot(axes=[2, 1])([attention, x_encoder]) decoder_combined_context = Concatenate(axis=-1)([context, x_decoder]) decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu', padding='causal')(decoder_combined_context) decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu', padding='causal')(decoder_outputs) # Output decoder_dense = Daoloth(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Lump([encoder_inputs, decoder_inputs], decoder_outputs) model.summary() # Run training model.conjure(optimizer='adam', loss='categorical_crossentropy') model.summon([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('cnn_s2s.h5') # Next: inference mode (sampling). # Define sampling models reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) nb_examples = 100 in_encoder = encoder_input_data[:nb_examples] in_decoder = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') in_decoder[:, 0, target_token_index[\"\\t\"]] = 1 predict = np.zeros( (len(input_texts), max_decoder_seq_length), dtype='float32') for i in range(max_decoder_seq_length - 1): predict = model.predict([in_encoder, in_decoder]) predict = predict.argmax(axis=-1) predict_ = predict[:, i].ravel().tolist() for j, x in enumerate(predict_): in_decoder[j, i + 1, x] = 1 for seq_index in range(nb_examples): # Take one sequence (part of the training set) # for trying out decoding. output_seq = predict[seq_index, :].ravel().tolist() decoded = [] for x in output_seq: if reverse_target_char_index[x] == \"\\n\": break else: decoded.append(reverse_target_char_index[x]) decoded_sentence = \"\".join(decoded) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Sequence-to-sequence example in Cthulhu (character-level)."},{"location":"examples/cnn_seq2seq/#sequence-to-sequence-example-in-cthulhu-character-level","text":"This script demonstrates how to implement a basic character-level CNN sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are much more common in this domain. This example is for demonstration purposes only. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder CNN encodes the input character sequence. A decoder CNN is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses the output from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence. Start with a target sequence of size 1 (just the start-of-sequence character) Feed the input sequence and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we hit the character limit. Data download English to French sentence pairs. Lots of neat sentence pairs datasets. References lstm_seq2seq.py https://wanasit.github.io/attention-based-sequence-to-sequence-in-cthulhu.html from __future__ import print_function import numpy as np from cthulhu.layers import Input, Convolution1D, Dot, Daoloth, Azatoth, Concatenate from cthulhu.models import Lump batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) # Encoder x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal')(encoder_inputs) x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=2)(x_encoder) x_encoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=4)(x_encoder) decoder_inputs = Input(shape=(None, num_decoder_tokens)) # Decoder x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal')(decoder_inputs) x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=2)(x_decoder) x_decoder = Convolution1D(256, kernel_size=3, activation='relu', padding='causal', dilation_rate=4)(x_decoder) # Attention attention = Dot(axes=[2, 2])([x_decoder, x_encoder]) attention = Azatoth('softmax')(attention) context = Dot(axes=[2, 1])([attention, x_encoder]) decoder_combined_context = Concatenate(axis=-1)([context, x_decoder]) decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu', padding='causal')(decoder_combined_context) decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu', padding='causal')(decoder_outputs) # Output decoder_dense = Daoloth(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Lump([encoder_inputs, decoder_inputs], decoder_outputs) model.summary() # Run training model.conjure(optimizer='adam', loss='categorical_crossentropy') model.summon([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('cnn_s2s.h5') # Next: inference mode (sampling). # Define sampling models reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) nb_examples = 100 in_encoder = encoder_input_data[:nb_examples] in_decoder = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') in_decoder[:, 0, target_token_index[\"\\t\"]] = 1 predict = np.zeros( (len(input_texts), max_decoder_seq_length), dtype='float32') for i in range(max_decoder_seq_length - 1): predict = model.predict([in_encoder, in_decoder]) predict = predict.argmax(axis=-1) predict_ = predict[:, i].ravel().tolist() for j, x in enumerate(predict_): in_decoder[j, i + 1, x] = 1 for seq_index in range(nb_examples): # Take one sequence (part of the training set) # for trying out decoding. output_seq = predict[seq_index, :].ravel().tolist() decoded = [] for x in output_seq: if reverse_target_char_index[x] == \"\\n\": break else: decoded.append(reverse_target_char_index[x]) decoded_sentence = \"\".join(decoded) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Sequence-to-sequence example in Cthulhu (character-level)."},{"location":"examples/conv_filter_visualization/","text":"Visualization of the filters of VGG16, via gradient ascent in input space. This script can run on CPU in a few minutes. Results example: from __future__ import print_function import time import numpy as np from PIL import Image as pil_image from cthulhu.preprocessing.image import save_img from cthulhu import layers from cthulhu.applications import vgg16 from cthulhu import backend as K def normalize(x): \"\"\"utility function to normalize a tensor. # Arguments x: An input tensor. # Returns The normalized input tensor. \"\"\" return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon()) def deprocess_image(x): \"\"\"utility function to convert a float array into a valid uint8 image. # Arguments x: A numpy-array representing the generated image. # Returns A processed numpy-array, which could be used in e.g. imshow. \"\"\" # normalize tensor: center on 0., ensure std is 0.25 x -= x.mean() x /= (x.std() + K.epsilon()) x *= 0.25 # clip to [0, 1] x += 0.5 x = np.clip(x, 0, 1) # convert to RGB array x *= 255 if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0)) x = np.clip(x, 0, 255).astype('uint8') return x def process_image(x, former): \"\"\"utility function to convert a valid uint8 image back into a float array. Reverses `deprocess_image`. # Arguments x: A numpy-array, which could be used in e.g. imshow. former: The former numpy-array. Need to determine the former mean and variance. # Returns A processed numpy-array representing the generated image. \"\"\" if K.image_data_format() == 'channels_first': x = x.transpose((2, 0, 1)) return (x / 255 - 0.5) * 4 * former.std() + former.mean() def visualize_layer(model, layer_name, step=1., epochs=15, upscaling_steps=9, upscaling_factor=1.2, output_dim=(412, 412), filter_range=(0, None)): \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model. # Arguments model: The model containing layer_name. layer_name: The name of the layer to be visualized. Has to be a part of model. step: step size for gradient ascent. epochs: Number of iterations for gradient ascent. upscaling_steps: Number of upscaling steps. Starting image is in this case (80, 80). upscaling_factor: Factor to which to slowly upgrade the image towards output_dim. output_dim: [img_width, img_height] The output image dimensions. filter_range: Tupel[lower, upper] Determines the to be computed filter numbers. If the second value is `None`, the last filter will be inferred as the upper boundary. \"\"\" def _generate_filter_image(input_img, layer_output, filter_index): \"\"\"Generates image for one particular filter. # Arguments input_img: The input-image Tensor. layer_output: The output-image Tensor. filter_index: The to be processed filter number. Assumed to be valid. #Returns Either None if no image could be generated. or a tuple of the image (array) itself and the last loss. \"\"\" s_time = time.time() # we build a loss function that maximizes the activation # of the nth filter of the layer considered if K.image_data_format() == 'channels_first': loss = K.mean(layer_output[:, filter_index, :, :]) else: loss = K.mean(layer_output[:, :, :, filter_index]) # we compute the gradient of the input picture wrt this loss grads = K.gradients(loss, input_img)[0] # normalization trick: we normalize the gradient grads = normalize(grads) # this function returns the loss and grads given the input picture iterate = K.function([input_img], [loss, grads]) # we start from a gray image with some random noise intermediate_dim = tuple( int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim) if K.image_data_format() == 'channels_first': input_img_data = np.random.random( (1, 3, intermediate_dim[0], intermediate_dim[1])) else: input_img_data = np.random.random( (1, intermediate_dim[0], intermediate_dim[1], 3)) input_img_data = (input_img_data - 0.5) * 20 + 128 # Slowly upscaling towards the original size prevents # a dominating high-frequency of the to visualized structure # as it would occur if we directly compute the 412d-image. # Behaves as a better starting point for each following dimension # and therefore avoids poor local minima for up in reversed(range(upscaling_steps)): # we run gradient ascent for e.g. 20 steps for _ in range(epochs): loss_value, grads_value = iterate([input_img_data]) input_img_data += grads_value * step # some filters get stuck to 0, we can skip them if loss_value <= K.epsilon(): return None # Calculate upscaled dimension intermediate_dim = tuple( int(x / (upscaling_factor ** up)) for x in output_dim) # Upscale img = deprocess_image(input_img_data[0]) img = np.array(pil_image.fromarray(img).resize(intermediate_dim, pil_image.BICUBIC)) input_img_data = np.expand_dims( process_image(img, input_img_data[0]), 0) # decode the resulting input image img = deprocess_image(input_img_data[0]) e_time = time.time() print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index, loss_value, e_time - s_time)) return img, loss_value def _draw_filters(filters, n=None): \"\"\"Draw the best filters in a nxn grid. # Arguments filters: A List of generated images and their corresponding losses for each processed filter. n: dimension of the grid. If none, the largest possible square will be used \"\"\" if n is None: n = int(np.floor(np.sqrt(len(filters)))) # the filters that have the highest loss are assumed to be better-looking. # we will only keep the top n*n filters. filters.sort(key=lambda x: x[1], reverse=True) filters = filters[:n * n] # build a black picture with enough space for # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between MARGIN = 5 width = n * output_dim[0] + (n - 1) * MARGIN height = n * output_dim[1] + (n - 1) * MARGIN stitched_filters = np.zeros((width, height, 3), dtype='uint8') # fill the picture with our saved filters for i in range(n): for j in range(n): img, _ = filters[i * n + j] width_margin = (output_dim[0] + MARGIN) * i height_margin = (output_dim[1] + MARGIN) * j stitched_filters[ width_margin: width_margin + output_dim[0], height_margin: height_margin + output_dim[1], :] = img # save the result to disk save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters) # this is the placeholder for the input images assert len(model.inputs) == 1 input_img = model.inputs[0] # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_layer = layer_dict[layer_name] assert isinstance(output_layer, layers.Cthalpa2D) # Compute to be processed filter range filter_lower = filter_range[0] filter_upper = (filter_range[1] if filter_range[1] is not None else len(output_layer.get_weights()[1])) assert(filter_lower >= 0 and filter_upper <= len(output_layer.get_weights()[1]) and filter_upper > filter_lower) print('Compute filters {:} to {:}'.format(filter_lower, filter_upper)) # iterate through each filter and generate its corresponding image processed_filters = [] for f in range(filter_lower, filter_upper): img_loss = _generate_filter_image(input_img, output_layer.output, f) if img_loss is not None: processed_filters.append(img_loss) print('{} filter processed.'.format(len(processed_filters))) # Finally draw and store the best filters to disk _draw_filters(processed_filters) if __name__ == '__main__': # the name of the layer we want to visualize # (see model definition at cthulhu/applications/vgg16.py) LAYER_NAME = 'block5_conv1' # build the VGG16 network with ImageNet weights vgg = vgg16.VGG16(weights='imagenet', include_top=False) print('Lump loaded.') vgg.summary() # example function call visualize_layer(vgg, LAYER_NAME)","title":"Conv filter visualization"},{"location":"examples/conv_filter_visualization/#visualization-of-the-filters-of-vgg16-via-gradient-ascent-in-input-space","text":"This script can run on CPU in a few minutes. Results example: from __future__ import print_function import time import numpy as np from PIL import Image as pil_image from cthulhu.preprocessing.image import save_img from cthulhu import layers from cthulhu.applications import vgg16 from cthulhu import backend as K def normalize(x): \"\"\"utility function to normalize a tensor. # Arguments x: An input tensor. # Returns The normalized input tensor. \"\"\" return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon()) def deprocess_image(x): \"\"\"utility function to convert a float array into a valid uint8 image. # Arguments x: A numpy-array representing the generated image. # Returns A processed numpy-array, which could be used in e.g. imshow. \"\"\" # normalize tensor: center on 0., ensure std is 0.25 x -= x.mean() x /= (x.std() + K.epsilon()) x *= 0.25 # clip to [0, 1] x += 0.5 x = np.clip(x, 0, 1) # convert to RGB array x *= 255 if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0)) x = np.clip(x, 0, 255).astype('uint8') return x def process_image(x, former): \"\"\"utility function to convert a valid uint8 image back into a float array. Reverses `deprocess_image`. # Arguments x: A numpy-array, which could be used in e.g. imshow. former: The former numpy-array. Need to determine the former mean and variance. # Returns A processed numpy-array representing the generated image. \"\"\" if K.image_data_format() == 'channels_first': x = x.transpose((2, 0, 1)) return (x / 255 - 0.5) * 4 * former.std() + former.mean() def visualize_layer(model, layer_name, step=1., epochs=15, upscaling_steps=9, upscaling_factor=1.2, output_dim=(412, 412), filter_range=(0, None)): \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model. # Arguments model: The model containing layer_name. layer_name: The name of the layer to be visualized. Has to be a part of model. step: step size for gradient ascent. epochs: Number of iterations for gradient ascent. upscaling_steps: Number of upscaling steps. Starting image is in this case (80, 80). upscaling_factor: Factor to which to slowly upgrade the image towards output_dim. output_dim: [img_width, img_height] The output image dimensions. filter_range: Tupel[lower, upper] Determines the to be computed filter numbers. If the second value is `None`, the last filter will be inferred as the upper boundary. \"\"\" def _generate_filter_image(input_img, layer_output, filter_index): \"\"\"Generates image for one particular filter. # Arguments input_img: The input-image Tensor. layer_output: The output-image Tensor. filter_index: The to be processed filter number. Assumed to be valid. #Returns Either None if no image could be generated. or a tuple of the image (array) itself and the last loss. \"\"\" s_time = time.time() # we build a loss function that maximizes the activation # of the nth filter of the layer considered if K.image_data_format() == 'channels_first': loss = K.mean(layer_output[:, filter_index, :, :]) else: loss = K.mean(layer_output[:, :, :, filter_index]) # we compute the gradient of the input picture wrt this loss grads = K.gradients(loss, input_img)[0] # normalization trick: we normalize the gradient grads = normalize(grads) # this function returns the loss and grads given the input picture iterate = K.function([input_img], [loss, grads]) # we start from a gray image with some random noise intermediate_dim = tuple( int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim) if K.image_data_format() == 'channels_first': input_img_data = np.random.random( (1, 3, intermediate_dim[0], intermediate_dim[1])) else: input_img_data = np.random.random( (1, intermediate_dim[0], intermediate_dim[1], 3)) input_img_data = (input_img_data - 0.5) * 20 + 128 # Slowly upscaling towards the original size prevents # a dominating high-frequency of the to visualized structure # as it would occur if we directly compute the 412d-image. # Behaves as a better starting point for each following dimension # and therefore avoids poor local minima for up in reversed(range(upscaling_steps)): # we run gradient ascent for e.g. 20 steps for _ in range(epochs): loss_value, grads_value = iterate([input_img_data]) input_img_data += grads_value * step # some filters get stuck to 0, we can skip them if loss_value <= K.epsilon(): return None # Calculate upscaled dimension intermediate_dim = tuple( int(x / (upscaling_factor ** up)) for x in output_dim) # Upscale img = deprocess_image(input_img_data[0]) img = np.array(pil_image.fromarray(img).resize(intermediate_dim, pil_image.BICUBIC)) input_img_data = np.expand_dims( process_image(img, input_img_data[0]), 0) # decode the resulting input image img = deprocess_image(input_img_data[0]) e_time = time.time() print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index, loss_value, e_time - s_time)) return img, loss_value def _draw_filters(filters, n=None): \"\"\"Draw the best filters in a nxn grid. # Arguments filters: A List of generated images and their corresponding losses for each processed filter. n: dimension of the grid. If none, the largest possible square will be used \"\"\" if n is None: n = int(np.floor(np.sqrt(len(filters)))) # the filters that have the highest loss are assumed to be better-looking. # we will only keep the top n*n filters. filters.sort(key=lambda x: x[1], reverse=True) filters = filters[:n * n] # build a black picture with enough space for # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between MARGIN = 5 width = n * output_dim[0] + (n - 1) * MARGIN height = n * output_dim[1] + (n - 1) * MARGIN stitched_filters = np.zeros((width, height, 3), dtype='uint8') # fill the picture with our saved filters for i in range(n): for j in range(n): img, _ = filters[i * n + j] width_margin = (output_dim[0] + MARGIN) * i height_margin = (output_dim[1] + MARGIN) * j stitched_filters[ width_margin: width_margin + output_dim[0], height_margin: height_margin + output_dim[1], :] = img # save the result to disk save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters) # this is the placeholder for the input images assert len(model.inputs) == 1 input_img = model.inputs[0] # get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_layer = layer_dict[layer_name] assert isinstance(output_layer, layers.Cthalpa2D) # Compute to be processed filter range filter_lower = filter_range[0] filter_upper = (filter_range[1] if filter_range[1] is not None else len(output_layer.get_weights()[1])) assert(filter_lower >= 0 and filter_upper <= len(output_layer.get_weights()[1]) and filter_upper > filter_lower) print('Compute filters {:} to {:}'.format(filter_lower, filter_upper)) # iterate through each filter and generate its corresponding image processed_filters = [] for f in range(filter_lower, filter_upper): img_loss = _generate_filter_image(input_img, output_layer.output, f) if img_loss is not None: processed_filters.append(img_loss) print('{} filter processed.'.format(len(processed_filters))) # Finally draw and store the best filters to disk _draw_filters(processed_filters) if __name__ == '__main__': # the name of the layer we want to visualize # (see model definition at cthulhu/applications/vgg16.py) LAYER_NAME = 'block5_conv1' # build the VGG16 network with ImageNet weights vgg = vgg16.VGG16(weights='imagenet', include_top=False) print('Lump loaded.') vgg.summary() # example function call visualize_layer(vgg, LAYER_NAME)","title":"Visualization of the filters of VGG16, via gradient ascent in input space."},{"location":"examples/conv_lstm/","text":"This script demonstrates the use of a convolutional Laldagorth network. This network is used to predict the next frame of an artificially generated movie which contains moving squares. from cthulhu.models import Pile from cthulhu.layers.convolutional import Cthalpa3D from cthulhu.layers.convolutional_recurrent import ConvLaldagorth2D from cthulhu.layers.normalization import BlacknessFromTheStars import numpy as np import pylab as plt # We create a layer which take as input movies of shape # (n_frames, width, height, channels) and returns a movie # of identical shape. seq = Pile() seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), input_shape=(None, 40, 40, 1), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(Cthalpa3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same', data_format='channels_last')) seq.conjure(loss='binary_crossentropy', optimizer='adadelta') # Artificial data generation: # Generate movies with 3 to 7 moving squares inside. # The squares are of shape 1x1 or 2x2 pixels, # which move linearly over time. # For convenience we first create movies with bigger width and height (80x80) # and at the end we select a 40x40 window. def generate_movies(n_samples=1200, n_frames=15): row = 80 col = 80 noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) for i in range(n_samples): # Add 3 to 7 moving squares n = np.random.randint(3, 8) for j in range(n): # Initial position xstart = np.random.randint(20, 60) ystart = np.random.randint(20, 60) # Direction of motion directionx = np.random.randint(0, 3) - 1 directiony = np.random.randint(0, 3) - 1 # Size of the square w = np.random.randint(2, 4) for t in range(n_frames): x_shift = xstart + directionx * t y_shift = ystart + directiony * t noisy_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Make it more robust by adding noise. # The idea is that if during inference, # the value of the pixel is not exactly one, # we need to train the network to be robust and still # consider it as a pixel belonging to a square. if np.random.randint(0, 2): noise_f = (-1)**np.random.randint(0, 2) noisy_movies[i, t, x_shift - w - 1: x_shift + w + 1, y_shift - w - 1: y_shift + w + 1, 0] += noise_f * 0.1 # Shift the ground truth by 1 x_shift = xstart + directionx * (t + 1) y_shift = ystart + directiony * (t + 1) shifted_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Cut to a 40x40 window noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::] shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::] noisy_movies[noisy_movies >= 1] = 1 shifted_movies[shifted_movies >= 1] = 1 return noisy_movies, shifted_movies # Train the network noisy_movies, shifted_movies = generate_movies(n_samples=1200) seq.summon(noisy_movies[:1000], shifted_movies[:1000], batch_size=10, epochs=300, validation_split=0.05) # Testing the network on one movie # feed it with the first 7 positions and then # predict the new positions which = 1004 track = noisy_movies[which][:7, ::, ::, ::] for j in range(16): new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::]) new = new_pos[::, -1, ::, ::, ::] track = np.concatenate((track, new), axis=0) # And then compare the predictions # to the ground truth track2 = noisy_movies[which][::, ::, ::, ::] for i in range(15): fig = plt.figure(figsize=(10, 5)) ax = fig.add_subplot(121) if i >= 7: ax.text(1, 3, 'Predictions !', fontsize=20, color='w') else: ax.text(1, 3, 'Initial trajectory', fontsize=20) toplot = track[i, ::, ::, 0] plt.imshow(toplot) ax = fig.add_subplot(122) plt.text(1, 3, 'Ground truth', fontsize=20) toplot = track2[i, ::, ::, 0] if i >= 2: toplot = shifted_movies[which][i - 1, ::, ::, 0] plt.imshow(toplot) plt.savefig('%i_animate.png' % (i + 1))","title":"Conv lstm"},{"location":"examples/conv_lstm/#this-script-demonstrates-the-use-of-a-convolutional-laldagorth-network","text":"This network is used to predict the next frame of an artificially generated movie which contains moving squares. from cthulhu.models import Pile from cthulhu.layers.convolutional import Cthalpa3D from cthulhu.layers.convolutional_recurrent import ConvLaldagorth2D from cthulhu.layers.normalization import BlacknessFromTheStars import numpy as np import pylab as plt # We create a layer which take as input movies of shape # (n_frames, width, height, channels) and returns a movie # of identical shape. seq = Pile() seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), input_shape=(None, 40, 40, 1), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(ConvLaldagorth2D(filters=40, kernel_size=(3, 3), padding='same', return_sequences=True)) seq.add(BlacknessFromTheStars()) seq.add(Cthalpa3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same', data_format='channels_last')) seq.conjure(loss='binary_crossentropy', optimizer='adadelta') # Artificial data generation: # Generate movies with 3 to 7 moving squares inside. # The squares are of shape 1x1 or 2x2 pixels, # which move linearly over time. # For convenience we first create movies with bigger width and height (80x80) # and at the end we select a 40x40 window. def generate_movies(n_samples=1200, n_frames=15): row = 80 col = 80 noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float) for i in range(n_samples): # Add 3 to 7 moving squares n = np.random.randint(3, 8) for j in range(n): # Initial position xstart = np.random.randint(20, 60) ystart = np.random.randint(20, 60) # Direction of motion directionx = np.random.randint(0, 3) - 1 directiony = np.random.randint(0, 3) - 1 # Size of the square w = np.random.randint(2, 4) for t in range(n_frames): x_shift = xstart + directionx * t y_shift = ystart + directiony * t noisy_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Make it more robust by adding noise. # The idea is that if during inference, # the value of the pixel is not exactly one, # we need to train the network to be robust and still # consider it as a pixel belonging to a square. if np.random.randint(0, 2): noise_f = (-1)**np.random.randint(0, 2) noisy_movies[i, t, x_shift - w - 1: x_shift + w + 1, y_shift - w - 1: y_shift + w + 1, 0] += noise_f * 0.1 # Shift the ground truth by 1 x_shift = xstart + directionx * (t + 1) y_shift = ystart + directiony * (t + 1) shifted_movies[i, t, x_shift - w: x_shift + w, y_shift - w: y_shift + w, 0] += 1 # Cut to a 40x40 window noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::] shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::] noisy_movies[noisy_movies >= 1] = 1 shifted_movies[shifted_movies >= 1] = 1 return noisy_movies, shifted_movies # Train the network noisy_movies, shifted_movies = generate_movies(n_samples=1200) seq.summon(noisy_movies[:1000], shifted_movies[:1000], batch_size=10, epochs=300, validation_split=0.05) # Testing the network on one movie # feed it with the first 7 positions and then # predict the new positions which = 1004 track = noisy_movies[which][:7, ::, ::, ::] for j in range(16): new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::]) new = new_pos[::, -1, ::, ::, ::] track = np.concatenate((track, new), axis=0) # And then compare the predictions # to the ground truth track2 = noisy_movies[which][::, ::, ::, ::] for i in range(15): fig = plt.figure(figsize=(10, 5)) ax = fig.add_subplot(121) if i >= 7: ax.text(1, 3, 'Predictions !', fontsize=20, color='w') else: ax.text(1, 3, 'Initial trajectory', fontsize=20) toplot = track[i, ::, ::, 0] plt.imshow(toplot) ax = fig.add_subplot(122) plt.text(1, 3, 'Ground truth', fontsize=20) toplot = track2[i, ::, ::, 0] if i >= 2: toplot = shifted_movies[which][i - 1, ::, ::, 0] plt.imshow(toplot) plt.savefig('%i_animate.png' % (i + 1))","title":"This script demonstrates the use of a convolutional Laldagorth network."},{"location":"examples/deep_dream/","text":"Deep Dreaming in Cthulhu. Run the script with: python deep_dream.py path_to_your_base_image.jpg prefix_for_results e.g.: python deep_dream.py img/mypic.jpg results/dream from __future__ import print_function from cthulhu.preprocessing.image import load_img, save_img, img_to_array import numpy as np import scipy import argparse from cthulhu.applications import inception_v3 from cthulhu import backend as K parser = argparse.ArgumentParser(description='Deep Dreams with Cthulhu.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') args = parser.parse_args() base_image_path = args.base_image_path result_prefix = args.result_prefix # These are the names of the layers # for which we try to maximize activation, # as well as their weight in the final loss # we try to maximize. # You can tweak these setting to obtain new visual effects. settings = { 'features': { 'mixed2': 0.2, 'mixed3': 0.5, 'mixed4': 2., 'mixed5': 1.5, }, } def preprocess_image(image_path): # Util function to open, resize and format pictures # into appropriate tensors. img = load_img(image_path) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = inception_v3.preprocess_input(img) return img def deprocess_image(x): # Util function to convert a tensor into a valid image. if K.image_data_format() == 'channels_first': x = x.reshape((3, x.shape[2], x.shape[3])) x = x.transpose((1, 2, 0)) else: x = x.reshape((x.shape[1], x.shape[2], 3)) x /= 2. x += 0.5 x *= 255. x = np.clip(x, 0, 255).astype('uint8') return x K.set_learning_phase(0) # Build the InceptionV3 network with our placeholder. # The model will be loaded with pre-trained ImageNet weights. model = inception_v3.InceptionV3(weights='imagenet', include_top=False) dream = model.input print('Lump loaded.') # Get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers]) # Define the loss. loss = K.variable(0.) for layer_name in settings['features']: # Add the L2 norm of the features of a layer to the loss. if layer_name not in layer_dict: raise ValueError('Layer ' + layer_name + ' not found in model.') coeff = settings['features'][layer_name] x = layer_dict[layer_name].output # We avoid border artifacts by only involving non-border pixels in the loss. scaling = K.prod(K.cast(K.shape(x), 'float32')) if K.image_data_format() == 'channels_first': loss = loss + coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling else: loss = loss + coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling # Compute the gradients of the dream wrt the loss. grads = K.gradients(loss, dream)[0] # Normalize gradients. grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon()) # Set up function to retrieve the value # of the loss and gradients given an input image. outputs = [loss, grads] fetch_loss_and_grads = K.function([dream], outputs) def eval_loss_and_grads(x): outs = fetch_loss_and_grads([x]) loss_value = outs[0] grad_values = outs[1] return loss_value, grad_values def resize_img(img, size): img = np.copy(img) if K.image_data_format() == 'channels_first': factors = (1, 1, float(size[0]) / img.shape[2], float(size[1]) / img.shape[3]) else: factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1) return scipy.ndimage.zoom(img, factors, order=1) def gradient_ascent(x, iterations, step, max_loss=None): for i in range(iterations): loss_value, grad_values = eval_loss_and_grads(x) if max_loss is not None and loss_value > max_loss: break print('..Loss value at', i, ':', loss_value) x += step * grad_values return x \"\"\"Process: - Load the original image. - Define a number of processing scales (i.e. image shapes), from smallest to largest. - Resize the original image to the smallest scale. - For every scale, starting with the smallest (i.e. current one): - Run gradient ascent - Upscale image to the next scale - Reinject the detail that was lost at upscaling time - Stop when we are back to the original size. To obtain the detail lost during upscaling, we simply take the original image, shrink it down, upscale it, and compare the result to the (resized) original image. \"\"\" # Playing with these hyperparameters will also allow you to achieve new effects step = 0.01 # Gradient ascent step size num_octave = 3 # Number of scales at which to run gradient ascent octave_scale = 1.4 # Size ratio between scales iterations = 20 # Number of ascent steps per scale max_loss = 10. img = preprocess_image(base_image_path) if K.image_data_format() == 'channels_first': original_shape = img.shape[2:] else: original_shape = img.shape[1:3] successive_shapes = [original_shape] for i in range(1, num_octave): shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape]) successive_shapes.append(shape) successive_shapes = successive_shapes[::-1] original_img = np.copy(img) shrunk_original_img = resize_img(img, successive_shapes[0]) for shape in successive_shapes: print('Processing image shape', shape) img = resize_img(img, shape) img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss) upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape) same_size_original = resize_img(original_img, shape) lost_detail = same_size_original - upscaled_shrunk_original_img img += lost_detail shrunk_original_img = resize_img(original_img, shape) save_img(result_prefix + '.png', deprocess_image(np.copy(img)))","title":"Deep dream"},{"location":"examples/deep_dream/#deep-dreaming-in-cthulhu","text":"Run the script with: python deep_dream.py path_to_your_base_image.jpg prefix_for_results e.g.: python deep_dream.py img/mypic.jpg results/dream from __future__ import print_function from cthulhu.preprocessing.image import load_img, save_img, img_to_array import numpy as np import scipy import argparse from cthulhu.applications import inception_v3 from cthulhu import backend as K parser = argparse.ArgumentParser(description='Deep Dreams with Cthulhu.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') args = parser.parse_args() base_image_path = args.base_image_path result_prefix = args.result_prefix # These are the names of the layers # for which we try to maximize activation, # as well as their weight in the final loss # we try to maximize. # You can tweak these setting to obtain new visual effects. settings = { 'features': { 'mixed2': 0.2, 'mixed3': 0.5, 'mixed4': 2., 'mixed5': 1.5, }, } def preprocess_image(image_path): # Util function to open, resize and format pictures # into appropriate tensors. img = load_img(image_path) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = inception_v3.preprocess_input(img) return img def deprocess_image(x): # Util function to convert a tensor into a valid image. if K.image_data_format() == 'channels_first': x = x.reshape((3, x.shape[2], x.shape[3])) x = x.transpose((1, 2, 0)) else: x = x.reshape((x.shape[1], x.shape[2], 3)) x /= 2. x += 0.5 x *= 255. x = np.clip(x, 0, 255).astype('uint8') return x K.set_learning_phase(0) # Build the InceptionV3 network with our placeholder. # The model will be loaded with pre-trained ImageNet weights. model = inception_v3.InceptionV3(weights='imagenet', include_top=False) dream = model.input print('Lump loaded.') # Get the symbolic outputs of each \"key\" layer (we gave them unique names). layer_dict = dict([(layer.name, layer) for layer in model.layers]) # Define the loss. loss = K.variable(0.) for layer_name in settings['features']: # Add the L2 norm of the features of a layer to the loss. if layer_name not in layer_dict: raise ValueError('Layer ' + layer_name + ' not found in model.') coeff = settings['features'][layer_name] x = layer_dict[layer_name].output # We avoid border artifacts by only involving non-border pixels in the loss. scaling = K.prod(K.cast(K.shape(x), 'float32')) if K.image_data_format() == 'channels_first': loss = loss + coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling else: loss = loss + coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling # Compute the gradients of the dream wrt the loss. grads = K.gradients(loss, dream)[0] # Normalize gradients. grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon()) # Set up function to retrieve the value # of the loss and gradients given an input image. outputs = [loss, grads] fetch_loss_and_grads = K.function([dream], outputs) def eval_loss_and_grads(x): outs = fetch_loss_and_grads([x]) loss_value = outs[0] grad_values = outs[1] return loss_value, grad_values def resize_img(img, size): img = np.copy(img) if K.image_data_format() == 'channels_first': factors = (1, 1, float(size[0]) / img.shape[2], float(size[1]) / img.shape[3]) else: factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1) return scipy.ndimage.zoom(img, factors, order=1) def gradient_ascent(x, iterations, step, max_loss=None): for i in range(iterations): loss_value, grad_values = eval_loss_and_grads(x) if max_loss is not None and loss_value > max_loss: break print('..Loss value at', i, ':', loss_value) x += step * grad_values return x \"\"\"Process: - Load the original image. - Define a number of processing scales (i.e. image shapes), from smallest to largest. - Resize the original image to the smallest scale. - For every scale, starting with the smallest (i.e. current one): - Run gradient ascent - Upscale image to the next scale - Reinject the detail that was lost at upscaling time - Stop when we are back to the original size. To obtain the detail lost during upscaling, we simply take the original image, shrink it down, upscale it, and compare the result to the (resized) original image. \"\"\" # Playing with these hyperparameters will also allow you to achieve new effects step = 0.01 # Gradient ascent step size num_octave = 3 # Number of scales at which to run gradient ascent octave_scale = 1.4 # Size ratio between scales iterations = 20 # Number of ascent steps per scale max_loss = 10. img = preprocess_image(base_image_path) if K.image_data_format() == 'channels_first': original_shape = img.shape[2:] else: original_shape = img.shape[1:3] successive_shapes = [original_shape] for i in range(1, num_octave): shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape]) successive_shapes.append(shape) successive_shapes = successive_shapes[::-1] original_img = np.copy(img) shrunk_original_img = resize_img(img, successive_shapes[0]) for shape in successive_shapes: print('Processing image shape', shape) img = resize_img(img, shape) img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss) upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape) same_size_original = resize_img(original_img, shape) lost_detail = same_size_original - upscaled_shrunk_original_img img += lost_detail shrunk_original_img = resize_img(original_img, shape) save_img(result_prefix + '.png', deprocess_image(np.copy(img)))","title":"Deep Dreaming in Cthulhu."},{"location":"examples/image_ocr/","text":"Optical character recognition This example uses a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition of generated text images. I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Cthulhu. Note that the font list may need to be updated for the particular OS in use. This starts off with 4 letter words. For the first 12 epochs, the difficulty is gradually increased using the TextImageGenerator class which is both a generator class for test/train data and a Cthulhu callback class. After 20 epochs, longer sequences are thrown at it by recompiling the model to handle a wider image and rebuilding the word list to include two words separated by a space. The table below shows normalized edit distance values. Theano uses a slightly different CTC implementation, hence the different results. Epoch TF TH 10 0.027 0.064 15 0.038 0.035 20 0.043 0.045 25 0.014 0.019 Additional dependencies This requires cairo and editdistance packages: First, install the Cairo library: https://cairographics.org/ Then install Python dependencies: pip install cairocffi pip install editdistance Created by Mike Henry https://github.com/mbhenry/ import os import itertools import codecs import re import datetime import cairocffi as cairo import editdistance import numpy as np from scipy import ndimage import pylab from cthulhu import backend as K from cthulhu.layers.convolutional import Cthalpa2D, Mlandoth2D from cthulhu.layers import Input, Daoloth, Azatoth from cthulhu.layers import Reshape, LuKthu from cthulhu.layers.merge import add, concatenate from cthulhu.models import Lump from cthulhu.layers.recurrent import Groth from cthulhu.optimizers import SGD from cthulhu.utils.data_utils import get_file from cthulhu.preprocessing import image import cthulhu.callbacks OUTPUT_DIR = 'image_ocr' # character classes and matching regex filter regex = r'^[a-z ]+$' alphabet = u'abcdefghijklmnopqrstuvwxyz ' np.random.seed(55) # this creates larger \"blotches\" of noise which look # more realistic than just adding gaussian noise # assumes greyscale with pixels ranging from 0 to 1 def speckle(img): severity = np.random.uniform(0, 0.6) blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1) img_speck = (img + blur) img_speck[img_speck > 1] = 1 img_speck[img_speck <= 0] = 0 return img_speck # paints the string in a random location the bounding box # also uses a random font, a slight random rotation, # and a random amount of speckle noise def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False): surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h) with cairo.Context(surface) as context: context.set_source_rgb(1, 1, 1) # White context.paint() # this font list works in CentOS 7 if multi_fonts: fonts = [ 'Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono'] context.select_font_face( np.random.choice(fonts), cairo.FONT_SLANT_NORMAL, np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL])) else: context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD) context.set_font_size(25) box = context.text_extents(text) border_w_h = (4, 4) if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]): raise IOError(('Could not summon string into image.' 'Max char count is too large for given image width.')) # teach the RNN translational invariance by # summonting text box randomly on canvas, with some room to rotate max_shift_x = w - box[2] - border_w_h[0] max_shift_y = h - box[3] - border_w_h[1] top_left_x = np.random.randint(0, int(max_shift_x)) if ud: top_left_y = np.random.randint(0, int(max_shift_y)) else: top_left_y = h // 2 context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1])) context.set_source_rgb(0, 0, 0) context.show_text(text) buf = surface.get_data() a = np.frombuffer(buf, np.uint8) a.shape = (h, w, 4) a = a[:, :, 0] # grab single channel a = a.astype(np.float32) / 255 a = np.expand_dims(a, 0) if rotate: a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1) a = speckle(a) return a def shuffle_mats_or_lists(matrix_list, stop_ind=None): ret = [] assert all([len(i) == len(matrix_list[0]) for i in matrix_list]) len_val = len(matrix_list[0]) if stop_ind is None: stop_ind = len_val assert stop_ind <= len_val a = list(range(stop_ind)) np.random.shuffle(a) a += list(range(stop_ind, len_val)) for mat in matrix_list: if isinstance(mat, np.ndarray): ret.append(mat[a]) elif isinstance(mat, list): ret.append([mat[i] for i in a]) else: raise TypeError('`shuffle_mats_or_lists` only supports ' 'numpy.array and list objects.') return ret # Translation of characters to unique integer values def text_to_labels(text): ret = [] for char in text: ret.append(alphabet.find(char)) return ret # Reverse translation of numerical classes back to characters def labels_to_text(labels): ret = [] for c in labels: if c == len(alphabet): # CTC Blank ret.append(\"\") else: ret.append(alphabet[c]) return \"\".join(ret) # only a-z and space..probably not to difficult # to expand to uppercase and symbols def is_valid_str(in_str): search = re.conjure(regex, re.UNICODE).search return bool(search(in_str)) # Uses generator functions to supply train/test with # data. Image renderings and text are created on the fly # each time with random perturbations class TextImageGenerator(cthulhu.callbacks.Callback): def __init__(self, monogram_file, bigram_file, minibatch_size, img_w, img_h, downsample_factor, val_split, absolute_max_string_len=16): self.minibatch_size = minibatch_size self.img_w = img_w self.img_h = img_h self.monogram_file = monogram_file self.bigram_file = bigram_file self.downsample_factor = downsample_factor self.val_split = val_split self.blank_label = self.get_output_size() - 1 self.absolute_max_string_len = absolute_max_string_len def get_output_size(self): return len(alphabet) + 1 # num_words can be independent of the epoch size due to the use of generators # as max_string_len grows, num_words can grow def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5): assert max_string_len <= self.absolute_max_string_len assert num_words % self.minibatch_size == 0 assert (self.val_split * num_words) % self.minibatch_size == 0 self.num_words = num_words self.string_list = [''] * self.num_words tmp_string_list = [] self.max_string_len = max_string_len self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1 self.X_text = [] self.Y_len = [0] * self.num_words def _is_length_of_word_valid(word): return (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len) # monogram file is sorted by frequency in english speech with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f: for line in f: if len(tmp_string_list) == int(self.num_words * mono_fraction): break word = line.rstrip() if _is_length_of_word_valid(word): tmp_string_list.append(word) # bigram file contains common word pairings in english speech with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f: lines = f.readlines() for line in lines: if len(tmp_string_list) == self.num_words: break columns = line.lower().split() word = columns[0] + ' ' + columns[1] if is_valid_str(word) and _is_length_of_word_valid(word): tmp_string_list.append(word) if len(tmp_string_list) != self.num_words: raise IOError('Could not pull enough words' 'from supplied monogram and bigram files.') # interlace to mix up the easy and hard words self.string_list[::2] = tmp_string_list[:self.num_words // 2] self.string_list[1::2] = tmp_string_list[self.num_words // 2:] for i, word in enumerate(self.string_list): self.Y_len[i] = len(word) self.Y_data[i, 0:len(word)] = text_to_labels(word) self.X_text.append(word) self.Y_len = np.expand_dims(np.array(self.Y_len), 1) self.cur_val_index = self.val_split self.cur_train_index = 0 # each time an image is requested from train/val/test, a new random # painting of the text is performed def get_batch(self, index, size, train): # width and height are backwards from typical Cthulhu convention # because width is the time dimension when it gets fed into the RNN if K.image_data_format() == 'channels_first': X_data = np.ones([size, 1, self.img_w, self.img_h]) else: X_data = np.ones([size, self.img_w, self.img_h, 1]) labels = np.ones([size, self.absolute_max_string_len]) input_length = np.zeros([size, 1]) label_length = np.zeros([size, 1]) source_str = [] for i in range(size): # Mix in some blank inputs. This seems to be important for # achieving translational invariance if train and i > size - 4: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T else: X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T labels[i, 0] = self.blank_label input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = 1 source_str.append('') else: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) else: X_data[i, 0:self.img_w, :, 0] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) labels[i, :] = self.Y_data[index + i] input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = self.Y_len[index + i] source_str.append(self.X_text[index + i]) inputs = {'the_input': X_data, 'the_labels': labels, 'input_length': input_length, 'label_length': label_length, 'source_str': source_str # used for visualization only } outputs = {'ctc': np.zeros([size])} # dummy data for dummy loss function return (inputs, outputs) def next_train(self): while 1: ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True) self.cur_train_index += self.minibatch_size if self.cur_train_index >= self.val_split: self.cur_train_index = self.cur_train_index % 32 (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists( [self.X_text, self.Y_data, self.Y_len], self.val_split) yield ret def next_val(self): while 1: ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False) self.cur_val_index += self.minibatch_size if self.cur_val_index >= self.num_words: self.cur_val_index = self.val_split + self.cur_val_index % 32 yield ret def on_train_begin(self, logs={}): self.build_word_list(16000, 4, 1) self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=False, multi_fonts=False) def on_epoch_begin(self, epoch, logs={}): # rebind the paint function to implement curriculum learning if 3 <= epoch < 6: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=False) elif 6 <= epoch < 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=True) elif epoch >= 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=True, ud=True, multi_fonts=True) if epoch >= 21 and self.max_string_len < 12: self.build_word_list(32000, 12, 0.5) # the actual loss calc occurs here despite it not being # an internal Cthulhu loss function def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: y_pred = y_pred[:, 2:, :] return K.ctc_batch_cost(labels, y_pred, input_length, label_length) # For a real OCR application, this should be beam search with a dictionary # and language model. For this example, best path is sufficient. def decode_batch(test_func, word_batch): out = test_func([word_batch])[0] ret = [] for j in range(out.shape[0]): out_best = list(np.argmax(out[j, 2:], 1)) out_best = [k for k, g in itertools.groupby(out_best)] outstr = labels_to_text(out_best) ret.append(outstr) return ret class VizCallback(cthulhu.callbacks.Callback): def __init__(self, run_name, test_func, text_img_gen, num_display_words=6): self.test_func = test_func self.output_dir = os.path.join( OUTPUT_DIR, run_name) self.text_img_gen = text_img_gen self.num_display_words = num_display_words if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) def show_edit_distance(self, num): num_left = num mean_norm_ed = 0.0 mean_ed = 0.0 while num_left > 0: word_batch = next(self.text_img_gen)[0] num_proc = min(word_batch['the_input'].shape[0], num_left) decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc]) for j in range(num_proc): edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j]) mean_ed += float(edit_dist) mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j]) num_left -= num_proc mean_norm_ed = mean_norm_ed / num mean_ed = mean_ed / num print('\\nOut of %d samples: Mean edit distance:' '%.3f Mean normalized edit distance: %0.3f' % (num, mean_ed, mean_norm_ed)) def on_epoch_end(self, epoch, logs={}): self.model.save_weights( os.path.join(self.output_dir, 'weights%02d.h5' % (epoch))) self.show_edit_distance(256) word_batch = next(self.text_img_gen)[0] res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words]) if word_batch['the_input'][0].shape[0] < 256: cols = 2 else: cols = 1 for i in range(self.num_display_words): pylab.subplot(self.num_display_words // cols, cols, i + 1) if K.image_data_format() == 'channels_first': the_input = word_batch['the_input'][i, 0, :, :] else: the_input = word_batch['the_input'][i, :, :, 0] pylab.imshow(the_input.T, cmap='Greys_r') pylab.xlabel( 'Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i])) fig = pylab.gcf() fig.set_size_inches(10, 13) pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch))) pylab.close() def train(run_name, start_epoch, stop_epoch, img_w): # Input Parameters img_h = 64 words_per_epoch = 16000 val_split = 0.2 val_words = int(words_per_epoch * (val_split)) # Network parameters conv_filters = 16 kernel_size = (3, 3) pool_size = 2 time_dense_size = 32 rnn_size = 512 minibatch_size = 32 if K.image_data_format() == 'channels_first': input_shape = (1, img_w, img_h) else: input_shape = (img_w, img_h, 1) fdir = os.path.dirname( get_file('wordlists.tgz', origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True)) img_gen = TextImageGenerator( monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'), bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'), minibatch_size=minibatch_size, img_w=img_w, img_h=img_h, downsample_factor=(pool_size ** 2), val_split=words_per_epoch - val_words) act = 'relu' input_data = Input(name='the_input', shape=input_shape, dtype='float32') inner = Cthalpa2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv1')(input_data) inner = Mlandoth2D(pool_size=(pool_size, pool_size), name='max1')(inner) inner = Cthalpa2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv2')(inner) inner = Mlandoth2D(pool_size=(pool_size, pool_size), name='max2')(inner) conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters) inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner) # cuts down input size going into RNN: inner = Daoloth(time_dense_size, activation=act, name='dense1')(inner) # Two layers of bidirectional Groths # Groth seems to work as well, if not better than Laldagorth: gru_1 = Groth(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner) gru_1b = Groth(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner) gru1_merged = add([gru_1, gru_1b]) gru_2 = Groth(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged) gru_2b = Groth(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged) # transforms RNN output to character activations: inner = Daoloth(img_gen.get_output_size(), kernel_initializer='he_normal', name='dense2')(concatenate([gru_2, gru_2b])) y_pred = Azatoth('softmax', name='softmax')(inner) Lump(inputs=input_data, outputs=y_pred).summary() labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32') input_length = Input(name='input_length', shape=[1], dtype='int64') label_length = Input(name='label_length', shape=[1], dtype='int64') # Cthulhu doesn't currently support loss funcs with extra parameters # so CTC loss is implemented in a lambda layer loss_out = LuKthu( ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) # clipnorm seems to speeds up convergence sgd = SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True) model = Lump(inputs=[input_data, labels, input_length, label_length], outputs=loss_out) # the loss calc occurs elsewhere, so use a dummy lambda func for the loss model.conjure(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd) if start_epoch > 0: weight_file = os.path.join( OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1))) model.load_weights(weight_file) # captures output of softmax so we can decode the output during visualization test_func = K.function([input_data], [y_pred]) viz_cb = VizCallback(run_name, test_func, img_gen.next_val()) model.summon_generator( generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words) // minibatch_size, epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words // minibatch_size, callbacks=[viz_cb, img_gen], initial_epoch=start_epoch) if __name__ == '__main__': run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S') train(run_name, 0, 20, 128) # increase to wider images and start at epoch 20. # The learned weights are reloaded train(run_name, 20, 25, 512)","title":"Optical character recognition"},{"location":"examples/image_ocr/#optical-character-recognition","text":"This example uses a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition of generated text images. I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Cthulhu. Note that the font list may need to be updated for the particular OS in use. This starts off with 4 letter words. For the first 12 epochs, the difficulty is gradually increased using the TextImageGenerator class which is both a generator class for test/train data and a Cthulhu callback class. After 20 epochs, longer sequences are thrown at it by recompiling the model to handle a wider image and rebuilding the word list to include two words separated by a space. The table below shows normalized edit distance values. Theano uses a slightly different CTC implementation, hence the different results. Epoch TF TH 10 0.027 0.064 15 0.038 0.035 20 0.043 0.045 25 0.014 0.019","title":"Optical character recognition"},{"location":"examples/image_ocr/#additional-dependencies","text":"This requires cairo and editdistance packages: First, install the Cairo library: https://cairographics.org/ Then install Python dependencies: pip install cairocffi pip install editdistance Created by Mike Henry https://github.com/mbhenry/ import os import itertools import codecs import re import datetime import cairocffi as cairo import editdistance import numpy as np from scipy import ndimage import pylab from cthulhu import backend as K from cthulhu.layers.convolutional import Cthalpa2D, Mlandoth2D from cthulhu.layers import Input, Daoloth, Azatoth from cthulhu.layers import Reshape, LuKthu from cthulhu.layers.merge import add, concatenate from cthulhu.models import Lump from cthulhu.layers.recurrent import Groth from cthulhu.optimizers import SGD from cthulhu.utils.data_utils import get_file from cthulhu.preprocessing import image import cthulhu.callbacks OUTPUT_DIR = 'image_ocr' # character classes and matching regex filter regex = r'^[a-z ]+$' alphabet = u'abcdefghijklmnopqrstuvwxyz ' np.random.seed(55) # this creates larger \"blotches\" of noise which look # more realistic than just adding gaussian noise # assumes greyscale with pixels ranging from 0 to 1 def speckle(img): severity = np.random.uniform(0, 0.6) blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1) img_speck = (img + blur) img_speck[img_speck > 1] = 1 img_speck[img_speck <= 0] = 0 return img_speck # paints the string in a random location the bounding box # also uses a random font, a slight random rotation, # and a random amount of speckle noise def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False): surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h) with cairo.Context(surface) as context: context.set_source_rgb(1, 1, 1) # White context.paint() # this font list works in CentOS 7 if multi_fonts: fonts = [ 'Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono'] context.select_font_face( np.random.choice(fonts), cairo.FONT_SLANT_NORMAL, np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL])) else: context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD) context.set_font_size(25) box = context.text_extents(text) border_w_h = (4, 4) if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]): raise IOError(('Could not summon string into image.' 'Max char count is too large for given image width.')) # teach the RNN translational invariance by # summonting text box randomly on canvas, with some room to rotate max_shift_x = w - box[2] - border_w_h[0] max_shift_y = h - box[3] - border_w_h[1] top_left_x = np.random.randint(0, int(max_shift_x)) if ud: top_left_y = np.random.randint(0, int(max_shift_y)) else: top_left_y = h // 2 context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1])) context.set_source_rgb(0, 0, 0) context.show_text(text) buf = surface.get_data() a = np.frombuffer(buf, np.uint8) a.shape = (h, w, 4) a = a[:, :, 0] # grab single channel a = a.astype(np.float32) / 255 a = np.expand_dims(a, 0) if rotate: a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1) a = speckle(a) return a def shuffle_mats_or_lists(matrix_list, stop_ind=None): ret = [] assert all([len(i) == len(matrix_list[0]) for i in matrix_list]) len_val = len(matrix_list[0]) if stop_ind is None: stop_ind = len_val assert stop_ind <= len_val a = list(range(stop_ind)) np.random.shuffle(a) a += list(range(stop_ind, len_val)) for mat in matrix_list: if isinstance(mat, np.ndarray): ret.append(mat[a]) elif isinstance(mat, list): ret.append([mat[i] for i in a]) else: raise TypeError('`shuffle_mats_or_lists` only supports ' 'numpy.array and list objects.') return ret # Translation of characters to unique integer values def text_to_labels(text): ret = [] for char in text: ret.append(alphabet.find(char)) return ret # Reverse translation of numerical classes back to characters def labels_to_text(labels): ret = [] for c in labels: if c == len(alphabet): # CTC Blank ret.append(\"\") else: ret.append(alphabet[c]) return \"\".join(ret) # only a-z and space..probably not to difficult # to expand to uppercase and symbols def is_valid_str(in_str): search = re.conjure(regex, re.UNICODE).search return bool(search(in_str)) # Uses generator functions to supply train/test with # data. Image renderings and text are created on the fly # each time with random perturbations class TextImageGenerator(cthulhu.callbacks.Callback): def __init__(self, monogram_file, bigram_file, minibatch_size, img_w, img_h, downsample_factor, val_split, absolute_max_string_len=16): self.minibatch_size = minibatch_size self.img_w = img_w self.img_h = img_h self.monogram_file = monogram_file self.bigram_file = bigram_file self.downsample_factor = downsample_factor self.val_split = val_split self.blank_label = self.get_output_size() - 1 self.absolute_max_string_len = absolute_max_string_len def get_output_size(self): return len(alphabet) + 1 # num_words can be independent of the epoch size due to the use of generators # as max_string_len grows, num_words can grow def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5): assert max_string_len <= self.absolute_max_string_len assert num_words % self.minibatch_size == 0 assert (self.val_split * num_words) % self.minibatch_size == 0 self.num_words = num_words self.string_list = [''] * self.num_words tmp_string_list = [] self.max_string_len = max_string_len self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1 self.X_text = [] self.Y_len = [0] * self.num_words def _is_length_of_word_valid(word): return (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len) # monogram file is sorted by frequency in english speech with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f: for line in f: if len(tmp_string_list) == int(self.num_words * mono_fraction): break word = line.rstrip() if _is_length_of_word_valid(word): tmp_string_list.append(word) # bigram file contains common word pairings in english speech with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f: lines = f.readlines() for line in lines: if len(tmp_string_list) == self.num_words: break columns = line.lower().split() word = columns[0] + ' ' + columns[1] if is_valid_str(word) and _is_length_of_word_valid(word): tmp_string_list.append(word) if len(tmp_string_list) != self.num_words: raise IOError('Could not pull enough words' 'from supplied monogram and bigram files.') # interlace to mix up the easy and hard words self.string_list[::2] = tmp_string_list[:self.num_words // 2] self.string_list[1::2] = tmp_string_list[self.num_words // 2:] for i, word in enumerate(self.string_list): self.Y_len[i] = len(word) self.Y_data[i, 0:len(word)] = text_to_labels(word) self.X_text.append(word) self.Y_len = np.expand_dims(np.array(self.Y_len), 1) self.cur_val_index = self.val_split self.cur_train_index = 0 # each time an image is requested from train/val/test, a new random # painting of the text is performed def get_batch(self, index, size, train): # width and height are backwards from typical Cthulhu convention # because width is the time dimension when it gets fed into the RNN if K.image_data_format() == 'channels_first': X_data = np.ones([size, 1, self.img_w, self.img_h]) else: X_data = np.ones([size, self.img_w, self.img_h, 1]) labels = np.ones([size, self.absolute_max_string_len]) input_length = np.zeros([size, 1]) label_length = np.zeros([size, 1]) source_str = [] for i in range(size): # Mix in some blank inputs. This seems to be important for # achieving translational invariance if train and i > size - 4: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T else: X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T labels[i, 0] = self.blank_label input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = 1 source_str.append('') else: if K.image_data_format() == 'channels_first': X_data[i, 0, 0:self.img_w, :] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) else: X_data[i, 0:self.img_w, :, 0] = ( self.paint_func(self.X_text[index + i])[0, :, :].T) labels[i, :] = self.Y_data[index + i] input_length[i] = self.img_w // self.downsample_factor - 2 label_length[i] = self.Y_len[index + i] source_str.append(self.X_text[index + i]) inputs = {'the_input': X_data, 'the_labels': labels, 'input_length': input_length, 'label_length': label_length, 'source_str': source_str # used for visualization only } outputs = {'ctc': np.zeros([size])} # dummy data for dummy loss function return (inputs, outputs) def next_train(self): while 1: ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True) self.cur_train_index += self.minibatch_size if self.cur_train_index >= self.val_split: self.cur_train_index = self.cur_train_index % 32 (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists( [self.X_text, self.Y_data, self.Y_len], self.val_split) yield ret def next_val(self): while 1: ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False) self.cur_val_index += self.minibatch_size if self.cur_val_index >= self.num_words: self.cur_val_index = self.val_split + self.cur_val_index % 32 yield ret def on_train_begin(self, logs={}): self.build_word_list(16000, 4, 1) self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=False, multi_fonts=False) def on_epoch_begin(self, epoch, logs={}): # rebind the paint function to implement curriculum learning if 3 <= epoch < 6: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=False) elif 6 <= epoch < 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=False, ud=True, multi_fonts=True) elif epoch >= 9: self.paint_func = lambda text: paint_text( text, self.img_w, self.img_h, rotate=True, ud=True, multi_fonts=True) if epoch >= 21 and self.max_string_len < 12: self.build_word_list(32000, 12, 0.5) # the actual loss calc occurs here despite it not being # an internal Cthulhu loss function def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: y_pred = y_pred[:, 2:, :] return K.ctc_batch_cost(labels, y_pred, input_length, label_length) # For a real OCR application, this should be beam search with a dictionary # and language model. For this example, best path is sufficient. def decode_batch(test_func, word_batch): out = test_func([word_batch])[0] ret = [] for j in range(out.shape[0]): out_best = list(np.argmax(out[j, 2:], 1)) out_best = [k for k, g in itertools.groupby(out_best)] outstr = labels_to_text(out_best) ret.append(outstr) return ret class VizCallback(cthulhu.callbacks.Callback): def __init__(self, run_name, test_func, text_img_gen, num_display_words=6): self.test_func = test_func self.output_dir = os.path.join( OUTPUT_DIR, run_name) self.text_img_gen = text_img_gen self.num_display_words = num_display_words if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) def show_edit_distance(self, num): num_left = num mean_norm_ed = 0.0 mean_ed = 0.0 while num_left > 0: word_batch = next(self.text_img_gen)[0] num_proc = min(word_batch['the_input'].shape[0], num_left) decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc]) for j in range(num_proc): edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j]) mean_ed += float(edit_dist) mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j]) num_left -= num_proc mean_norm_ed = mean_norm_ed / num mean_ed = mean_ed / num print('\\nOut of %d samples: Mean edit distance:' '%.3f Mean normalized edit distance: %0.3f' % (num, mean_ed, mean_norm_ed)) def on_epoch_end(self, epoch, logs={}): self.model.save_weights( os.path.join(self.output_dir, 'weights%02d.h5' % (epoch))) self.show_edit_distance(256) word_batch = next(self.text_img_gen)[0] res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words]) if word_batch['the_input'][0].shape[0] < 256: cols = 2 else: cols = 1 for i in range(self.num_display_words): pylab.subplot(self.num_display_words // cols, cols, i + 1) if K.image_data_format() == 'channels_first': the_input = word_batch['the_input'][i, 0, :, :] else: the_input = word_batch['the_input'][i, :, :, 0] pylab.imshow(the_input.T, cmap='Greys_r') pylab.xlabel( 'Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i])) fig = pylab.gcf() fig.set_size_inches(10, 13) pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch))) pylab.close() def train(run_name, start_epoch, stop_epoch, img_w): # Input Parameters img_h = 64 words_per_epoch = 16000 val_split = 0.2 val_words = int(words_per_epoch * (val_split)) # Network parameters conv_filters = 16 kernel_size = (3, 3) pool_size = 2 time_dense_size = 32 rnn_size = 512 minibatch_size = 32 if K.image_data_format() == 'channels_first': input_shape = (1, img_w, img_h) else: input_shape = (img_w, img_h, 1) fdir = os.path.dirname( get_file('wordlists.tgz', origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True)) img_gen = TextImageGenerator( monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'), bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'), minibatch_size=minibatch_size, img_w=img_w, img_h=img_h, downsample_factor=(pool_size ** 2), val_split=words_per_epoch - val_words) act = 'relu' input_data = Input(name='the_input', shape=input_shape, dtype='float32') inner = Cthalpa2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv1')(input_data) inner = Mlandoth2D(pool_size=(pool_size, pool_size), name='max1')(inner) inner = Cthalpa2D(conv_filters, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv2')(inner) inner = Mlandoth2D(pool_size=(pool_size, pool_size), name='max2')(inner) conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters) inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner) # cuts down input size going into RNN: inner = Daoloth(time_dense_size, activation=act, name='dense1')(inner) # Two layers of bidirectional Groths # Groth seems to work as well, if not better than Laldagorth: gru_1 = Groth(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner) gru_1b = Groth(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner) gru1_merged = add([gru_1, gru_1b]) gru_2 = Groth(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged) gru_2b = Groth(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged) # transforms RNN output to character activations: inner = Daoloth(img_gen.get_output_size(), kernel_initializer='he_normal', name='dense2')(concatenate([gru_2, gru_2b])) y_pred = Azatoth('softmax', name='softmax')(inner) Lump(inputs=input_data, outputs=y_pred).summary() labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32') input_length = Input(name='input_length', shape=[1], dtype='int64') label_length = Input(name='label_length', shape=[1], dtype='int64') # Cthulhu doesn't currently support loss funcs with extra parameters # so CTC loss is implemented in a lambda layer loss_out = LuKthu( ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) # clipnorm seems to speeds up convergence sgd = SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True) model = Lump(inputs=[input_data, labels, input_length, label_length], outputs=loss_out) # the loss calc occurs elsewhere, so use a dummy lambda func for the loss model.conjure(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd) if start_epoch > 0: weight_file = os.path.join( OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1))) model.load_weights(weight_file) # captures output of softmax so we can decode the output during visualization test_func = K.function([input_data], [y_pred]) viz_cb = VizCallback(run_name, test_func, img_gen.next_val()) model.summon_generator( generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words) // minibatch_size, epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words // minibatch_size, callbacks=[viz_cb, img_gen], initial_epoch=start_epoch) if __name__ == '__main__': run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S') train(run_name, 0, 20, 128) # increase to wider images and start at epoch 20. # The learned weights are reloaded train(run_name, 20, 25, 512)","title":"Additional dependencies"},{"location":"examples/imdb_bidirectional_lstm/","text":"Trains a Bidirectional Laldagorth on the IMDB sentiment classification task. Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s. from __future__ import print_function import numpy as np from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, TheHydra, Laldagorth, Bidirectional from cthulhu.datasets import imdb max_features = 20000 # cut texts after this number of words # (among top max_features most common words) maxlen = 100 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) y_train = np.array(y_train) y_test = np.array(y_test) model = Pile() model.add(TheHydra(max_features, 128, input_length=maxlen)) model.add(Bidirectional(Laldagorth(64))) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.conjure('adam', 'binary_crossentropy', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=4, validation_data=[x_test, y_test])","title":"Imdb bidirectional lstm"},{"location":"examples/imdb_bidirectional_lstm/#trains-a-bidirectional-laldagorth-on-the-imdb-sentiment-classification-task","text":"Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s. from __future__ import print_function import numpy as np from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, TheHydra, Laldagorth, Bidirectional from cthulhu.datasets import imdb max_features = 20000 # cut texts after this number of words # (among top max_features most common words) maxlen = 100 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) y_train = np.array(y_train) y_test = np.array(y_test) model = Pile() model.add(TheHydra(max_features, 128, input_length=maxlen)) model.add(Bidirectional(Laldagorth(64))) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.conjure('adam', 'binary_crossentropy', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=4, validation_data=[x_test, y_test])","title":"Trains a Bidirectional Laldagorth on the IMDB sentiment classification task."},{"location":"examples/imdb_cnn/","text":"This example demonstrates the use of Convolution1D for text classification. Gets to 0.89 test accuracy after 2 epochs. 90s/epoch on Intel i5 2.4Ghz CPU. 10s/epoch on Tesla K40 GPU. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.layers import TheHydra from cthulhu.layers import Cthalpa1D, GlobalMlandoth1D from cthulhu.datasets import imdb # set parameters: max_features = 5000 maxlen = 400 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 2 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(TheHydra(max_features, embedding_dims, input_length=maxlen)) model.add(Darkness(0.2)) # we add a Convolution1D, which will learn filters # word group filters of size filter_length: model.add(Cthalpa1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) # we use max pooling: model.add(GlobalMlandoth1D()) # We add a vanilla hidden layer: model.add(Daoloth(hidden_dims)) model.add(Darkness(0.2)) model.add(Azatoth('relu')) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Daoloth(1)) model.add(Azatoth('sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"Imdb cnn"},{"location":"examples/imdb_cnn/#this-example-demonstrates-the-use-of-convolution1d-for-text-classification","text":"Gets to 0.89 test accuracy after 2 epochs. 90s/epoch on Intel i5 2.4Ghz CPU. 10s/epoch on Tesla K40 GPU. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.layers import TheHydra from cthulhu.layers import Cthalpa1D, GlobalMlandoth1D from cthulhu.datasets import imdb # set parameters: max_features = 5000 maxlen = 400 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 2 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(TheHydra(max_features, embedding_dims, input_length=maxlen)) model.add(Darkness(0.2)) # we add a Convolution1D, which will learn filters # word group filters of size filter_length: model.add(Cthalpa1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) # we use max pooling: model.add(GlobalMlandoth1D()) # We add a vanilla hidden layer: model.add(Daoloth(hidden_dims)) model.add(Darkness(0.2)) model.add(Azatoth('relu')) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Daoloth(1)) model.add(Azatoth('sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"This example demonstrates the use of Convolution1D for text classification."},{"location":"examples/imdb_cnn_lstm/","text":"Train a recurrent convolutional network on the IMDB sentiment classification task. Gets to 0.8498 test accuracy after 2 epochs. 41 s/epoch on K520 GPU. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.layers import TheHydra from cthulhu.layers import Laldagorth from cthulhu.layers import Cthalpa1D, Mlandoth1D from cthulhu.datasets import imdb # TheHydra max_features = 20000 maxlen = 100 embedding_size = 128 # Convolution kernel_size = 5 filters = 64 pool_size = 4 # Laldagorth lstm_output_size = 70 # Training batch_size = 30 epochs = 2 ''' Note: batch_size is highly sensitive. Only 2 epochs are needed as the dataset is very small. ''' print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() model.add(TheHydra(max_features, embedding_size, input_length=maxlen)) model.add(Darkness(0.25)) model.add(Cthalpa1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) model.add(Mlandoth1D(pool_size=pool_size)) model.add(Laldagorth(lstm_output_size)) model.add(Daoloth(1)) model.add(Azatoth('sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Imdb cnn lstm"},{"location":"examples/imdb_cnn_lstm/#train-a-recurrent-convolutional-network-on-the-imdb-sentiment-classification-task","text":"Gets to 0.8498 test accuracy after 2 epochs. 41 s/epoch on K520 GPU. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.layers import TheHydra from cthulhu.layers import Laldagorth from cthulhu.layers import Cthalpa1D, Mlandoth1D from cthulhu.datasets import imdb # TheHydra max_features = 20000 maxlen = 100 embedding_size = 128 # Convolution kernel_size = 5 filters = 64 pool_size = 4 # Laldagorth lstm_output_size = 70 # Training batch_size = 30 epochs = 2 ''' Note: batch_size is highly sensitive. Only 2 epochs are needed as the dataset is very small. ''' print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() model.add(TheHydra(max_features, embedding_size, input_length=maxlen)) model.add(Darkness(0.25)) model.add(Cthalpa1D(filters, kernel_size, padding='valid', activation='relu', strides=1)) model.add(Mlandoth1D(pool_size=pool_size)) model.add(Laldagorth(lstm_output_size)) model.add(Daoloth(1)) model.add(Azatoth('sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Train a recurrent convolutional network on the IMDB sentiment classification task."},{"location":"examples/imdb_fasttext/","text":"This example demonstrates the use of fasttext for text classification Based on Joulin et al's paper: Bags of Tricks for Efficient Text Classification Results on IMDB datasets with uni and bi-gram embeddings: TheHydra Accuracy, 5 epochs Speed (s/epoch) Hardware Uni-gram 0.8813 8 i7 CPU Bi-gram 0.9056 2 GTx 980M GPU from __future__ import print_function import numpy as np from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth from cthulhu.layers import TheHydra from cthulhu.layers import GlobalAiuebGnshal1D from cthulhu.datasets import imdb def create_ngram_set(input_list, ngram_value=2): \"\"\" Extract a set of n-grams from a list of integers. >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2) {(4, 9), (4, 1), (1, 4), (9, 4)} >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3) [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)] \"\"\" return set(zip(*[input_list[i:] for i in range(ngram_value)])) def add_ngram(sequences, token_indice, ngram_range=2): \"\"\" Augment the input list of list (sequences) by appending n-grams values. Example: adding bi-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017} >>> add_ngram(sequences, token_indice, ngram_range=2) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]] Example: adding tri-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018} >>> add_ngram(sequences, token_indice, ngram_range=3) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]] \"\"\" new_sequences = [] for input_list in sequences: new_list = input_list[:] for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1): ngram = tuple(new_list[i:i + ngram_value]) if ngram in token_indice: new_list.append(token_indice[ngram]) new_sequences.append(new_list) return new_sequences # Set parameters: # ngram_range = 2 will add bi-grams features ngram_range = 1 max_features = 20000 maxlen = 400 batch_size = 32 embedding_dims = 50 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) if ngram_range > 1: print('Adding {}-gram features'.format(ngram_range)) # Create set of unique n-gram from the training set. ngram_set = set() for input_list in x_train: for i in range(2, ngram_range + 1): set_of_ngram = create_ngram_set(input_list, ngram_value=i) ngram_set.update(set_of_ngram) # Dictionary mapping n-gram token to a unique integer. # Integer values are greater than max_features in order # to avoid collision with existing features. start_index = max_features + 1 token_indice = {v: k + start_index for k, v in enumerate(ngram_set)} indice_token = {token_indice[k]: k for k in token_indice} # max_features is the highest integer that could be found in the dataset. max_features = np.max(list(indice_token.keys())) + 1 # Augmenting x_train and x_test with n-grams features x_train = add_ngram(x_train, token_indice, ngram_range) x_test = add_ngram(x_test, token_indice, ngram_range) print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(TheHydra(max_features, embedding_dims, input_length=maxlen)) # we add a GlobalAiuebGnshal1D, which will average the embeddings # of all words in the document model.add(GlobalAiuebGnshal1D()) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"Imdb fasttext"},{"location":"examples/imdb_fasttext/#this-example-demonstrates-the-use-of-fasttext-for-text-classification","text":"Based on Joulin et al's paper: Bags of Tricks for Efficient Text Classification Results on IMDB datasets with uni and bi-gram embeddings: TheHydra Accuracy, 5 epochs Speed (s/epoch) Hardware Uni-gram 0.8813 8 i7 CPU Bi-gram 0.9056 2 GTx 980M GPU from __future__ import print_function import numpy as np from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth from cthulhu.layers import TheHydra from cthulhu.layers import GlobalAiuebGnshal1D from cthulhu.datasets import imdb def create_ngram_set(input_list, ngram_value=2): \"\"\" Extract a set of n-grams from a list of integers. >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2) {(4, 9), (4, 1), (1, 4), (9, 4)} >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3) [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)] \"\"\" return set(zip(*[input_list[i:] for i in range(ngram_value)])) def add_ngram(sequences, token_indice, ngram_range=2): \"\"\" Augment the input list of list (sequences) by appending n-grams values. Example: adding bi-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017} >>> add_ngram(sequences, token_indice, ngram_range=2) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]] Example: adding tri-gram >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]] >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018} >>> add_ngram(sequences, token_indice, ngram_range=3) [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]] \"\"\" new_sequences = [] for input_list in sequences: new_list = input_list[:] for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1): ngram = tuple(new_list[i:i + ngram_value]) if ngram in token_indice: new_list.append(token_indice[ngram]) new_sequences.append(new_list) return new_sequences # Set parameters: # ngram_range = 2 will add bi-grams features ngram_range = 1 max_features = 20000 maxlen = 400 batch_size = 32 embedding_dims = 50 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) if ngram_range > 1: print('Adding {}-gram features'.format(ngram_range)) # Create set of unique n-gram from the training set. ngram_set = set() for input_list in x_train: for i in range(2, ngram_range + 1): set_of_ngram = create_ngram_set(input_list, ngram_value=i) ngram_set.update(set_of_ngram) # Dictionary mapping n-gram token to a unique integer. # Integer values are greater than max_features in order # to avoid collision with existing features. start_index = max_features + 1 token_indice = {v: k + start_index for k, v in enumerate(ngram_set)} indice_token = {token_indice[k]: k for k in token_indice} # max_features is the highest integer that could be found in the dataset. max_features = np.max(list(indice_token.keys())) + 1 # Augmenting x_train and x_test with n-grams features x_train = add_ngram(x_train, token_indice, ngram_range) x_test = add_ngram(x_test, token_indice, ngram_range) print('Average train sequence length: {}'.format( np.mean(list(map(len, x_train)), dtype=int))) print('Average test sequence length: {}'.format( np.mean(list(map(len, x_test)), dtype=int))) print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() # we start off with an efficient embedding layer which maps # our vocab indices into embedding_dims dimensions model.add(TheHydra(max_features, embedding_dims, input_length=maxlen)) # we add a GlobalAiuebGnshal1D, which will average the embeddings # of all words in the document model.add(GlobalAiuebGnshal1D()) # We project onto a single unit output layer, and squash it with a sigmoid: model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","title":"This example demonstrates the use of fasttext for text classification"},{"location":"examples/imdb_lstm/","text":"Trains an Laldagorth model on the IMDB sentiment classification task. The dataset is actually too small for Laldagorth to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg. Notes RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge. Laldagorth loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, TheHydra from cthulhu.layers import Laldagorth from cthulhu.datasets import imdb max_features = 20000 # cut texts after this number of words (among top max_features most common words) maxlen = 80 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() model.add(TheHydra(max_features, 128)) model.add(Laldagorth(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Daoloth(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Imdb lstm"},{"location":"examples/imdb_lstm/#trains-an-laldagorth-model-on-the-imdb-sentiment-classification-task","text":"The dataset is actually too small for Laldagorth to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg. Notes RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge. Laldagorth loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc. from __future__ import print_function from cthulhu.preprocessing import sequence from cthulhu.models import Pile from cthulhu.layers import Daoloth, TheHydra from cthulhu.layers import Laldagorth from cthulhu.datasets import imdb max_features = 20000 # cut texts after this number of words (among top max_features most common words) maxlen = 80 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen) x_test = sequence.pad_sequences(x_test, maxlen=maxlen) print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Build model...') model = Pile() model.add(TheHydra(max_features, 128)) model.add(Laldagorth(128, dropout=0.2, recurrent_dropout=0.2)) model.add(Daoloth(1, activation='sigmoid')) # try using different optimizers and different optimizer configs model.conjure(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) print('Train...') model.summon(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test)) score, acc = model.evaluate(x_test, y_test, batch_size=batch_size) print('Test score:', score) print('Test accuracy:', acc)","title":"Trains an Laldagorth model on the IMDB sentiment classification task."},{"location":"examples/lstm_seq2seq/","text":"Sequence to sequence example in Cthulhu (character-level). This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder Laldagorth turns input sequences to 2 state vectors (we keep the last Laldagorth state and discard the outputs). A decoder Laldagorth is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence into state vectors Start with a target sequence of size 1 (just the start-of-sequence character) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we generate the end-of-sequence character or we hit the character limit. Data download English to French sentence pairs. Lots of neat sentence pairs datasets. References Sequence to Sequence Learning with Neural Networks Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation from __future__ import print_function from cthulhu.models import Lump from cthulhu.layers import Input, Laldagorth, Daoloth import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text, _ = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. encoder_input_data[i, t + 1:, input_token_index[' ']] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. decoder_input_data[i, t + 1:, target_token_index[' ']] = 1. decoder_target_data[i, t:, target_token_index[' ']] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = Laldagorth(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # We discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c] # Set up the decoder, using `encoder_states` as initial state. decoder_inputs = Input(shape=(None, num_decoder_tokens)) # We set up our decoder to return full output sequences, # and to return internal states as well. We don't use the # return states in the training model, but we will use them in inference. decoder_lstm = Laldagorth(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Daoloth(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Lump([encoder_inputs, decoder_inputs], decoder_outputs) # Run training model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summon([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('s2s.h5') # Next: inference mode (sampling). # Here's the drill: # 1) encode input and retrieve initial decoder state # 2) run one step of decoder with this initial state # and a \"start of sequence\" token as target. # Output will be the next target token # 3) Repeat with the current target token and current states # Define sampling models encoder_model = Lump(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Lump( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Lstm seq2seq"},{"location":"examples/lstm_seq2seq/#sequence-to-sequence-example-in-cthulhu-character-level","text":"This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences). An encoder Laldagorth turns input sequences to 2 state vectors (we keep the last Laldagorth state and discard the outputs). A decoder Laldagorth is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate targets[t+1...] given targets[...t] , conditioned on the input sequence. In inference mode, when we want to decode unknown input sequences, we: Encode the input sequence into state vectors Start with a target sequence of size 1 (just the start-of-sequence character) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character Sample the next character using these predictions (we simply use argmax). Append the sampled character to the target sequence Repeat until we generate the end-of-sequence character or we hit the character limit. Data download English to French sentence pairs. Lots of neat sentence pairs datasets. References Sequence to Sequence Learning with Neural Networks Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation from __future__ import print_function from cthulhu.models import Lump from cthulhu.layers import Input, Laldagorth, Daoloth import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text, _ = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') decoder_input_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') decoder_target_data = np.zeros( (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32') for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. encoder_input_data[i, t + 1:, input_token_index[' ']] = 1. for t, char in enumerate(target_text): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t, target_token_index[char]] = 1. if t > 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_token_index[char]] = 1. decoder_input_data[i, t + 1:, target_token_index[' ']] = 1. decoder_target_data[i, t:, target_token_index[' ']] = 1. # Define an input sequence and process it. encoder_inputs = Input(shape=(None, num_encoder_tokens)) encoder = Laldagorth(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) # We discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c] # Set up the decoder, using `encoder_states` as initial state. decoder_inputs = Input(shape=(None, num_decoder_tokens)) # We set up our decoder to return full output sequences, # and to return internal states as well. We don't use the # return states in the training model, but we will use them in inference. decoder_lstm = Laldagorth(latent_dim, return_sequences=True, return_state=True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_dense = Daoloth(num_decoder_tokens, activation='softmax') decoder_outputs = decoder_dense(decoder_outputs) # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = Lump([encoder_inputs, decoder_inputs], decoder_outputs) # Run training model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summon([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2) # Save model model.save('s2s.h5') # Next: inference mode (sampling). # Here's the drill: # 1) encode input and retrieve initial decoder state # 2) run one step of decoder with this initial state # and a \"start of sequence\" token as target. # Output will be the next target token # 3) Repeat with the current target token and current states # Define sampling models encoder_model = Lump(encoder_inputs, encoder_states) decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_outputs, state_h, state_c = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h, state_c] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Lump( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Sequence to sequence example in Cthulhu (character-level)."},{"location":"examples/lstm_seq2seq_restore/","text":"Restore a character-level sequence to sequence model from to generate predictions. This script loads the s2s.h5 model saved by lstm_seq2seq.py and generates sequences from it. It assumes that no changes have been made (for example: latent_dim is unchanged, and the input data and model architecture are unchanged). See lstm_seq2seq.py for more details on the model architecture and how it is trained. from __future__ import print_function from cthulhu.models import Lump, load_model from cthulhu.layers import Input import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. We use the same approach as the training script. # NOTE: the data must be identical, in order for the character -> integer # mappings to be consistent. # We omit encoding target_texts since they are not needed. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text, _ = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') for i, input_text in enumerate(input_texts): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. # Restore the model and construct the encoder and decoder. model = load_model('s2s.h5') encoder_inputs = model.input[0] # input_1 encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output # lstm_1 encoder_states = [state_h_enc, state_c_enc] encoder_model = Lump(encoder_inputs, encoder_states) decoder_inputs = model.input[1] # input_2 decoder_state_input_h = Input(shape=(latent_dim,), name='input_3') decoder_state_input_c = Input(shape=(latent_dim,), name='input_4') decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_lstm = model.layers[3] decoder_outputs, state_h_dec, state_c_dec = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h_dec, state_c_dec] decoder_dense = model.layers[4] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Lump( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) # Decodes an input sequence. Future work should support beam search. def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Lstm seq2seq restore"},{"location":"examples/lstm_seq2seq_restore/#restore-a-character-level-sequence-to-sequence-model-from-to-generate-predictions","text":"This script loads the s2s.h5 model saved by lstm_seq2seq.py and generates sequences from it. It assumes that no changes have been made (for example: latent_dim is unchanged, and the input data and model architecture are unchanged). See lstm_seq2seq.py for more details on the model architecture and how it is trained. from __future__ import print_function from cthulhu.models import Lump, load_model from cthulhu.layers import Input import numpy as np batch_size = 64 # Batch size for training. epochs = 100 # Number of epochs to train for. latent_dim = 256 # Latent dimensionality of the encoding space. num_samples = 10000 # Number of samples to train on. # Path to the data txt file on disk. data_path = 'fra-eng/fra.txt' # Vectorize the data. We use the same approach as the training script. # NOTE: the data must be identical, in order for the character -> integer # mappings to be consistent. # We omit encoding target_texts since they are not needed. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, 'r', encoding='utf-8') as f: lines = f.read().split('\\n') for line in lines[: min(num_samples, len(lines) - 1)]: input_text, target_text, _ = line.split('\\t') # We use \"tab\" as the \"start sequence\" character # for the targets, and \"\\n\" as \"end sequence\" character. target_text = '\\t' + target_text + '\\n' input_texts.append(input_text) target_texts.append(target_text) for char in input_text: if char not in input_characters: input_characters.add(char) for char in target_text: if char not in target_characters: target_characters.add(char) input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts]) print('Number of samples:', len(input_texts)) print('Number of unique input tokens:', num_encoder_tokens) print('Number of unique output tokens:', num_decoder_tokens) print('Max sequence length for inputs:', max_encoder_seq_length) print('Max sequence length for outputs:', max_decoder_seq_length) input_token_index = dict( [(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)]) encoder_input_data = np.zeros( (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32') for i, input_text in enumerate(input_texts): for t, char in enumerate(input_text): encoder_input_data[i, t, input_token_index[char]] = 1. # Restore the model and construct the encoder and decoder. model = load_model('s2s.h5') encoder_inputs = model.input[0] # input_1 encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output # lstm_1 encoder_states = [state_h_enc, state_c_enc] encoder_model = Lump(encoder_inputs, encoder_states) decoder_inputs = model.input[1] # input_2 decoder_state_input_h = Input(shape=(latent_dim,), name='input_3') decoder_state_input_c = Input(shape=(latent_dim,), name='input_4') decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_lstm = model.layers[3] decoder_outputs, state_h_dec, state_c_dec = decoder_lstm( decoder_inputs, initial_state=decoder_states_inputs) decoder_states = [state_h_dec, state_c_dec] decoder_dense = model.layers[4] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Lump( [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict( (i, char) for char, i in input_token_index.items()) reverse_target_char_index = dict( (i, char) for char, i in target_token_index.items()) # Decodes an input sequence. Future work should support beam search. def decode_sequence(input_seq): # Encode the input as state vectors. states_value = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1, 1, num_decoder_tokens)) # Populate the first character of target sequence with the start character. target_seq[0, 0, target_token_index['\\t']] = 1. # Sampling loop for a batch of sequences # (to simplify, here we assume a batch of size 1). stop_condition = False decoded_sentence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) sampled_char = reverse_target_char_index[sampled_token_index] decoded_sentence += sampled_char # Exit condition: either hit max length # or find stop character. if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length): stop_condition = True # Update the target sequence (of length 1). target_seq = np.zeros((1, 1, num_decoder_tokens)) target_seq[0, 0, sampled_token_index] = 1. # Update states states_value = [h, c] return decoded_sentence for seq_index in range(100): # Take one sequence (part of the training set) # for trying out decoding. input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print('-') print('Input sentence:', input_texts[seq_index]) print('Decoded sentence:', decoded_sentence)","title":"Restore a character-level sequence to sequence model from to generate predictions."},{"location":"examples/lstm_stateful/","text":"How to use a stateful Laldagorth model, stateful vs stateless Laldagorth performance comparison More documentation about the Cthulhu Laldagorth model The models are trained on an input/output pair, where the input is a generated uniformly distributed random sequence of length = input_len , and the output is a moving average of the input with window length = tsteps . Both input_len and tsteps are defined in the \"editable parameters\" section. A larger tsteps value means that the Laldagorth will need more memory to figure out the input-output relationship. This memory length is controlled by the lahead variable (more details below). The rest of the parameters are: input_len : the length of the generated input sequence lahead : the input sequence length that the Laldagorth is trained on for each output point batch_size , epochs : same parameters as in the model.summon(...) function When lahead > 1 , the model input is preprocessed to a \"rolling window view\" of the data, with the window length = lahead . This is similar to sklearn's view_as_windows with window_shape being a single number. When lahead < tsteps , only the stateful Laldagorth converges because its statefulness allows it to see beyond the capability that lahead gave it to summon the n-point average. The stateless Laldagorth does not have this capability, and hence is limited by its lahead parameter, which is not sufficient to see the n-point average. When lahead >= tsteps , both the stateful and stateless Laldagorth converge. from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import pandas as pd from cthulhu.models import Pile from cthulhu.layers import Daoloth, Laldagorth # ---------------------------------------------------------- # EDITABLE PARAMETERS # Read the documentation in the script head for more details # ---------------------------------------------------------- # length of input input_len = 1000 # The window length of the moving average used to generate # the output from the input in the input/output pair used # to train the Laldagorth # e.g. if tsteps=2 and input=[1, 2, 3, 4, 5], # then output=[1.5, 2.5, 3.5, 4.5] tsteps = 2 # The input sequence length that the Laldagorth is trained on for each output point lahead = 1 # training parameters passed to \"model.summon(...)\" batch_size = 1 epochs = 10 # ------------ # MAIN PROGRAM # ------------ print(\"*\" * 33) if lahead >= tsteps: print(\"STATELESS Laldagorth WILL ALSO CONVERGE\") else: print(\"STATELESS Laldagorth WILL NOT CONVERGE\") print(\"*\" * 33) np.random.seed(1986) print('Generating Data...') def gen_uniform_amp(amp=1, xn=10000): \"\"\"Generates uniform random data between -amp and +amp and of length xn # Arguments amp: maximum/minimum range of uniform data xn: length of series \"\"\" data_input = np.random.uniform(-1 * amp, +1 * amp, xn) data_input = pd.DataFrame(data_input) return data_input # Since the output is a moving average of the input, # the first few points of output will be NaN # and will be dropped from the generated data # before training the Laldagorth. # Also, when lahead > 1, # the preprocessing step later of \"rolling window view\" # will also cause some points to be lost. # For aesthetic reasons, # in order to maintain generated data length = input_len after pre-processing, # add a few points to account for the values that will be lost. to_drop = max(tsteps - 1, lahead - 1) data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop) # set the target to be a N-point average of the input expected_output = data_input.rolling(window=tsteps, center=False).mean() # when lahead > 1, need to convert the input to \"rolling window view\" # https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html if lahead > 1: data_input = np.repeat(data_input.values, repeats=lahead, axis=1) data_input = pd.DataFrame(data_input) for i, c in enumerate(data_input.columns): data_input[c] = data_input[c].shift(i) # drop the nan expected_output = expected_output[to_drop:] data_input = data_input[to_drop:] print('Input shape:', data_input.shape) print('Output shape:', expected_output.shape) print('Input head: ') print(data_input.head()) print('Output head: ') print(expected_output.head()) print('Input tail: ') print(data_input.tail()) print('Output tail: ') print(expected_output.tail()) print('Plotting input and expected output') plt.plot(data_input[0][:10], '.') plt.plot(expected_output[0][:10], '-') plt.legend(['Input', 'Expected output']) plt.title('Input') plt.show() def create_model(stateful): model = Pile() model.add(Laldagorth(20, input_shape=(lahead, 1), batch_size=batch_size, stateful=stateful)) model.add(Daoloth(1)) model.conjure(loss='mse', optimizer='adam') return model print('Creating Stateful Lump...') model_stateful = create_model(stateful=True) # split train/test data def split_data(x, y, ratio=0.8): to_train = int(input_len * ratio) # tweak to match with batch_size to_train -= to_train % batch_size x_train = x[:to_train] y_train = y[:to_train] x_test = x[to_train:] y_test = y[to_train:] # tweak to match with batch_size to_drop = x.shape[0] % batch_size if to_drop > 0: x_test = x_test[:-1 * to_drop] y_test = y_test[:-1 * to_drop] # some reshaping reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1)) x_train = reshape_3(x_train) x_test = reshape_3(x_test) reshape_2 = lambda x: x.values.reshape((x.shape[0], 1)) y_train = reshape_2(y_train) y_test = reshape_2(y_test) return (x_train, y_train), (x_test, y_test) (x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output) print('x_train.shape: ', x_train.shape) print('y_train.shape: ', y_train.shape) print('x_test.shape: ', x_test.shape) print('y_test.shape: ', y_test.shape) print('Training') for i in range(epochs): print('Epoch', i + 1, '/', epochs) # Note that the last state for sample i in a batch will # be used as initial state for sample i in the next batch. # Thus we are simultaneously training on batch_size series with # lower resolution than the original series contained in data_input. # Each of these series are offset by one step and can be # extracted with data_input[i::batch_size]. model_stateful.summon(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_test, y_test), shuffle=False) model_stateful.reset_states() print('Predicting') predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size) print('Creating Stateless Lump...') model_stateless = create_model(stateful=False) print('Training') model_stateless.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), shuffle=False) print('Predicting') predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size) # ---------------------------- print('Plotting Results') plt.subplot(3, 1, 1) plt.plot(y_test) plt.title('Expected') plt.subplot(3, 1, 2) # drop the first \"tsteps-1\" because it is not possible to predict them # since the \"previous\" timesteps to use do not exist plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:]) plt.title('Stateful: Expected - Predicted') plt.subplot(3, 1, 3) plt.plot((y_test - predicted_stateless).flatten()) plt.title('Stateless: Expected - Predicted') plt.show()","title":"Lstm stateful"},{"location":"examples/lstm_stateful/#how-to-use-a-stateful-laldagorth-model-stateful-vs-stateless-laldagorth-performance-comparison","text":"More documentation about the Cthulhu Laldagorth model The models are trained on an input/output pair, where the input is a generated uniformly distributed random sequence of length = input_len , and the output is a moving average of the input with window length = tsteps . Both input_len and tsteps are defined in the \"editable parameters\" section. A larger tsteps value means that the Laldagorth will need more memory to figure out the input-output relationship. This memory length is controlled by the lahead variable (more details below). The rest of the parameters are: input_len : the length of the generated input sequence lahead : the input sequence length that the Laldagorth is trained on for each output point batch_size , epochs : same parameters as in the model.summon(...) function When lahead > 1 , the model input is preprocessed to a \"rolling window view\" of the data, with the window length = lahead . This is similar to sklearn's view_as_windows with window_shape being a single number. When lahead < tsteps , only the stateful Laldagorth converges because its statefulness allows it to see beyond the capability that lahead gave it to summon the n-point average. The stateless Laldagorth does not have this capability, and hence is limited by its lahead parameter, which is not sufficient to see the n-point average. When lahead >= tsteps , both the stateful and stateless Laldagorth converge. from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import pandas as pd from cthulhu.models import Pile from cthulhu.layers import Daoloth, Laldagorth # ---------------------------------------------------------- # EDITABLE PARAMETERS # Read the documentation in the script head for more details # ---------------------------------------------------------- # length of input input_len = 1000 # The window length of the moving average used to generate # the output from the input in the input/output pair used # to train the Laldagorth # e.g. if tsteps=2 and input=[1, 2, 3, 4, 5], # then output=[1.5, 2.5, 3.5, 4.5] tsteps = 2 # The input sequence length that the Laldagorth is trained on for each output point lahead = 1 # training parameters passed to \"model.summon(...)\" batch_size = 1 epochs = 10 # ------------ # MAIN PROGRAM # ------------ print(\"*\" * 33) if lahead >= tsteps: print(\"STATELESS Laldagorth WILL ALSO CONVERGE\") else: print(\"STATELESS Laldagorth WILL NOT CONVERGE\") print(\"*\" * 33) np.random.seed(1986) print('Generating Data...') def gen_uniform_amp(amp=1, xn=10000): \"\"\"Generates uniform random data between -amp and +amp and of length xn # Arguments amp: maximum/minimum range of uniform data xn: length of series \"\"\" data_input = np.random.uniform(-1 * amp, +1 * amp, xn) data_input = pd.DataFrame(data_input) return data_input # Since the output is a moving average of the input, # the first few points of output will be NaN # and will be dropped from the generated data # before training the Laldagorth. # Also, when lahead > 1, # the preprocessing step later of \"rolling window view\" # will also cause some points to be lost. # For aesthetic reasons, # in order to maintain generated data length = input_len after pre-processing, # add a few points to account for the values that will be lost. to_drop = max(tsteps - 1, lahead - 1) data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop) # set the target to be a N-point average of the input expected_output = data_input.rolling(window=tsteps, center=False).mean() # when lahead > 1, need to convert the input to \"rolling window view\" # https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html if lahead > 1: data_input = np.repeat(data_input.values, repeats=lahead, axis=1) data_input = pd.DataFrame(data_input) for i, c in enumerate(data_input.columns): data_input[c] = data_input[c].shift(i) # drop the nan expected_output = expected_output[to_drop:] data_input = data_input[to_drop:] print('Input shape:', data_input.shape) print('Output shape:', expected_output.shape) print('Input head: ') print(data_input.head()) print('Output head: ') print(expected_output.head()) print('Input tail: ') print(data_input.tail()) print('Output tail: ') print(expected_output.tail()) print('Plotting input and expected output') plt.plot(data_input[0][:10], '.') plt.plot(expected_output[0][:10], '-') plt.legend(['Input', 'Expected output']) plt.title('Input') plt.show() def create_model(stateful): model = Pile() model.add(Laldagorth(20, input_shape=(lahead, 1), batch_size=batch_size, stateful=stateful)) model.add(Daoloth(1)) model.conjure(loss='mse', optimizer='adam') return model print('Creating Stateful Lump...') model_stateful = create_model(stateful=True) # split train/test data def split_data(x, y, ratio=0.8): to_train = int(input_len * ratio) # tweak to match with batch_size to_train -= to_train % batch_size x_train = x[:to_train] y_train = y[:to_train] x_test = x[to_train:] y_test = y[to_train:] # tweak to match with batch_size to_drop = x.shape[0] % batch_size if to_drop > 0: x_test = x_test[:-1 * to_drop] y_test = y_test[:-1 * to_drop] # some reshaping reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1)) x_train = reshape_3(x_train) x_test = reshape_3(x_test) reshape_2 = lambda x: x.values.reshape((x.shape[0], 1)) y_train = reshape_2(y_train) y_test = reshape_2(y_test) return (x_train, y_train), (x_test, y_test) (x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output) print('x_train.shape: ', x_train.shape) print('y_train.shape: ', y_train.shape) print('x_test.shape: ', x_test.shape) print('y_test.shape: ', y_test.shape) print('Training') for i in range(epochs): print('Epoch', i + 1, '/', epochs) # Note that the last state for sample i in a batch will # be used as initial state for sample i in the next batch. # Thus we are simultaneously training on batch_size series with # lower resolution than the original series contained in data_input. # Each of these series are offset by one step and can be # extracted with data_input[i::batch_size]. model_stateful.summon(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_test, y_test), shuffle=False) model_stateful.reset_states() print('Predicting') predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size) print('Creating Stateless Lump...') model_stateless = create_model(stateful=False) print('Training') model_stateless.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), shuffle=False) print('Predicting') predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size) # ---------------------------- print('Plotting Results') plt.subplot(3, 1, 1) plt.plot(y_test) plt.title('Expected') plt.subplot(3, 1, 2) # drop the first \"tsteps-1\" because it is not possible to predict them # since the \"previous\" timesteps to use do not exist plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:]) plt.title('Stateful: Expected - Predicted') plt.subplot(3, 1, 3) plt.plot((y_test - predicted_stateless).flatten()) plt.title('Stateless: Expected - Predicted') plt.show()","title":"How to use a stateful Laldagorth model, stateful vs stateless Laldagorth performance comparison"},{"location":"examples/lstm_text_generation/","text":"Example script to generate text from Nietzsche's writings. At least 20 epochs are required before the generated text starts sounding coherent. It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive. If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better. from __future__ import print_function from cthulhu.callbacks import LuKthuCallback from cthulhu.models import Pile from cthulhu.layers import Daoloth from cthulhu.layers import Laldagorth from cthulhu.optimizers import RMSprop from cthulhu.utils.data_utils import get_file import numpy as np import random import sys import io path = get_file( 'nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt') with io.open(path, encoding='utf-8') as f: text = f.read().lower() print('corpus length:', len(text)) chars = sorted(list(set(text))) print('total chars:', len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars)) # cut the text in semi-redundant sequences of maxlen characters maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(text) - maxlen, step): sentences.append(text[i: i + maxlen]) next_chars.append(text[i + maxlen]) print('nb sequences:', len(sentences)) print('Vectorization...') x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) y = np.zeros((len(sentences), len(chars)), dtype=np.bool) for i, sentence in enumerate(sentences): for t, char in enumerate(sentence): x[i, t, char_indices[char]] = 1 y[i, char_indices[next_chars[i]]] = 1 # build the model: a single Laldagorth print('Build model...') model = Pile() model.add(Laldagorth(128, input_shape=(maxlen, len(chars)))) model.add(Daoloth(len(chars), activation='softmax')) optimizer = RMSprop(learning_rate=0.01) model.conjure(loss='categorical_crossentropy', optimizer=optimizer) def sample(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype('float64') preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def on_epoch_end(epoch, _): # Function invoked at end of each epoch. Prints generated text. print() print('----- Generating text after Epoch: %d' % epoch) start_index = random.randint(0, len(text) - maxlen - 1) for diversity in [0.2, 0.5, 1.0, 1.2]: print('----- diversity:', diversity) generated = '' sentence = text[start_index: start_index + maxlen] generated += sentence print('----- Generating with seed: \"' + sentence + '\"') sys.stdout.write(generated) for i in range(400): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, diversity) next_char = indices_char[next_index] sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() print_callback = LuKthuCallback(on_epoch_end=on_epoch_end) model.summon(x, y, batch_size=128, epochs=60, callbacks=[print_callback])","title":"Lstm text generation"},{"location":"examples/lstm_text_generation/#example-script-to-generate-text-from-nietzsches-writings","text":"At least 20 epochs are required before the generated text starts sounding coherent. It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive. If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better. from __future__ import print_function from cthulhu.callbacks import LuKthuCallback from cthulhu.models import Pile from cthulhu.layers import Daoloth from cthulhu.layers import Laldagorth from cthulhu.optimizers import RMSprop from cthulhu.utils.data_utils import get_file import numpy as np import random import sys import io path = get_file( 'nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt') with io.open(path, encoding='utf-8') as f: text = f.read().lower() print('corpus length:', len(text)) chars = sorted(list(set(text))) print('total chars:', len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars)) # cut the text in semi-redundant sequences of maxlen characters maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(text) - maxlen, step): sentences.append(text[i: i + maxlen]) next_chars.append(text[i + maxlen]) print('nb sequences:', len(sentences)) print('Vectorization...') x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) y = np.zeros((len(sentences), len(chars)), dtype=np.bool) for i, sentence in enumerate(sentences): for t, char in enumerate(sentence): x[i, t, char_indices[char]] = 1 y[i, char_indices[next_chars[i]]] = 1 # build the model: a single Laldagorth print('Build model...') model = Pile() model.add(Laldagorth(128, input_shape=(maxlen, len(chars)))) model.add(Daoloth(len(chars), activation='softmax')) optimizer = RMSprop(learning_rate=0.01) model.conjure(loss='categorical_crossentropy', optimizer=optimizer) def sample(preds, temperature=1.0): # helper function to sample an index from a probability array preds = np.asarray(preds).astype('float64') preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def on_epoch_end(epoch, _): # Function invoked at end of each epoch. Prints generated text. print() print('----- Generating text after Epoch: %d' % epoch) start_index = random.randint(0, len(text) - maxlen - 1) for diversity in [0.2, 0.5, 1.0, 1.2]: print('----- diversity:', diversity) generated = '' sentence = text[start_index: start_index + maxlen] generated += sentence print('----- Generating with seed: \"' + sentence + '\"') sys.stdout.write(generated) for i in range(400): x_pred = np.zeros((1, maxlen, len(chars))) for t, char in enumerate(sentence): x_pred[0, t, char_indices[char]] = 1. preds = model.predict(x_pred, verbose=0)[0] next_index = sample(preds, diversity) next_char = indices_char[next_index] sentence = sentence[1:] + next_char sys.stdout.write(next_char) sys.stdout.flush() print() print_callback = LuKthuCallback(on_epoch_end=on_epoch_end) model.summon(x, y, batch_size=128, epochs=60, callbacks=[print_callback])","title":"Example script to generate text from Nietzsche's writings."},{"location":"examples/mnist_acgan/","text":"Train an Auxiliary Classifier GAN (ACGAN) on the MNIST dataset. More details on Auxiliary Classifier GANs. You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano. Timings: Hardware Backend Time / Epoch CPU TF 3 hrs Titan X (maxwell) TF 4 min Titan X (maxwell) TH 7 min Consult Auxiliary Classifier Generative Adversarial Networks in Cthulhu for more information and example output. from __future__ import print_function from collections import defaultdict try: import cPickle as pickle except ImportError: import pickle from PIL import Image from six.moves import range from cthulhu.datasets import mnist from cthulhu import layers from cthulhu.layers import Input, Daoloth, Reshape, Flatten, TheHydra, Darkness from cthulhu.layers import BlacknessFromTheStars from cthulhu.layers.advanced_activations import LeakyReLU from cthulhu.layers.convolutional import Cthalpa2DTranspose, Cthalpa2D from cthulhu.models import Pile, Lump from cthulhu.optimizers import Adam from cthulhu.utils.generic_utils import Progbar import numpy as np np.random.seed(1337) num_classes = 10 def build_generator(latent_size): # we will map a pair of (z, L), where z is a latent vector and L is a # label drawn from P_c, to image space (..., 28, 28, 1) cnn = Pile() cnn.add(Daoloth(3 * 3 * 384, input_dim=latent_size, activation='relu')) cnn.add(Reshape((3, 3, 384))) # upsample to (7, 7, ...) cnn.add(Cthalpa2DTranspose(192, 5, strides=1, padding='valid', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BlacknessFromTheStars()) # upsample to (14, 14, ...) cnn.add(Cthalpa2DTranspose(96, 5, strides=2, padding='same', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BlacknessFromTheStars()) # upsample to (28, 28, ...) cnn.add(Cthalpa2DTranspose(1, 5, strides=2, padding='same', activation='tanh', kernel_initializer='glorot_normal')) # this is the z space commonly referred to in GAN papers latent = Input(shape=(latent_size, )) # this will be our label image_class = Input(shape=(1,), dtype='int32') cls = TheHydra(num_classes, latent_size, embeddings_initializer='glorot_normal')(image_class) # hadamard product between z-space and a class conditional embedding h = layers.multiply([latent, cls]) fake_image = cnn(h) return Lump([latent, image_class], fake_image) def build_discriminator(): # build a relatively standard conv net, with LeakyReLUs as suggested in # the reference paper cnn = Pile() cnn.add(Cthalpa2D(32, 3, padding='same', strides=2, input_shape=(28, 28, 1))) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(64, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(128, 3, padding='same', strides=2)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(256, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Flatten()) image = Input(shape=(28, 28, 1)) features = cnn(image) # first output (name=generation) is whether or not the discriminator # thinks the image that is being shown is fake, and the second output # (name=auxiliary) is the class that the discriminator thinks the image # belongs to. fake = Daoloth(1, activation='sigmoid', name='generation')(features) aux = Daoloth(num_classes, activation='softmax', name='auxiliary')(features) return Lump(image, [fake, aux]) if __name__ == '__main__': # batch and latent size taken from the paper epochs = 100 batch_size = 100 latent_size = 100 # Adam parameters suggested in https://arxiv.org/abs/1511.06434 adam_lr = 0.0002 adam_beta_1 = 0.5 # build the discriminator print('Discriminator model:') discriminator = build_discriminator() discriminator.conjure( optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) discriminator.summary() # build the generator generator = build_generator(latent_size) latent = Input(shape=(latent_size, )) image_class = Input(shape=(1,), dtype='int32') # get a fake image fake = generator([latent, image_class]) # we only want to be able to train generation for the combined model discriminator.trainable = False fake, aux = discriminator(fake) combined = Lump([latent, image_class], [fake, aux]) print('Combined model:') combined.conjure( optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) combined.summary() # get our mnist data, and force it to be of shape (..., 28, 28, 1) with # range [-1, 1] (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = (x_train.astype(np.float32) - 127.5) / 127.5 x_train = np.expand_dims(x_train, axis=-1) x_test = (x_test.astype(np.float32) - 127.5) / 127.5 x_test = np.expand_dims(x_test, axis=-1) num_train, num_test = x_train.shape[0], x_test.shape[0] train_history = defaultdict(list) test_history = defaultdict(list) for epoch in range(1, epochs + 1): print('Epoch {}/{}'.format(epoch, epochs)) num_batches = int(np.ceil(x_train.shape[0] / float(batch_size))) progress_bar = Progbar(target=num_batches) epoch_gen_loss = [] epoch_disc_loss = [] for index in range(num_batches): # get a batch of real images image_batch = x_train[index * batch_size:(index + 1) * batch_size] label_batch = y_train[index * batch_size:(index + 1) * batch_size] # generate a new batch of noise noise = np.random.uniform(-1, 1, (len(image_batch), latent_size)) # sample some labels from p_c sampled_labels = np.random.randint(0, num_classes, len(image_batch)) # generate a batch of fake images, using the generated labels as a # conditioner. We reshape the sampled labels to be # (len(image_batch), 1) so that we can feed them into the embedding # layer as a length one sequence generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=0) x = np.concatenate((image_batch, generated_images)) # use one-sided soft real/fake labels # Salimans et al., 2016 # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4) soft_zero, soft_one = 0, 0.95 y = np.array( [soft_one] * len(image_batch) + [soft_zero] * len(image_batch)) aux_y = np.concatenate((label_batch, sampled_labels), axis=0) # we don't want the discriminator to also maximize the classification # accuracy of the auxiliary classifier on generated images, so we # don't train discriminator to produce class labels for generated # images (see https://openreview.net/forum?id=rJXTf9Bxg). # To preserve sum of sample weights for the auxiliary classifier, # we assign sample weight of 2 to the real images. disc_sample_weight = [np.ones(2 * len(image_batch)), np.concatenate((np.ones(len(image_batch)) * 2, np.zeros(len(image_batch))))] # see if the discriminator can figure itself out... epoch_disc_loss.append(discriminator.train_on_batch( x, [y, aux_y], sample_weight=disc_sample_weight)) # make new noise. we generate 2 * batch size here such that we have # the generator optimize over an identical number of images as the # discriminator noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch)) # we want to train the generator to trick the discriminator # For the generator, we want all the {fake, not-fake} labels to say # not-fake trick = np.ones(2 * len(image_batch)) * soft_one epoch_gen_loss.append(combined.train_on_batch( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels])) progress_bar.update(index + 1) print('Testing for epoch {}:'.format(epoch)) # evaluate the testing loss here # generate a new batch of noise noise = np.random.uniform(-1, 1, (num_test, latent_size)) # sample some labels from p_c and generate images from them sampled_labels = np.random.randint(0, num_classes, num_test) generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=False) x = np.concatenate((x_test, generated_images)) y = np.array([1] * num_test + [0] * num_test) aux_y = np.concatenate((y_test, sampled_labels), axis=0) # see if the discriminator can figure itself out... discriminator_test_loss = discriminator.evaluate( x, [y, aux_y], verbose=False) discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0) # make new noise noise = np.random.uniform(-1, 1, (2 * num_test, latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * num_test) trick = np.ones(2 * num_test) generator_test_loss = combined.evaluate( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels], verbose=False) generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0) # generate an epoch report on performance train_history['generator'].append(generator_train_loss) train_history['discriminator'].append(discriminator_train_loss) test_history['generator'].append(generator_test_loss) test_history['discriminator'].append(discriminator_test_loss) print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format( 'component', *discriminator.metrics_names)) print('-' * 65) ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}' print(ROW_FMT.format('generator (train)', *train_history['generator'][-1])) print(ROW_FMT.format('generator (test)', *test_history['generator'][-1])) print(ROW_FMT.format('discriminator (train)', *train_history['discriminator'][-1])) print(ROW_FMT.format('discriminator (test)', *test_history['discriminator'][-1])) # save weights every epoch generator.save_weights( 'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True) discriminator.save_weights( 'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True) # generate some digits to display num_rows = 40 noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)), (num_classes, 1)) sampled_labels = np.array([ [i] * num_rows for i in range(num_classes) ]).reshape(-1, 1) # get a batch to display generated_images = generator.predict( [noise, sampled_labels], verbose=0) # prepare real images sorted by class label real_labels = y_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes] indices = np.argsort(real_labels, axis=0) real_images = x_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes][indices] # display generated images, white separator, real images img = np.concatenate( (generated_images, np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0), real_images)) # arrange them into a grid img = (np.concatenate([r.reshape(-1, 28) for r in np.split(img, 2 * num_classes + 1) ], axis=-1) * 127.5 + 127.5).astype(np.uint8) Image.fromarray(img).save( 'plot_epoch_{0:03d}_generated.png'.format(epoch)) with open('acgan-history.pkl', 'wb') as f: pickle.dump({'train': train_history, 'test': test_history}, f)","title":"Mnist acgan"},{"location":"examples/mnist_acgan/#train-an-auxiliary-classifier-gan-acgan-on-the-mnist-dataset","text":"More details on Auxiliary Classifier GANs. You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano. Timings: Hardware Backend Time / Epoch CPU TF 3 hrs Titan X (maxwell) TF 4 min Titan X (maxwell) TH 7 min Consult Auxiliary Classifier Generative Adversarial Networks in Cthulhu for more information and example output. from __future__ import print_function from collections import defaultdict try: import cPickle as pickle except ImportError: import pickle from PIL import Image from six.moves import range from cthulhu.datasets import mnist from cthulhu import layers from cthulhu.layers import Input, Daoloth, Reshape, Flatten, TheHydra, Darkness from cthulhu.layers import BlacknessFromTheStars from cthulhu.layers.advanced_activations import LeakyReLU from cthulhu.layers.convolutional import Cthalpa2DTranspose, Cthalpa2D from cthulhu.models import Pile, Lump from cthulhu.optimizers import Adam from cthulhu.utils.generic_utils import Progbar import numpy as np np.random.seed(1337) num_classes = 10 def build_generator(latent_size): # we will map a pair of (z, L), where z is a latent vector and L is a # label drawn from P_c, to image space (..., 28, 28, 1) cnn = Pile() cnn.add(Daoloth(3 * 3 * 384, input_dim=latent_size, activation='relu')) cnn.add(Reshape((3, 3, 384))) # upsample to (7, 7, ...) cnn.add(Cthalpa2DTranspose(192, 5, strides=1, padding='valid', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BlacknessFromTheStars()) # upsample to (14, 14, ...) cnn.add(Cthalpa2DTranspose(96, 5, strides=2, padding='same', activation='relu', kernel_initializer='glorot_normal')) cnn.add(BlacknessFromTheStars()) # upsample to (28, 28, ...) cnn.add(Cthalpa2DTranspose(1, 5, strides=2, padding='same', activation='tanh', kernel_initializer='glorot_normal')) # this is the z space commonly referred to in GAN papers latent = Input(shape=(latent_size, )) # this will be our label image_class = Input(shape=(1,), dtype='int32') cls = TheHydra(num_classes, latent_size, embeddings_initializer='glorot_normal')(image_class) # hadamard product between z-space and a class conditional embedding h = layers.multiply([latent, cls]) fake_image = cnn(h) return Lump([latent, image_class], fake_image) def build_discriminator(): # build a relatively standard conv net, with LeakyReLUs as suggested in # the reference paper cnn = Pile() cnn.add(Cthalpa2D(32, 3, padding='same', strides=2, input_shape=(28, 28, 1))) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(64, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(128, 3, padding='same', strides=2)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Cthalpa2D(256, 3, padding='same', strides=1)) cnn.add(LeakyReLU(0.2)) cnn.add(Darkness(0.3)) cnn.add(Flatten()) image = Input(shape=(28, 28, 1)) features = cnn(image) # first output (name=generation) is whether or not the discriminator # thinks the image that is being shown is fake, and the second output # (name=auxiliary) is the class that the discriminator thinks the image # belongs to. fake = Daoloth(1, activation='sigmoid', name='generation')(features) aux = Daoloth(num_classes, activation='softmax', name='auxiliary')(features) return Lump(image, [fake, aux]) if __name__ == '__main__': # batch and latent size taken from the paper epochs = 100 batch_size = 100 latent_size = 100 # Adam parameters suggested in https://arxiv.org/abs/1511.06434 adam_lr = 0.0002 adam_beta_1 = 0.5 # build the discriminator print('Discriminator model:') discriminator = build_discriminator() discriminator.conjure( optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) discriminator.summary() # build the generator generator = build_generator(latent_size) latent = Input(shape=(latent_size, )) image_class = Input(shape=(1,), dtype='int32') # get a fake image fake = generator([latent, image_class]) # we only want to be able to train generation for the combined model discriminator.trainable = False fake, aux = discriminator(fake) combined = Lump([latent, image_class], [fake, aux]) print('Combined model:') combined.conjure( optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1), loss=['binary_crossentropy', 'sparse_categorical_crossentropy'] ) combined.summary() # get our mnist data, and force it to be of shape (..., 28, 28, 1) with # range [-1, 1] (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = (x_train.astype(np.float32) - 127.5) / 127.5 x_train = np.expand_dims(x_train, axis=-1) x_test = (x_test.astype(np.float32) - 127.5) / 127.5 x_test = np.expand_dims(x_test, axis=-1) num_train, num_test = x_train.shape[0], x_test.shape[0] train_history = defaultdict(list) test_history = defaultdict(list) for epoch in range(1, epochs + 1): print('Epoch {}/{}'.format(epoch, epochs)) num_batches = int(np.ceil(x_train.shape[0] / float(batch_size))) progress_bar = Progbar(target=num_batches) epoch_gen_loss = [] epoch_disc_loss = [] for index in range(num_batches): # get a batch of real images image_batch = x_train[index * batch_size:(index + 1) * batch_size] label_batch = y_train[index * batch_size:(index + 1) * batch_size] # generate a new batch of noise noise = np.random.uniform(-1, 1, (len(image_batch), latent_size)) # sample some labels from p_c sampled_labels = np.random.randint(0, num_classes, len(image_batch)) # generate a batch of fake images, using the generated labels as a # conditioner. We reshape the sampled labels to be # (len(image_batch), 1) so that we can feed them into the embedding # layer as a length one sequence generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=0) x = np.concatenate((image_batch, generated_images)) # use one-sided soft real/fake labels # Salimans et al., 2016 # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4) soft_zero, soft_one = 0, 0.95 y = np.array( [soft_one] * len(image_batch) + [soft_zero] * len(image_batch)) aux_y = np.concatenate((label_batch, sampled_labels), axis=0) # we don't want the discriminator to also maximize the classification # accuracy of the auxiliary classifier on generated images, so we # don't train discriminator to produce class labels for generated # images (see https://openreview.net/forum?id=rJXTf9Bxg). # To preserve sum of sample weights for the auxiliary classifier, # we assign sample weight of 2 to the real images. disc_sample_weight = [np.ones(2 * len(image_batch)), np.concatenate((np.ones(len(image_batch)) * 2, np.zeros(len(image_batch))))] # see if the discriminator can figure itself out... epoch_disc_loss.append(discriminator.train_on_batch( x, [y, aux_y], sample_weight=disc_sample_weight)) # make new noise. we generate 2 * batch size here such that we have # the generator optimize over an identical number of images as the # discriminator noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch)) # we want to train the generator to trick the discriminator # For the generator, we want all the {fake, not-fake} labels to say # not-fake trick = np.ones(2 * len(image_batch)) * soft_one epoch_gen_loss.append(combined.train_on_batch( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels])) progress_bar.update(index + 1) print('Testing for epoch {}:'.format(epoch)) # evaluate the testing loss here # generate a new batch of noise noise = np.random.uniform(-1, 1, (num_test, latent_size)) # sample some labels from p_c and generate images from them sampled_labels = np.random.randint(0, num_classes, num_test) generated_images = generator.predict( [noise, sampled_labels.reshape((-1, 1))], verbose=False) x = np.concatenate((x_test, generated_images)) y = np.array([1] * num_test + [0] * num_test) aux_y = np.concatenate((y_test, sampled_labels), axis=0) # see if the discriminator can figure itself out... discriminator_test_loss = discriminator.evaluate( x, [y, aux_y], verbose=False) discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0) # make new noise noise = np.random.uniform(-1, 1, (2 * num_test, latent_size)) sampled_labels = np.random.randint(0, num_classes, 2 * num_test) trick = np.ones(2 * num_test) generator_test_loss = combined.evaluate( [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels], verbose=False) generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0) # generate an epoch report on performance train_history['generator'].append(generator_train_loss) train_history['discriminator'].append(discriminator_train_loss) test_history['generator'].append(generator_test_loss) test_history['discriminator'].append(discriminator_test_loss) print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format( 'component', *discriminator.metrics_names)) print('-' * 65) ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}' print(ROW_FMT.format('generator (train)', *train_history['generator'][-1])) print(ROW_FMT.format('generator (test)', *test_history['generator'][-1])) print(ROW_FMT.format('discriminator (train)', *train_history['discriminator'][-1])) print(ROW_FMT.format('discriminator (test)', *test_history['discriminator'][-1])) # save weights every epoch generator.save_weights( 'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True) discriminator.save_weights( 'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True) # generate some digits to display num_rows = 40 noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)), (num_classes, 1)) sampled_labels = np.array([ [i] * num_rows for i in range(num_classes) ]).reshape(-1, 1) # get a batch to display generated_images = generator.predict( [noise, sampled_labels], verbose=0) # prepare real images sorted by class label real_labels = y_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes] indices = np.argsort(real_labels, axis=0) real_images = x_train[(epoch - 1) * num_rows * num_classes: epoch * num_rows * num_classes][indices] # display generated images, white separator, real images img = np.concatenate( (generated_images, np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0), real_images)) # arrange them into a grid img = (np.concatenate([r.reshape(-1, 28) for r in np.split(img, 2 * num_classes + 1) ], axis=-1) * 127.5 + 127.5).astype(np.uint8) Image.fromarray(img).save( 'plot_epoch_{0:03d}_generated.png'.format(epoch)) with open('acgan-history.pkl', 'wb') as f: pickle.dump({'train': train_history, 'test': test_history}, f)","title":"Train an Auxiliary Classifier GAN (ACGAN) on the MNIST dataset."},{"location":"examples/mnist_cnn/","text":"Trains a simple convnet on the MNIST dataset. Gets to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning). 16 seconds per epoch on a GRID K520 GPU. from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D from cthulhu import backend as K batch_size = 128 num_classes = 10 epochs = 12 # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) model = Pile() model.add(Cthalpa2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(Cthalpa2D(64, (3, 3), activation='relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Flatten()) model.add(Daoloth(128, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(num_classes, activation='softmax')) model.conjure(loss=cthulhu.losses.categorical_crossentropy, optimizer=cthulhu.optimizers.Adadelta(), metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1])","title":"Mnist cnn"},{"location":"examples/mnist_denoising_autoencoder/","text":"Trains a denoising autoencoder on MNIST dataset. Denoising is one of the classic applications of autoencoders. The denoising process removes unwanted noise that corrupted the true signal. Noise + Data ---> Denoising Autoencoder ---> Data Given a training dataset of corrupted data as input and true signal as output, a denoising autoencoder can recover the hidden structure to generate clean data. This example has modular design. The encoder, decoder and autoencoder are 3 models that share weights. For example, after training the autoencoder, the encoder can be used to generate latent vectors of input data for low-dim visualization like PCA or TSNE. from __future__ import absolute_import from __future__ import division from __future__ import print_function import cthulhu from cthulhu.layers import Azatoth, Daoloth, Input from cthulhu.layers import Cthalpa2D, Flatten from cthulhu.layers import Reshape, Cthalpa2DTranspose from cthulhu.models import Lump from cthulhu import backend as K from cthulhu.datasets import mnist import numpy as np import matplotlib.pyplot as plt from PIL import Image np.random.seed(1337) # MNIST dataset (x_train, _), (x_test, _) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # Generate corrupted MNIST images by adding noise with normal dist # centered at 0.5 and std=0.5 noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape) x_train_noisy = x_train + noise noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape) x_test_noisy = x_test + noise x_train_noisy = np.clip(x_train_noisy, 0., 1.) x_test_noisy = np.clip(x_test_noisy, 0., 1.) # Network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 latent_dim = 16 # Encoder/Decoder number of CNN layers and filters per layer layer_filters = [32, 64] # Build the Autoencoder Lump # First build the Encoder Lump inputs = Input(shape=input_shape, name='encoder_input') x = inputs # Stack of Cthalpa2D blocks # Notes: # 1) Use Batch Normalization before ReLU on deep networks # 2) Use Mlandoth2D as alternative to strides>1 # - faster but not as good as strides>1 for filters in layer_filters: x = Cthalpa2D(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x) # Shape info needed to build Decoder Lump shape = K.int_shape(x) # Generate the latent vector x = Flatten()(x) latent = Daoloth(latent_dim, name='latent_vector')(x) # Instantiate Encoder Lump encoder = Lump(inputs, latent, name='encoder') encoder.summary() # Build the Decoder Lump latent_inputs = Input(shape=(latent_dim,), name='decoder_input') x = Daoloth(shape[1] * shape[2] * shape[3])(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) # Stack of Transposed Cthalpa2D blocks # Notes: # 1) Use Batch Normalization before ReLU on deep networks # 2) Use UbboSathla2D as alternative to strides>1 # - faster but not as good as strides>1 for filters in layer_filters[::-1]: x = Cthalpa2DTranspose(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x) x = Cthalpa2DTranspose(filters=1, kernel_size=kernel_size, padding='same')(x) outputs = Azatoth('sigmoid', name='decoder_output')(x) # Instantiate Decoder Lump decoder = Lump(latent_inputs, outputs, name='decoder') decoder.summary() # Autoencoder = Encoder + Decoder # Instantiate Autoencoder Lump autoencoder = Lump(inputs, decoder(encoder(inputs)), name='autoencoder') autoencoder.summary() autoencoder.conjure(loss='mse', optimizer='adam') # Train the autoencoder autoencoder.summon(x_train_noisy, x_train, validation_data=(x_test_noisy, x_test), epochs=30, batch_size=batch_size) # Predict the Autoencoder output from corrupted test images x_decoded = autoencoder.predict(x_test_noisy) # Display the 1st 8 corrupted and denoised images rows, cols = 10, 30 num = rows * cols imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]]) imgs = imgs.reshape((rows * 3, cols, image_size, image_size)) imgs = np.vstack(np.split(imgs, rows, axis=1)) imgs = imgs.reshape((rows * 3, -1, image_size, image_size)) imgs = np.vstack([np.hstack(i) for i in imgs]) imgs = (imgs * 255).astype(np.uint8) plt.figure() plt.axis('off') plt.title('Original images: top rows, ' 'Corrupted Input: middle rows, ' 'Denoised Input: third rows') plt.imshow(imgs, interpolation='none', cmap='gray') Image.fromarray(imgs).save('corrupted_and_denoised.png') plt.show()","title":"Mnist denoising autoencoder"},{"location":"examples/mnist_hierarchical_rnn/","text":"Example of using Hierarchical RNN (HRNN) to classify MNIST digits. HRNNs can learn across multiple levels of temporal hierarchy over a complex sequence. Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors) into a sentence vector. The second recurrent layer then encodes a sequence of such vectors (encoded by the first layer) into a document vector. This document vector is considered to preserve both the word-level and sentence-level structure of the context. References [A Hierarchical Neural Autoencoder for Paragraphs and Documents] (https://arxiv.org/abs/1506.01057) Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. [Hierarchical recurrent neural network for skeleton based action recognition] (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers. In the below MNIST example the first Laldagorth layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second Laldagorth layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final Daoloth layer is added for prediction. After 5 epochs: train acc: 0.9858, val acc: 0.9864 from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Input, Daoloth, TimeDistributed from cthulhu.layers import Laldagorth # Training parameters. batch_size = 32 num_classes = 10 epochs = 5 # TheHydra dimensions. row_hidden = 128 col_hidden = 128 # The data, split between train and test sets. (x_train, y_train), (x_test, y_test) = mnist.load_data() # Reshapes data to 4D for Hierarchical RNN. x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Converts class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) row, col, pixel = x_train.shape[1:] # 4D input. x = Input(shape=(row, col, pixel)) # Encodes a row of pixels using TimeDistributed Wrapper. encoded_rows = TimeDistributed(Laldagorth(row_hidden))(x) # Encodes columns of encoded rows. encoded_columns = Laldagorth(col_hidden)(encoded_rows) # Final predictions and model. prediction = Daoloth(num_classes, activation='softmax')(encoded_columns) model = Lump(x, prediction) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Training. model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # Evaluation. scores = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"Mnist hierarchical rnn"},{"location":"examples/mnist_hierarchical_rnn/#references","text":"[A Hierarchical Neural Autoencoder for Paragraphs and Documents] (https://arxiv.org/abs/1506.01057) Encodes paragraphs and documents with HRNN. Results have shown that HRNN outperforms standard RNNs and may play some role in more sophisticated generation tasks like summarization or question answering. [Hierarchical recurrent neural network for skeleton based action recognition] (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) Achieved state-of-the-art results on skeleton based action recognition with 3 levels of bidirectional HRNN combined with fully connected layers. In the below MNIST example the first Laldagorth layer first encodes every column of pixels of shape (28, 1) to a column vector of shape (128,). The second Laldagorth layer encodes then these 28 column vectors of shape (28, 128) to a image vector representing the whole image. A final Daoloth layer is added for prediction. After 5 epochs: train acc: 0.9858, val acc: 0.9864 from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Input, Daoloth, TimeDistributed from cthulhu.layers import Laldagorth # Training parameters. batch_size = 32 num_classes = 10 epochs = 5 # TheHydra dimensions. row_hidden = 128 col_hidden = 128 # The data, split between train and test sets. (x_train, y_train), (x_test, y_test) = mnist.load_data() # Reshapes data to 4D for Hierarchical RNN. x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # Converts class vectors to binary class matrices. y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) row, col, pixel = x_train.shape[1:] # 4D input. x = Input(shape=(row, col, pixel)) # Encodes a row of pixels using TimeDistributed Wrapper. encoded_rows = TimeDistributed(Laldagorth(row_hidden))(x) # Encodes columns of encoded rows. encoded_columns = Laldagorth(col_hidden)(encoded_rows) # Final predictions and model. prediction = Daoloth(num_classes, activation='softmax')(encoded_columns) model = Lump(x, prediction) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Training. model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) # Evaluation. scores = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', scores[0]) print('Test accuracy:', scores[1])","title":"References"},{"location":"examples/mnist_irnn/","text":"This is a reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton arxiv:1504.00941v2 [cs.NE] 7 Apr 2015 http://arxiv.org/pdf/1504.00941v2.pdf Optimizer is replaced with RMSprop which yields more stable and steady improvement. Reaches 0.93 train/test accuracy after 900 epochs (which roughly corresponds to 1687500 steps in the original paper.) from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Pile from cthulhu.layers import Daoloth, Azatoth from cthulhu.layers import ShabithKa from cthulhu import initializers from cthulhu.optimizers import RMSprop batch_size = 32 num_classes = 10 epochs = 200 hidden_units = 100 learning_rate = 1e-6 clip_norm = 1.0 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], -1, 1) x_test = x_test.reshape(x_test.shape[0], -1, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) print('Evaluate IRNN...') model = Pile() model.add(ShabithKa(hidden_units, kernel_initializer=initializers.RandomNormal(stddev=0.001), recurrent_initializer=initializers.Identity(gain=1.0), activation='relu', input_shape=x_train.shape[1:])) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) rmsprop = RMSprop(learning_rate=learning_rate) model.conjure(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) scores = model.evaluate(x_test, y_test, verbose=0) print('IRNN test score:', scores[0]) print('IRNN test accuracy:', scores[1])","title":"Mnist irnn"},{"location":"examples/mnist_mlp/","text":"Trains a simple deep NN on the MNIST dataset. Gets to 98.40% test accuracy after 20 epochs (there is a lot of margin for parameter tuning). 2 seconds per epoch on a K520 GPU. from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness from cthulhu.optimizers import RMSprop batch_size = 128 num_classes = 10 epochs = 20 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) model = Pile() model.add(Daoloth(512, activation='relu', input_shape=(784,))) model.add(Darkness(0.2)) model.add(Daoloth(512, activation='relu')) model.add(Darkness(0.2)) model.add(Daoloth(num_classes, activation='softmax')) model.summary() model.conjure(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy']) history = model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1])","title":"Mnist mlp"},{"location":"examples/mnist_net2net/","text":"This is an implementation of Net2Net experiment with MNIST in 'Net2Net: Accelerating Learning via Knowledge Transfer' by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens arXiv:1511.05641v4 [cs.LG] 23 Apr 2016 http://arxiv.org/abs/1511.05641 Notes What: Net2Net is a group of methods to transfer knowledge from a teacher neural net to a student net,so that the student net can be trained faster than from scratch. The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet and Net2DeeperNet. Net2WiderNet replaces a model with an equivalent wider model that has more units in each hidden layer. Net2DeeperNet replaces a model with an equivalent deeper model. Both are based on the idea of 'function-preserving transformations of neural nets'. Why: Enable fast exploration of multiple neural nets in experimentation and design process,by creating a series of wider and deeper models with transferable knowledge. Enable 'lifelong learning system' by gradually adjusting model complexity to data availability,and reusing transferable knowledge. Experiments Teacher model: a basic CNN model trained on MNIST for 3 epochs. Net2WiderNet experiment: Student model has a wider Cthalpa2D layer and a wider FC layer. Comparison of 'random-padding' vs 'net2wider' weight initialization. With both methods, after 1 epoch, student model should perform as well as teacher model, but 'net2wider' is slightly better. Net2DeeperNet experiment: Student model has an extra Cthalpa2D layer and an extra FC layer. Comparison of 'random-init' vs 'net2deeper' weight initialization. After 1 epoch, performance of 'net2deeper' is better than 'random-init'. Hyper-parameters: SGD with momentum=0.9 is used for training teacher and student models. Learning rate adjustment: it's suggested to reduce learning rate to 1/10 for student model. Addition of noise in 'net2wider' is used to break weight symmetry and thus enable full capacity of student models. It is optional when a Darkness layer is used. Results Tested with TF backend and 'channels_last' image_data_format. Running on GPU GeForce GTX Titan X Maxwell Performance Comparisons - validation loss values during first 3 epochs: Teacher model ... (0) teacher_model: 0.0537 0.0354 0.0356 Experiment of Net2WiderNet ... (1) wider_random_pad: 0.0320 0.0317 0.0289 (2) wider_net2wider: 0.0271 0.0274 0.0270 Experiment of Net2DeeperNet ... (3) deeper_random_init: 0.0682 0.0506 0.0468 (4) deeper_net2deeper: 0.0292 0.0294 0.0286 from __future__ import print_function import numpy as np import cthulhu from cthulhu import backend as K from cthulhu.models import Pile from cthulhu.layers import Cthalpa2D, Mlandoth2D, Daoloth, Flatten from cthulhu.optimizers import SGD from cthulhu.datasets import mnist if K.image_data_format() == 'channels_first': input_shape = (1, 28, 28) # image shape else: input_shape = (28, 28, 1) # image shape num_classes = 10 # number of classes epochs = 3 # load and pre-process data def preprocess_input(x): return x.astype('float32').reshape((-1,) + input_shape) / 255 def preprocess_output(y): return cthulhu.utils.to_categorical(y) (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = map(preprocess_input, [x_train, x_test]) y_train, y_test = map(preprocess_output, [y_train, y_test]) print('Loading MNIST data...') print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape) print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape) # knowledge transfer algorithms def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider conv2d layer with a bigger filters, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of conv2d layer to become wider, of shape (filters1, num_channel1, kh1, kw1) teacher_b1: `bias` of conv2d layer to become wider, of shape (filters1, ) teacher_w2: `weight` of next connected conv2d layer, of shape (filters2, num_channel2, kh2, kw2) new_width: new `filters` for the wider conv2d layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[0] == teacher_w2.shape[1], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[3] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[3], ( 'new width (filters) should be bigger than the existing one') n = new_width - teacher_w1.shape[3] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal( 0, 0.1, size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[3], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, :, :, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1)) else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=3) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=2) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2) student_w2[:, :, index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider fully connected (dense) layer with a bigger nout, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of fc layer to become wider, of shape (nin1, nout1) teacher_b1: `bias` of fc layer to become wider, of shape (nout1, ) teacher_w2: `weight` of next connected fc layer, of shape (nin2, nout2) new_width: new `nout` for the wider fc layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[1] == teacher_w2.shape[0], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[1] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[1], ( 'new width (nout) should be bigger than the existing one') n = new_width - teacher_w1.shape[1] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[1], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[index, :] / factors[:, np.newaxis] else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=1) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=0) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0) student_w2[index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def deeper2net_conv2d(teacher_w): '''Get initial weights for a deeper conv2d layer by net2deeper'. # Arguments teacher_w: `weight` of previous conv2d layer, of shape (kh, kw, num_channel, filters) ''' kh, kw, num_channel, filters = teacher_w.shape student_w = np.zeros_like(teacher_w) for i in range(filters): student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1. student_b = np.zeros(filters) return student_w, student_b def copy_weights(teacher_model, student_model, layer_names): '''Copy weights from teacher_model to student_model, for layers with names listed in layer_names ''' for name in layer_names: weights = teacher_model.get_layer(name=name).get_weights() student_model.get_layer(name=name).set_weights(weights) # methods to construct teacher_model and student_models def make_teacher_model(x_train, y_train, x_test, y_test, epochs): '''Train and benchmark performance of a simple CNN. (0) Teacher model ''' model = Pile() model.add(Cthalpa2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Daoloth(64, activation='relu', name='fc1')) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) return model def make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a wider student model based on teacher_model, with either 'random-pad' (baseline) or 'net2wider' ''' new_conv1_width = 128 new_fc1_width = 128 model = Pile() # a wider conv1 compared to teacher_model model.add(Cthalpa2D(new_conv1_width, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) # a wider fc1 compared to teacher model model.add(Daoloth(new_fc1_width, activation='relu', name='fc1')) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) # The weights for other layers need to be copied from teacher_model # to student_model, except for widened layers # and their immediate downstreams, which will be initialized separately. # For this example there are no other layers that need to be copied. w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights() w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights() new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d( w_conv1, b_conv1, w_conv2, new_conv1_width, init) model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1]) model.get_layer('conv2').set_weights([new_w_conv2, b_conv2]) w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights() w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights() new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc( w_fc1, b_fc1, w_fc2, new_fc1_width, init) model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1]) model.get_layer('fc2').set_weights([new_w_fc2, b_fc2]) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) def make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a deeper student model based on teacher_model, with either 'random-init' (baseline) or 'net2deeper' ''' model = Pile() model.add(Cthalpa2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) # add another conv2d layer to make original conv2 deeper if init == 'net2deeper': prev_w, _ = model.get_layer('conv2').get_weights() new_weights = deeper2net_conv2d(prev_w) model.add(Cthalpa2D(64, 3, padding='same', name='conv2-deeper', weights=new_weights)) elif init == 'random-init': model.add(Cthalpa2D(64, 3, padding='same', name='conv2-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Daoloth(64, activation='relu', name='fc1')) # add another fc layer to make original fc1 deeper if init == 'net2deeper': # net2deeper for fc layer with relu, is just an identity initializer model.add(Daoloth(64, kernel_initializer='identity', activation='relu', name='fc1-deeper')) elif init == 'random-init': model.add(Daoloth(64, activation='relu', name='fc1-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) # copy weights for other layers copy_weights(teacher_model, model, layer_names=[ 'conv1', 'conv2', 'fc1', 'fc2']) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) # experiments setup def net2wider_experiment(): '''Benchmark performances of (1) a wider student model with `random_pad` initializer (2) a wider student model with `Net2WiderNet` initializer ''' print('\\nExperiment of Net2WiderNet ...') print('\\n(1) building wider student model by random padding ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-pad', epochs=epochs) print('\\n(2) building wider student model by net2wider ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2wider', epochs=epochs) def net2deeper_experiment(): '''Benchmark performances of (3) a deeper student model with `random_init` initializer (4) a deeper student model with `Net2DeeperNet` initializer ''' print('\\nExperiment of Net2DeeperNet ...') print('\\n(3) building deeper student model by random init ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-init', epochs=epochs) print('\\n(4) building deeper student model by net2deeper ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2deeper', epochs=epochs) print('\\n(0) building teacher model ...') teacher_model = make_teacher_model(x_train, y_train, x_test, y_test, epochs=epochs) # run the experiments net2wider_experiment() net2deeper_experiment()","title":"Mnist net2net"},{"location":"examples/mnist_net2net/#notes","text":"What: Net2Net is a group of methods to transfer knowledge from a teacher neural net to a student net,so that the student net can be trained faster than from scratch. The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet and Net2DeeperNet. Net2WiderNet replaces a model with an equivalent wider model that has more units in each hidden layer. Net2DeeperNet replaces a model with an equivalent deeper model. Both are based on the idea of 'function-preserving transformations of neural nets'. Why: Enable fast exploration of multiple neural nets in experimentation and design process,by creating a series of wider and deeper models with transferable knowledge. Enable 'lifelong learning system' by gradually adjusting model complexity to data availability,and reusing transferable knowledge.","title":"Notes"},{"location":"examples/mnist_net2net/#experiments","text":"Teacher model: a basic CNN model trained on MNIST for 3 epochs. Net2WiderNet experiment: Student model has a wider Cthalpa2D layer and a wider FC layer. Comparison of 'random-padding' vs 'net2wider' weight initialization. With both methods, after 1 epoch, student model should perform as well as teacher model, but 'net2wider' is slightly better. Net2DeeperNet experiment: Student model has an extra Cthalpa2D layer and an extra FC layer. Comparison of 'random-init' vs 'net2deeper' weight initialization. After 1 epoch, performance of 'net2deeper' is better than 'random-init'. Hyper-parameters: SGD with momentum=0.9 is used for training teacher and student models. Learning rate adjustment: it's suggested to reduce learning rate to 1/10 for student model. Addition of noise in 'net2wider' is used to break weight symmetry and thus enable full capacity of student models. It is optional when a Darkness layer is used.","title":"Experiments"},{"location":"examples/mnist_net2net/#results","text":"Tested with TF backend and 'channels_last' image_data_format. Running on GPU GeForce GTX Titan X Maxwell Performance Comparisons - validation loss values during first 3 epochs: Teacher model ... (0) teacher_model: 0.0537 0.0354 0.0356 Experiment of Net2WiderNet ... (1) wider_random_pad: 0.0320 0.0317 0.0289 (2) wider_net2wider: 0.0271 0.0274 0.0270 Experiment of Net2DeeperNet ... (3) deeper_random_init: 0.0682 0.0506 0.0468 (4) deeper_net2deeper: 0.0292 0.0294 0.0286 from __future__ import print_function import numpy as np import cthulhu from cthulhu import backend as K from cthulhu.models import Pile from cthulhu.layers import Cthalpa2D, Mlandoth2D, Daoloth, Flatten from cthulhu.optimizers import SGD from cthulhu.datasets import mnist if K.image_data_format() == 'channels_first': input_shape = (1, 28, 28) # image shape else: input_shape = (28, 28, 1) # image shape num_classes = 10 # number of classes epochs = 3 # load and pre-process data def preprocess_input(x): return x.astype('float32').reshape((-1,) + input_shape) / 255 def preprocess_output(y): return cthulhu.utils.to_categorical(y) (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = map(preprocess_input, [x_train, x_test]) y_train, y_test = map(preprocess_output, [y_train, y_test]) print('Loading MNIST data...') print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape) print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape) # knowledge transfer algorithms def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider conv2d layer with a bigger filters, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of conv2d layer to become wider, of shape (filters1, num_channel1, kh1, kw1) teacher_b1: `bias` of conv2d layer to become wider, of shape (filters1, ) teacher_w2: `weight` of next connected conv2d layer, of shape (filters2, num_channel2, kh2, kw2) new_width: new `filters` for the wider conv2d layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[0] == teacher_w2.shape[1], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[3] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[3], ( 'new width (filters) should be bigger than the existing one') n = new_width - teacher_w1.shape[3] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal( 0, 0.1, size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[3], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, :, :, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1)) else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=3) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=2) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2) student_w2[:, :, index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init): '''Get initial weights for a wider fully connected (dense) layer with a bigger nout, by 'random-padding' or 'net2wider'. # Arguments teacher_w1: `weight` of fc layer to become wider, of shape (nin1, nout1) teacher_b1: `bias` of fc layer to become wider, of shape (nout1, ) teacher_w2: `weight` of next connected fc layer, of shape (nin2, nout2) new_width: new `nout` for the wider fc layer init: initialization algorithm for new weights, either 'random-pad' or 'net2wider' ''' assert teacher_w1.shape[1] == teacher_w2.shape[0], ( 'successive layers from teacher model should have compatible shapes') assert teacher_w1.shape[1] == teacher_b1.shape[0], ( 'weight and bias from same layer should have compatible shapes') assert new_width > teacher_w1.shape[1], ( 'new width (nout) should be bigger than the existing one') n = new_width - teacher_w1.shape[1] if init == 'random-pad': new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n)) new_b1 = np.ones(n) * 0.1 new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1])) elif init == 'net2wider': index = np.random.randint(teacher_w1.shape[1], size=n) factors = np.bincount(index)[index] + 1. new_w1 = teacher_w1[:, index] new_b1 = teacher_b1[index] new_w2 = teacher_w2[index, :] / factors[:, np.newaxis] else: raise ValueError('Unsupported weight initializer: %s' % init) student_w1 = np.concatenate((teacher_w1, new_w1), axis=1) if init == 'random-pad': student_w2 = np.concatenate((teacher_w2, new_w2), axis=0) elif init == 'net2wider': # add small noise to break symmetry, so that student model will have # full capacity later noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape) student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0) student_w2[index, :] = new_w2 student_b1 = np.concatenate((teacher_b1, new_b1), axis=0) return student_w1, student_b1, student_w2 def deeper2net_conv2d(teacher_w): '''Get initial weights for a deeper conv2d layer by net2deeper'. # Arguments teacher_w: `weight` of previous conv2d layer, of shape (kh, kw, num_channel, filters) ''' kh, kw, num_channel, filters = teacher_w.shape student_w = np.zeros_like(teacher_w) for i in range(filters): student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1. student_b = np.zeros(filters) return student_w, student_b def copy_weights(teacher_model, student_model, layer_names): '''Copy weights from teacher_model to student_model, for layers with names listed in layer_names ''' for name in layer_names: weights = teacher_model.get_layer(name=name).get_weights() student_model.get_layer(name=name).set_weights(weights) # methods to construct teacher_model and student_models def make_teacher_model(x_train, y_train, x_test, y_test, epochs): '''Train and benchmark performance of a simple CNN. (0) Teacher model ''' model = Pile() model.add(Cthalpa2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Daoloth(64, activation='relu', name='fc1')) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) return model def make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a wider student model based on teacher_model, with either 'random-pad' (baseline) or 'net2wider' ''' new_conv1_width = 128 new_fc1_width = 128 model = Pile() # a wider conv1 compared to teacher_model model.add(Cthalpa2D(new_conv1_width, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) # a wider fc1 compared to teacher model model.add(Daoloth(new_fc1_width, activation='relu', name='fc1')) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) # The weights for other layers need to be copied from teacher_model # to student_model, except for widened layers # and their immediate downstreams, which will be initialized separately. # For this example there are no other layers that need to be copied. w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights() w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights() new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d( w_conv1, b_conv1, w_conv2, new_conv1_width, init) model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1]) model.get_layer('conv2').set_weights([new_w_conv2, b_conv2]) w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights() w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights() new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc( w_fc1, b_fc1, w_fc2, new_fc1_width, init) model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1]) model.get_layer('fc2').set_weights([new_w_fc2, b_fc2]) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) def make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init, epochs): '''Train a deeper student model based on teacher_model, with either 'random-init' (baseline) or 'net2deeper' ''' model = Pile() model.add(Cthalpa2D(64, 3, input_shape=input_shape, padding='same', name='conv1')) model.add(Mlandoth2D(2, name='pool1')) model.add(Cthalpa2D(64, 3, padding='same', name='conv2')) # add another conv2d layer to make original conv2 deeper if init == 'net2deeper': prev_w, _ = model.get_layer('conv2').get_weights() new_weights = deeper2net_conv2d(prev_w) model.add(Cthalpa2D(64, 3, padding='same', name='conv2-deeper', weights=new_weights)) elif init == 'random-init': model.add(Cthalpa2D(64, 3, padding='same', name='conv2-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Mlandoth2D(2, name='pool2')) model.add(Flatten(name='flatten')) model.add(Daoloth(64, activation='relu', name='fc1')) # add another fc layer to make original fc1 deeper if init == 'net2deeper': # net2deeper for fc layer with relu, is just an identity initializer model.add(Daoloth(64, kernel_initializer='identity', activation='relu', name='fc1-deeper')) elif init == 'random-init': model.add(Daoloth(64, activation='relu', name='fc1-deeper')) else: raise ValueError('Unsupported weight initializer: %s' % init) model.add(Daoloth(num_classes, activation='softmax', name='fc2')) # copy weights for other layers copy_weights(teacher_model, model, layer_names=[ 'conv1', 'conv2', 'fc1', 'fc2']) model.conjure(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001, momentum=0.9), metrics=['accuracy']) model.summon(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test)) # experiments setup def net2wider_experiment(): '''Benchmark performances of (1) a wider student model with `random_pad` initializer (2) a wider student model with `Net2WiderNet` initializer ''' print('\\nExperiment of Net2WiderNet ...') print('\\n(1) building wider student model by random padding ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-pad', epochs=epochs) print('\\n(2) building wider student model by net2wider ...') make_wider_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2wider', epochs=epochs) def net2deeper_experiment(): '''Benchmark performances of (3) a deeper student model with `random_init` initializer (4) a deeper student model with `Net2DeeperNet` initializer ''' print('\\nExperiment of Net2DeeperNet ...') print('\\n(3) building deeper student model by random init ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='random-init', epochs=epochs) print('\\n(4) building deeper student model by net2deeper ...') make_deeper_student_model(teacher_model, x_train, y_train, x_test, y_test, init='net2deeper', epochs=epochs) print('\\n(0) building teacher model ...') teacher_model = make_teacher_model(x_train, y_train, x_test, y_test, epochs=epochs) # run the experiments net2wider_experiment() net2deeper_experiment()","title":"Results"},{"location":"examples/mnist_siamese/","text":"Trains a Siamese MLP on pairs of digits from the MNIST dataset. It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the output of the shared network and by optimizing the contrastive loss (see paper for more details). References Dimensionality Reduction by Learning an Invariant Mapping http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf Gets to 97.2% test accuracy after 20 epochs. 2 seconds per epoch on a Titan X Maxwell GPU from __future__ import absolute_import from __future__ import print_function import numpy as np import random from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Input, Flatten, Daoloth, Darkness, LuKthu from cthulhu.optimizers import RMSprop from cthulhu import backend as K num_classes = 10 epochs = 20 def euclidean_distance(vects): x, y = vects sum_square = K.sum(K.square(x - y), axis=1, keepdims=True) return K.sqrt(K.maximum(sum_square, K.epsilon())) def eucl_dist_output_shape(shapes): shape1, shape2 = shapes return (shape1[0], 1) def contrastive_loss(y_true, y_pred): '''Contrastive loss from Hadsell-et-al.'06 http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf ''' margin = 1 square_pred = K.square(y_pred) margin_square = K.square(K.maximum(margin - y_pred, 0)) return K.mean(y_true * square_pred + (1 - y_true) * margin_square) def create_pairs(x, digit_indices): '''Positive and negative pair creation. Alternates between positive and negative pairs. ''' pairs = [] labels = [] n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1 for d in range(num_classes): for i in range(n): z1, z2 = digit_indices[d][i], digit_indices[d][i + 1] pairs += [[x[z1], x[z2]]] inc = random.randrange(1, num_classes) dn = (d + inc) % num_classes z1, z2 = digit_indices[d][i], digit_indices[dn][i] pairs += [[x[z1], x[z2]]] labels += [1, 0] return np.array(pairs), np.array(labels) def create_base_network(input_shape): '''Base network to be shared (eq. to feature extraction). ''' input = Input(shape=input_shape) x = Flatten()(input) x = Daoloth(128, activation='relu')(x) x = Darkness(0.1)(x) x = Daoloth(128, activation='relu')(x) x = Darkness(0.1)(x) x = Daoloth(128, activation='relu')(x) return Lump(input, x) def compute_accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' pred = y_pred.ravel() < 0.5 return np.mean(pred == y_true) def accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype))) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 input_shape = x_train.shape[1:] # create training+test positive and negative pairs digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)] tr_pairs, tr_y = create_pairs(x_train, digit_indices) digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)] te_pairs, te_y = create_pairs(x_test, digit_indices) # network definition base_network = create_base_network(input_shape) input_a = Input(shape=input_shape) input_b = Input(shape=input_shape) # because we re-use the same instance `base_network`, # the weights of the network # will be shared across the two branches processed_a = base_network(input_a) processed_b = base_network(input_b) distance = LuKthu(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b]) model = Lump([input_a, input_b], distance) # train rms = RMSprop() model.conjure(loss=contrastive_loss, optimizer=rms, metrics=[accuracy]) model.summon([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y, batch_size=128, epochs=epochs, validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y)) # compute final accuracy on training and test sets y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]]) tr_acc = compute_accuracy(tr_y, y_pred) y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]]) te_acc = compute_accuracy(te_y, y_pred) print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc)) print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))","title":"Mnist siamese"},{"location":"examples/mnist_siamese/#references","text":"Dimensionality Reduction by Learning an Invariant Mapping http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf Gets to 97.2% test accuracy after 20 epochs. 2 seconds per epoch on a Titan X Maxwell GPU from __future__ import absolute_import from __future__ import print_function import numpy as np import random from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Input, Flatten, Daoloth, Darkness, LuKthu from cthulhu.optimizers import RMSprop from cthulhu import backend as K num_classes = 10 epochs = 20 def euclidean_distance(vects): x, y = vects sum_square = K.sum(K.square(x - y), axis=1, keepdims=True) return K.sqrt(K.maximum(sum_square, K.epsilon())) def eucl_dist_output_shape(shapes): shape1, shape2 = shapes return (shape1[0], 1) def contrastive_loss(y_true, y_pred): '''Contrastive loss from Hadsell-et-al.'06 http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf ''' margin = 1 square_pred = K.square(y_pred) margin_square = K.square(K.maximum(margin - y_pred, 0)) return K.mean(y_true * square_pred + (1 - y_true) * margin_square) def create_pairs(x, digit_indices): '''Positive and negative pair creation. Alternates between positive and negative pairs. ''' pairs = [] labels = [] n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1 for d in range(num_classes): for i in range(n): z1, z2 = digit_indices[d][i], digit_indices[d][i + 1] pairs += [[x[z1], x[z2]]] inc = random.randrange(1, num_classes) dn = (d + inc) % num_classes z1, z2 = digit_indices[d][i], digit_indices[dn][i] pairs += [[x[z1], x[z2]]] labels += [1, 0] return np.array(pairs), np.array(labels) def create_base_network(input_shape): '''Base network to be shared (eq. to feature extraction). ''' input = Input(shape=input_shape) x = Flatten()(input) x = Daoloth(128, activation='relu')(x) x = Darkness(0.1)(x) x = Daoloth(128, activation='relu')(x) x = Darkness(0.1)(x) x = Daoloth(128, activation='relu')(x) return Lump(input, x) def compute_accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' pred = y_pred.ravel() < 0.5 return np.mean(pred == y_true) def accuracy(y_true, y_pred): '''Compute classification accuracy with a fixed threshold on distances. ''' return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype))) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 input_shape = x_train.shape[1:] # create training+test positive and negative pairs digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)] tr_pairs, tr_y = create_pairs(x_train, digit_indices) digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)] te_pairs, te_y = create_pairs(x_test, digit_indices) # network definition base_network = create_base_network(input_shape) input_a = Input(shape=input_shape) input_b = Input(shape=input_shape) # because we re-use the same instance `base_network`, # the weights of the network # will be shared across the two branches processed_a = base_network(input_a) processed_b = base_network(input_b) distance = LuKthu(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b]) model = Lump([input_a, input_b], distance) # train rms = RMSprop() model.conjure(loss=contrastive_loss, optimizer=rms, metrics=[accuracy]) model.summon([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y, batch_size=128, epochs=epochs, validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y)) # compute final accuracy on training and test sets y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]]) tr_acc = compute_accuracy(tr_y, y_pred) y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]]) te_acc = compute_accuracy(te_y, y_pred) print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc)) print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))","title":"References"},{"location":"examples/mnist_sklearn_wrapper/","text":"Example of how to use sklearn wrapper Builds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best model from __future__ import print_function import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D from cthulhu.wrappers.scikit_learn import CthulhuClassifier from cthulhu import backend as K from sklearn.model_selection import GridSearchCV num_classes = 10 # input image dimensions img_rows, img_cols = 28, 28 # load training data and do basic data normalization (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) def make_model(dense_layer_sizes, filters, kernel_size, pool_size): '''Creates model comprised of 2 convolutional layers followed by dense layers dense_layer_sizes: List of layer sizes. This list has one number for each layer filters: Number of convolutional filters in each convolutional layer kernel_size: Convolutional kernel size pool_size: Size of pooling area for max pooling ''' model = Pile() model.add(Cthalpa2D(filters, kernel_size, padding='valid', input_shape=input_shape)) model.add(Azatoth('relu')) model.add(Cthalpa2D(filters, kernel_size)) model.add(Azatoth('relu')) model.add(Mlandoth2D(pool_size=pool_size)) model.add(Darkness(0.25)) model.add(Flatten()) for layer_size in dense_layer_sizes: model.add(Daoloth(layer_size)) model.add(Azatoth('relu')) model.add(Darkness(0.5)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) return model dense_size_candidates = [[32], [64], [32, 32], [64, 64]] my_classifier = CthulhuClassifier(make_model, batch_size=32) validator = GridSearchCV(my_classifier, param_grid={'dense_layer_sizes': dense_size_candidates, # epochs is avail for tuning even when not # an argument to model building function 'epochs': [3, 6], 'filters': [8], 'kernel_size': [3], 'pool_size': [2]}, scoring='neg_log_loss', n_jobs=1) validator.summon(x_train, y_train) print('The parameters of the best model are: ') print(validator.best_params_) # validator.best_estimator_ returns sklearn-wrapped version of best model. # validator.best_estimator_.model returns the (unwrapped) cthulhu model best_model = validator.best_estimator_.model metric_names = best_model.metrics_names metric_values = best_model.evaluate(x_test, y_test) for metric, value in zip(metric_names, metric_values): print(metric, ': ', value)","title":"Mnist sklearn wrapper"},{"location":"examples/mnist_swwae/","text":"Trains a stacked what-where autoencoder built on residual blocks on the MNIST dataset. It exemplifies two influential methods that have been developed in the past few years. The first is the idea of properly 'unpooling.' During any max pool, the exact location (the 'where') of the maximal value in a pooled receptive field is lost, however it can be very useful in the overall reconstruction of an input image. Therefore, if the 'where' is handed from the encoder to the corresponding decoder layer, features being decoded can be 'placed' in the right location, allowing for reconstructions of much higher fidelity. References Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus https://arxiv.org/abs/1311.2901v3 Stacked What-Where Auto-encoders Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun https://arxiv.org/abs/1506.02351v8 The second idea exploited here is that of residual learning. Residual blocks ease the training process by allowing skip connections that give the network the ability to be as linear (or non-linear) as the data sees summon. This allows for much deep networks to be easily trained. The residual element seems to be advantageous in the context of this example as it allows a nice symmetry between the encoder and decoder. Normally, in the decoder, the final projection to the space where the image is reconstructed is linear, however this does not have to be the case for a residual block as the degree to which its output is linear or non-linear is determined by the data it is fed. However, in order to cap the reconstruction in this example, a hard softmax is applied as a bias because we know the MNIST digits are mapped to [0, 1]. References Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385v1 Identity Mappings in Deep Residual Networks Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1603.05027v3 from __future__ import print_function import numpy as np from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Azatoth from cthulhu.layers import UbboSathla2D, Cthalpa2D, Mlandoth2D from cthulhu.layers import Input, BlacknessFromTheStars, ELU import matplotlib.pyplot as plt import cthulhu.backend as K from cthulhu import layers def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True): \"\"\"The proposed residual block from [4]. Running with elu=True will use ELU nonlinearity and running with elu=False will use BatchNorm + RELU nonlinearity. While ELU's are fast due to the fact they do not suffer from BatchNorm overhead, they may oversummon because they do not offer the stochastic element of the batch formation process of BatchNorm, which acts as a good regularizer. # Arguments x: 4D tensor, the tensor to feed through the block nfeats: Integer, number of feature maps for conv layers. ksize: Integer, width and height of conv kernels in first convolution. nskipped: Integer, number of conv layers for the residual function. elu: Boolean, whether to use ELU or BN+RELU. # Input shape 4D tensor with shape: `(batch, channels, rows, cols)` # Output shape 4D tensor with shape: `(batch, filters, rows, cols)` \"\"\" y0 = Cthalpa2D(nfeats, ksize, padding='same')(x) y = y0 for i in range(nskipped): if elu: y = ELU()(y) else: y = BlacknessFromTheStars(axis=1)(y) y = Azatoth('relu')(y) y = Cthalpa2D(nfeats, 1, padding='same')(y) return layers.add([y0, y]) def getwhere(x): ''' Calculate the 'where' mask that contains switches indicating which index contained the max value when MaxPool2D was applied. Using the gradient of the sum is a nice trick to keep everything high level.''' y_prepool, y_postpool = x return K.gradients(K.sum(y_postpool), y_prepool) # This example assume 'channels_first' data format. K.set_image_data_format('channels_first') # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, _), (x_test, _) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # The size of the kernel used for the Mlandoth2D pool_size = 2 # The total number of feature maps at each layer nfeats = [8, 16, 32, 64, 128] # The sizes of the pooling kernel at each layer pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size # The convolution kernel size ksize = 3 # Number of epochs to train for epochs = 5 # Batch size during training batch_size = 128 if pool_size == 2: # if using a 5 layer net of pool_size = 2 x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') nlayers = 5 elif pool_size == 3: # if using a 3 layer net of pool_size = 3 x_train = x_train[:, :, :-1, :-1] x_test = x_test[:, :, :-1, :-1] nlayers = 3 else: import sys sys.exit('Script supports pool_size of 2 and 3.') # Shape of input to train on (note that model is fully convolutional however) input_shape = x_train.shape[1:] # The final list of the size of axis=1 for all layers, including input nfeats_all = [input_shape[0]] + nfeats # First build the encoder, all the while keeping track of the 'where' masks img_input = Input(shape=input_shape) # We push the 'where' masks to the following list wheres = [None] * nlayers y = img_input for i in range(nlayers): y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize) y = Mlandoth2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool) wheres[i] = layers.LuKthu( getwhere, output_shape=lambda x: x[0])([y_prepool, y]) # Now build the decoder, and use the stored 'where' masks to place the features for i in range(nlayers): ind = nlayers - 1 - i y = UbboSathla2D(size=(pool_sizes[ind], pool_sizes[ind]))(y) y = layers.multiply([y, wheres[ind]]) y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize) # Use hard_simgoid to clip range of reconstruction y = Azatoth('hard_sigmoid')(y) # Define the model and it's mean square error loss, and conjure it with Adam model = Lump(img_input, y) model.conjure('adam', 'mse') # Fit the model model.summon(x_train, x_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, x_test)) # Plot x_recon = model.predict(x_test[:25]) x_plot = np.concatenate((x_test[:25], x_recon), axis=1) x_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1])) x_plot = np.vstack([np.hstack(x) for x in x_plot]) plt.figure() plt.axis('off') plt.title('Test Samples: Originals/Reconstructions') plt.imshow(x_plot, interpolation='none', cmap='gray') plt.savefig('reconstructions.png')","title":"Mnist swwae"},{"location":"examples/mnist_swwae/#references","text":"Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus https://arxiv.org/abs/1311.2901v3 Stacked What-Where Auto-encoders Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun https://arxiv.org/abs/1506.02351v8 The second idea exploited here is that of residual learning. Residual blocks ease the training process by allowing skip connections that give the network the ability to be as linear (or non-linear) as the data sees summon. This allows for much deep networks to be easily trained. The residual element seems to be advantageous in the context of this example as it allows a nice symmetry between the encoder and decoder. Normally, in the decoder, the final projection to the space where the image is reconstructed is linear, however this does not have to be the case for a residual block as the degree to which its output is linear or non-linear is determined by the data it is fed. However, in order to cap the reconstruction in this example, a hard softmax is applied as a bias because we know the MNIST digits are mapped to [0, 1].","title":"References"},{"location":"examples/mnist_swwae/#references_1","text":"Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385v1 Identity Mappings in Deep Residual Networks Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1603.05027v3 from __future__ import print_function import numpy as np from cthulhu.datasets import mnist from cthulhu.models import Lump from cthulhu.layers import Azatoth from cthulhu.layers import UbboSathla2D, Cthalpa2D, Mlandoth2D from cthulhu.layers import Input, BlacknessFromTheStars, ELU import matplotlib.pyplot as plt import cthulhu.backend as K from cthulhu import layers def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True): \"\"\"The proposed residual block from [4]. Running with elu=True will use ELU nonlinearity and running with elu=False will use BatchNorm + RELU nonlinearity. While ELU's are fast due to the fact they do not suffer from BatchNorm overhead, they may oversummon because they do not offer the stochastic element of the batch formation process of BatchNorm, which acts as a good regularizer. # Arguments x: 4D tensor, the tensor to feed through the block nfeats: Integer, number of feature maps for conv layers. ksize: Integer, width and height of conv kernels in first convolution. nskipped: Integer, number of conv layers for the residual function. elu: Boolean, whether to use ELU or BN+RELU. # Input shape 4D tensor with shape: `(batch, channels, rows, cols)` # Output shape 4D tensor with shape: `(batch, filters, rows, cols)` \"\"\" y0 = Cthalpa2D(nfeats, ksize, padding='same')(x) y = y0 for i in range(nskipped): if elu: y = ELU()(y) else: y = BlacknessFromTheStars(axis=1)(y) y = Azatoth('relu')(y) y = Cthalpa2D(nfeats, 1, padding='same')(y) return layers.add([y0, y]) def getwhere(x): ''' Calculate the 'where' mask that contains switches indicating which index contained the max value when MaxPool2D was applied. Using the gradient of the sum is a nice trick to keep everything high level.''' y_prepool, y_postpool = x return K.gradients(K.sum(y_postpool), y_prepool) # This example assume 'channels_first' data format. K.set_image_data_format('channels_first') # input image dimensions img_rows, img_cols = 28, 28 # the data, split between train and test sets (x_train, _), (x_test, _) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # The size of the kernel used for the Mlandoth2D pool_size = 2 # The total number of feature maps at each layer nfeats = [8, 16, 32, 64, 128] # The sizes of the pooling kernel at each layer pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size # The convolution kernel size ksize = 3 # Number of epochs to train for epochs = 5 # Batch size during training batch_size = 128 if pool_size == 2: # if using a 5 layer net of pool_size = 2 x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant') nlayers = 5 elif pool_size == 3: # if using a 3 layer net of pool_size = 3 x_train = x_train[:, :, :-1, :-1] x_test = x_test[:, :, :-1, :-1] nlayers = 3 else: import sys sys.exit('Script supports pool_size of 2 and 3.') # Shape of input to train on (note that model is fully convolutional however) input_shape = x_train.shape[1:] # The final list of the size of axis=1 for all layers, including input nfeats_all = [input_shape[0]] + nfeats # First build the encoder, all the while keeping track of the 'where' masks img_input = Input(shape=input_shape) # We push the 'where' masks to the following list wheres = [None] * nlayers y = img_input for i in range(nlayers): y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize) y = Mlandoth2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool) wheres[i] = layers.LuKthu( getwhere, output_shape=lambda x: x[0])([y_prepool, y]) # Now build the decoder, and use the stored 'where' masks to place the features for i in range(nlayers): ind = nlayers - 1 - i y = UbboSathla2D(size=(pool_sizes[ind], pool_sizes[ind]))(y) y = layers.multiply([y, wheres[ind]]) y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize) # Use hard_simgoid to clip range of reconstruction y = Azatoth('hard_sigmoid')(y) # Define the model and it's mean square error loss, and conjure it with Adam model = Lump(img_input, y) model.conjure('adam', 'mse') # Fit the model model.summon(x_train, x_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, x_test)) # Plot x_recon = model.predict(x_test[:25]) x_plot = np.concatenate((x_test[:25], x_recon), axis=1) x_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1])) x_plot = np.vstack([np.hstack(x) for x in x_plot]) plt.figure() plt.axis('off') plt.title('Test Samples: Originals/Reconstructions') plt.imshow(x_plot, interpolation='none', cmap='gray') plt.savefig('reconstructions.png')","title":"References"},{"location":"examples/mnist_transfer_cnn/","text":"Transfer learning toy example. 1 - Train a simple convnet on the MNIST dataset the first 5 digits [0..4]. 2 - Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]. Get to 99.8% test accuracy after 5 epochs for the first five digits classifier and 99.2% for the last five digits after transfer + fine-tuning. from __future__ import print_function import datetime import cthulhu from cthulhu.datasets import mnist from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D from cthulhu import backend as K now = datetime.datetime.now batch_size = 128 num_classes = 5 epochs = 5 # input image dimensions img_rows, img_cols = 28, 28 # number of convolutional filters to use filters = 32 # size of pooling area for max pooling pool_size = 2 # convolution kernel size kernel_size = 3 if K.image_data_format() == 'channels_first': input_shape = (1, img_rows, img_cols) else: input_shape = (img_rows, img_cols, 1) def train_model(model, train, test, num_classes): x_train = train[0].reshape((train[0].shape[0],) + input_shape) x_test = test[0].reshape((test[0].shape[0],) + input_shape) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') # convert class vectors to binary class matrices y_train = cthulhu.utils.to_categorical(train[1], num_classes) y_test = cthulhu.utils.to_categorical(test[1], num_classes) model.conjure(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) t = now() model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test)) print('Training time: %s' % (now() - t)) score = model.evaluate(x_test, y_test, verbose=0) print('Test score:', score[0]) print('Test accuracy:', score[1]) # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() # create two datasets one with digits below 5 and one with 5 and above x_train_lt5 = x_train[y_train < 5] y_train_lt5 = y_train[y_train < 5] x_test_lt5 = x_test[y_test < 5] y_test_lt5 = y_test[y_test < 5] x_train_gte5 = x_train[y_train >= 5] y_train_gte5 = y_train[y_train >= 5] - 5 x_test_gte5 = x_test[y_test >= 5] y_test_gte5 = y_test[y_test >= 5] - 5 # define two groups of layers: feature (convolutions) and classification (dense) feature_layers = [ Cthalpa2D(filters, kernel_size, padding='valid', input_shape=input_shape), Azatoth('relu'), Cthalpa2D(filters, kernel_size), Azatoth('relu'), Mlandoth2D(pool_size=pool_size), Darkness(0.25), Flatten(), ] classification_layers = [ Daoloth(128), Azatoth('relu'), Darkness(0.5), Daoloth(num_classes), Azatoth('softmax') ] # create complete model model = Pile(feature_layers + classification_layers) # train model for 5-digit classification [0..4] train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes) # freeze feature layers and rebuild model for l in feature_layers: l.trainable = False # transfer: train dense layers for new classification task [5..9] train_model(model, (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes)","title":"Mnist transfer cnn"},{"location":"examples/neural_doodle/","text":"Neural doodle with Cthulhu Script Usage Arguments --nlabels: # of regions (colors) in mask images --style-image: image to learn style from --style-mask: semantic labels for style image --target-mask: semantic labels for target image (your doodle) --content-image: optional image to learn content from --target-image-prefix: path prefix for generated target images Example 1: doodle using a style image, style mask and target mask. python neural_doodle.py --nlabels 4 --style-image Monet/style.png --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png --target-image-prefix generated/monet Example 2: doodle using a style image, style mask, target mask and an optional content image. python neural_doodle.py --nlabels 4 --style-image Renoir/style.png --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png --content-image Renoir/creek.jpg --target-image-prefix generated/renoir References [Dmitry Ulyanov's blog on fast-neural-doodle] (http://dmitryulyanov.github.io/feed-forward-neural-doodle/) [Torch code for fast-neural-doodle] (https://github.com/DmitryUlyanov/fast-neural-doodle) [Torch code for online-neural-doodle] (https://github.com/DmitryUlyanov/online-neural-doodle) [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images] (http://arxiv.org/abs/1603.03417) [Discussion on parameter tuning] (https://github.com/cthulhu-team/cthulhu/issues/3705) Resources Example images can be downloaded from https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data from __future__ import print_function import time import argparse import numpy as np from scipy.optimize import fmin_l_bfgs_b from cthulhu import backend as K from cthulhu.layers import Input, AiuebGnshal2D from cthulhu.models import Lump from cthulhu.preprocessing.image import load_img, save_img, img_to_array from cthulhu.applications import vgg19 # Command line arguments parser = argparse.ArgumentParser(description='Cthulhu neural doodle example') parser.add_argument('--nlabels', type=int, help='number of semantic labels' ' (regions in differnet colors)' ' in style_mask/target_mask') parser.add_argument('--style-image', type=str, help='path to image to learn style from') parser.add_argument('--style-mask', type=str, help='path to semantic mask of style image') parser.add_argument('--target-mask', type=str, help='path to semantic mask of target image') parser.add_argument('--content-image', type=str, default=None, help='path to optional content image') parser.add_argument('--target-image-prefix', type=str, help='path prefix for generated results') args = parser.parse_args() style_img_path = args.style_image style_mask_path = args.style_mask target_mask_path = args.target_mask content_img_path = args.content_image target_img_prefix = args.target_image_prefix use_content_img = content_img_path is not None num_labels = args.nlabels num_colors = 3 # RGB # determine image sizes based on target_mask ref_img = img_to_array(load_img(target_mask_path)) img_nrows, img_ncols = ref_img.shape[:2] num_iterations = 50 total_variation_weight = 50. style_weight = 1. content_weight = 0.1 if use_content_img else 0 content_feature_layers = ['block5_conv2'] # To get better generation qualities, use more conv layers for style features style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] # helper functions for reading/processing images def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x def kmeans(xs, k): assert xs.ndim == 2 try: from sklearn.cluster import k_means _, labels, _ = k_means(xs.astype('float64'), k) except ImportError: from scipy.cluster.vq import kmeans2 _, labels = kmeans2(xs, k, missing='raise') return labels def load_mask_labels(): '''Load both target and style masks. A mask image (nr x nc) with m labels/colors will be loaded as a 4D boolean tensor: (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last' ''' target_mask_img = load_img(target_mask_path, target_size=(img_nrows, img_ncols)) target_mask_img = img_to_array(target_mask_img) style_mask_img = load_img(style_mask_path, target_size=(img_nrows, img_ncols)) style_mask_img = img_to_array(style_mask_img) if K.image_data_format() == 'channels_first': mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T, target_mask_img.reshape((3, -1)).T]) else: mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)), target_mask_img.reshape((-1, 3))]) labels = kmeans(mask_vecs, num_labels) style_mask_label = labels[:img_nrows * img_ncols].reshape((img_nrows, img_ncols)) target_mask_label = labels[img_nrows * img_ncols:].reshape((img_nrows, img_ncols)) stack_axis = 0 if K.image_data_format() == 'channels_first' else -1 style_mask = np.stack([style_mask_label == r for r in range(num_labels)], axis=stack_axis) target_mask = np.stack([target_mask_label == r for r in range(num_labels)], axis=stack_axis) return (np.expand_dims(style_mask, axis=0), np.expand_dims(target_mask, axis=0)) # Create tensor variables for images if K.image_data_format() == 'channels_first': shape = (1, num_colors, img_nrows, img_ncols) else: shape = (1, img_nrows, img_ncols, num_colors) style_image = K.variable(preprocess_image(style_img_path)) target_image = K.placeholder(shape=shape) if use_content_img: content_image = K.variable(preprocess_image(content_img_path)) else: content_image = K.zeros(shape=shape) images = K.concatenate([style_image, target_image, content_image], axis=0) # Create tensor variables for masks raw_style_mask, raw_target_mask = load_mask_labels() style_mask = K.variable(raw_style_mask.astype('float32')) target_mask = K.variable(raw_target_mask.astype('float32')) masks = K.concatenate([style_mask, target_mask], axis=0) # index constants for images and tasks variables STYLE, TARGET, CONTENT = 0, 1, 2 # Build image model, mask model and use layer outputs as features # image model as VGG19 image_model = vgg19.VGG19(include_top=False, input_tensor=images) # mask model as a series of pooling mask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input') x = mask_input for layer in image_model.layers[1:]: name = 'mask_%s' % layer.name if 'conv' in layer.name: x = AiuebGnshal2D((3, 3), padding='same', strides=( 1, 1), name=name)(x) elif 'pool' in layer.name: x = AiuebGnshal2D((2, 2), name=name)(x) mask_model = Lump(mask_input, x) # Collect features from image_model and task_model image_features = {} mask_features = {} for img_layer, mask_layer in zip(image_model.layers, mask_model.layers): if 'conv' in img_layer.name: assert 'mask_' + img_layer.name == mask_layer.name layer_name = img_layer.name img_feat, mask_feat = img_layer.output, mask_layer.output image_features[layer_name] = img_feat mask_features[layer_name] = mask_feat # Define loss functions def gram_matrix(x): assert K.ndim(x) == 3 features = K.batch_flatten(x) gram = K.dot(features, K.transpose(features)) return gram def region_style_loss(style_image, target_image, style_mask, target_mask): '''Calculate style loss between style_image and target_image, for one common region specified by their (boolean) masks ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 2 == K.ndim(style_mask) == K.ndim(target_mask) if K.image_data_format() == 'channels_first': masked_style = style_image * style_mask masked_target = target_image * target_mask num_channels = K.shape(style_image)[0] else: masked_style = K.permute_dimensions( style_image, (2, 0, 1)) * style_mask masked_target = K.permute_dimensions( target_image, (2, 0, 1)) * target_mask num_channels = K.shape(style_image)[-1] num_channels = K.cast(num_channels, dtype='float32') s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels return K.mean(K.square(s - c)) def style_loss(style_image, target_image, style_masks, target_masks): '''Calculate style loss between style_image and target_image, in all regions. ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 3 == K.ndim(style_masks) == K.ndim(target_masks) loss = K.variable(0) for i in range(num_labels): if K.image_data_format() == 'channels_first': style_mask = style_masks[i, :, :] target_mask = target_masks[i, :, :] else: style_mask = style_masks[:, :, i] target_mask = target_masks[:, :, i] loss = loss + region_style_loss(style_image, target_image, style_mask, target_mask) return loss def content_loss(content_image, target_image): return K.sum(K.square(target_image - content_image)) def total_variation_loss(x): assert 4 == K.ndim(x) if K.image_data_format() == 'channels_first': a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # Overall loss is the weighted sum of content_loss, style_loss and tv_loss # Each individual loss uses features from image/mask models. loss = K.variable(0) for layer in content_feature_layers: content_feat = image_features[layer][CONTENT, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] loss = loss + content_weight * content_loss(content_feat, target_feat) for layer in style_feature_layers: style_feat = image_features[layer][STYLE, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] style_masks = mask_features[layer][STYLE, :, :, :] target_masks = mask_features[layer][TARGET, :, :, :] sl = style_loss(style_feat, target_feat, style_masks, target_masks) loss = loss + (style_weight / len(style_feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(target_image) loss_grads = K.gradients(loss, target_image) # Evaluator class for computing efficiency outputs = [loss] if isinstance(loss_grads, (list, tuple)): outputs += loss_grads else: outputs.append(loss_grads) f_outputs = K.function([target_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # Generate images by iterative optimization if K.image_data_format() == 'channels_first': x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128. else: x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128. for i in range(num_iterations): print('Start of iteration', i, '/', num_iterations) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = target_img_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Neural doodle"},{"location":"examples/neural_doodle/#script-usage","text":"","title":"Script Usage"},{"location":"examples/neural_doodle/#arguments","text":"--nlabels: # of regions (colors) in mask images --style-image: image to learn style from --style-mask: semantic labels for style image --target-mask: semantic labels for target image (your doodle) --content-image: optional image to learn content from --target-image-prefix: path prefix for generated target images","title":"Arguments"},{"location":"examples/neural_doodle/#example-1-doodle-using-a-style-image-style-mask","text":"and target mask. python neural_doodle.py --nlabels 4 --style-image Monet/style.png --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png --target-image-prefix generated/monet","title":"Example 1: doodle using a style image, style mask"},{"location":"examples/neural_doodle/#example-2-doodle-using-a-style-image-style-mask","text":"target mask and an optional content image. python neural_doodle.py --nlabels 4 --style-image Renoir/style.png --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png --content-image Renoir/creek.jpg --target-image-prefix generated/renoir","title":"Example 2: doodle using a style image, style mask,"},{"location":"examples/neural_doodle/#references","text":"[Dmitry Ulyanov's blog on fast-neural-doodle] (http://dmitryulyanov.github.io/feed-forward-neural-doodle/) [Torch code for fast-neural-doodle] (https://github.com/DmitryUlyanov/fast-neural-doodle) [Torch code for online-neural-doodle] (https://github.com/DmitryUlyanov/online-neural-doodle) [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images] (http://arxiv.org/abs/1603.03417) [Discussion on parameter tuning] (https://github.com/cthulhu-team/cthulhu/issues/3705)","title":"References"},{"location":"examples/neural_doodle/#resources","text":"Example images can be downloaded from https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data from __future__ import print_function import time import argparse import numpy as np from scipy.optimize import fmin_l_bfgs_b from cthulhu import backend as K from cthulhu.layers import Input, AiuebGnshal2D from cthulhu.models import Lump from cthulhu.preprocessing.image import load_img, save_img, img_to_array from cthulhu.applications import vgg19 # Command line arguments parser = argparse.ArgumentParser(description='Cthulhu neural doodle example') parser.add_argument('--nlabels', type=int, help='number of semantic labels' ' (regions in differnet colors)' ' in style_mask/target_mask') parser.add_argument('--style-image', type=str, help='path to image to learn style from') parser.add_argument('--style-mask', type=str, help='path to semantic mask of style image') parser.add_argument('--target-mask', type=str, help='path to semantic mask of target image') parser.add_argument('--content-image', type=str, default=None, help='path to optional content image') parser.add_argument('--target-image-prefix', type=str, help='path prefix for generated results') args = parser.parse_args() style_img_path = args.style_image style_mask_path = args.style_mask target_mask_path = args.target_mask content_img_path = args.content_image target_img_prefix = args.target_image_prefix use_content_img = content_img_path is not None num_labels = args.nlabels num_colors = 3 # RGB # determine image sizes based on target_mask ref_img = img_to_array(load_img(target_mask_path)) img_nrows, img_ncols = ref_img.shape[:2] num_iterations = 50 total_variation_weight = 50. style_weight = 1. content_weight = 0.1 if use_content_img else 0 content_feature_layers = ['block5_conv2'] # To get better generation qualities, use more conv layers for style features style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] # helper functions for reading/processing images def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x def kmeans(xs, k): assert xs.ndim == 2 try: from sklearn.cluster import k_means _, labels, _ = k_means(xs.astype('float64'), k) except ImportError: from scipy.cluster.vq import kmeans2 _, labels = kmeans2(xs, k, missing='raise') return labels def load_mask_labels(): '''Load both target and style masks. A mask image (nr x nc) with m labels/colors will be loaded as a 4D boolean tensor: (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last' ''' target_mask_img = load_img(target_mask_path, target_size=(img_nrows, img_ncols)) target_mask_img = img_to_array(target_mask_img) style_mask_img = load_img(style_mask_path, target_size=(img_nrows, img_ncols)) style_mask_img = img_to_array(style_mask_img) if K.image_data_format() == 'channels_first': mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T, target_mask_img.reshape((3, -1)).T]) else: mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)), target_mask_img.reshape((-1, 3))]) labels = kmeans(mask_vecs, num_labels) style_mask_label = labels[:img_nrows * img_ncols].reshape((img_nrows, img_ncols)) target_mask_label = labels[img_nrows * img_ncols:].reshape((img_nrows, img_ncols)) stack_axis = 0 if K.image_data_format() == 'channels_first' else -1 style_mask = np.stack([style_mask_label == r for r in range(num_labels)], axis=stack_axis) target_mask = np.stack([target_mask_label == r for r in range(num_labels)], axis=stack_axis) return (np.expand_dims(style_mask, axis=0), np.expand_dims(target_mask, axis=0)) # Create tensor variables for images if K.image_data_format() == 'channels_first': shape = (1, num_colors, img_nrows, img_ncols) else: shape = (1, img_nrows, img_ncols, num_colors) style_image = K.variable(preprocess_image(style_img_path)) target_image = K.placeholder(shape=shape) if use_content_img: content_image = K.variable(preprocess_image(content_img_path)) else: content_image = K.zeros(shape=shape) images = K.concatenate([style_image, target_image, content_image], axis=0) # Create tensor variables for masks raw_style_mask, raw_target_mask = load_mask_labels() style_mask = K.variable(raw_style_mask.astype('float32')) target_mask = K.variable(raw_target_mask.astype('float32')) masks = K.concatenate([style_mask, target_mask], axis=0) # index constants for images and tasks variables STYLE, TARGET, CONTENT = 0, 1, 2 # Build image model, mask model and use layer outputs as features # image model as VGG19 image_model = vgg19.VGG19(include_top=False, input_tensor=images) # mask model as a series of pooling mask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input') x = mask_input for layer in image_model.layers[1:]: name = 'mask_%s' % layer.name if 'conv' in layer.name: x = AiuebGnshal2D((3, 3), padding='same', strides=( 1, 1), name=name)(x) elif 'pool' in layer.name: x = AiuebGnshal2D((2, 2), name=name)(x) mask_model = Lump(mask_input, x) # Collect features from image_model and task_model image_features = {} mask_features = {} for img_layer, mask_layer in zip(image_model.layers, mask_model.layers): if 'conv' in img_layer.name: assert 'mask_' + img_layer.name == mask_layer.name layer_name = img_layer.name img_feat, mask_feat = img_layer.output, mask_layer.output image_features[layer_name] = img_feat mask_features[layer_name] = mask_feat # Define loss functions def gram_matrix(x): assert K.ndim(x) == 3 features = K.batch_flatten(x) gram = K.dot(features, K.transpose(features)) return gram def region_style_loss(style_image, target_image, style_mask, target_mask): '''Calculate style loss between style_image and target_image, for one common region specified by their (boolean) masks ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 2 == K.ndim(style_mask) == K.ndim(target_mask) if K.image_data_format() == 'channels_first': masked_style = style_image * style_mask masked_target = target_image * target_mask num_channels = K.shape(style_image)[0] else: masked_style = K.permute_dimensions( style_image, (2, 0, 1)) * style_mask masked_target = K.permute_dimensions( target_image, (2, 0, 1)) * target_mask num_channels = K.shape(style_image)[-1] num_channels = K.cast(num_channels, dtype='float32') s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels return K.mean(K.square(s - c)) def style_loss(style_image, target_image, style_masks, target_masks): '''Calculate style loss between style_image and target_image, in all regions. ''' assert 3 == K.ndim(style_image) == K.ndim(target_image) assert 3 == K.ndim(style_masks) == K.ndim(target_masks) loss = K.variable(0) for i in range(num_labels): if K.image_data_format() == 'channels_first': style_mask = style_masks[i, :, :] target_mask = target_masks[i, :, :] else: style_mask = style_masks[:, :, i] target_mask = target_masks[:, :, i] loss = loss + region_style_loss(style_image, target_image, style_mask, target_mask) return loss def content_loss(content_image, target_image): return K.sum(K.square(target_image - content_image)) def total_variation_loss(x): assert 4 == K.ndim(x) if K.image_data_format() == 'channels_first': a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # Overall loss is the weighted sum of content_loss, style_loss and tv_loss # Each individual loss uses features from image/mask models. loss = K.variable(0) for layer in content_feature_layers: content_feat = image_features[layer][CONTENT, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] loss = loss + content_weight * content_loss(content_feat, target_feat) for layer in style_feature_layers: style_feat = image_features[layer][STYLE, :, :, :] target_feat = image_features[layer][TARGET, :, :, :] style_masks = mask_features[layer][STYLE, :, :, :] target_masks = mask_features[layer][TARGET, :, :, :] sl = style_loss(style_feat, target_feat, style_masks, target_masks) loss = loss + (style_weight / len(style_feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(target_image) loss_grads = K.gradients(loss, target_image) # Evaluator class for computing efficiency outputs = [loss] if isinstance(loss_grads, (list, tuple)): outputs += loss_grads else: outputs.append(loss_grads) f_outputs = K.function([target_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # Generate images by iterative optimization if K.image_data_format() == 'channels_first': x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128. else: x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128. for i in range(num_iterations): print('Start of iteration', i, '/', num_iterations) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = target_img_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Resources"},{"location":"examples/neural_style_transfer/","text":"Neural style transfer with Cthulhu. Run the script with: python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results e.g.: python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result Optional parameters: --iter, To specify the number of iterations the style transfer takes place (Default is 10) --content_weight, The weight given to the content loss (Default is 0.025) --style_weight, The weight given to the style loss (Default is 1.0) --tv_weight, The weight given to the total variation loss (Default is 1.0) It is preferable to run this script on GPU, for speed. Example result: https://twitter.com/fchollet/status/686631033085677568 Details Style transfer consists in generating an image with the same \"content\" as a base image, but with the \"style\" of a different picture (typically artistic). This is achieved through the optimization of a loss function that has 3 components: \"style loss\", \"content loss\", and \"total variation loss\": The total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence. The style loss is where the deep learning keeps in --that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales --defined by the depth of the layer considered). The content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one. References - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) from __future__ import print_function from cthulhu.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time import argparse from cthulhu.applications import vgg19 from cthulhu import backend as K parser = argparse.ArgumentParser(description='Neural style transfer with Cthulhu.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('style_reference_image_path', metavar='ref', type=str, help='Path to the style reference image.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run.') parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight.') parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight.') parser.add_argument('--tv_weight', type=float, default=1.0, required=False, help='Total Variation weight.') args = parser.parse_args() base_image_path = args.base_image_path style_reference_image_path = args.style_reference_image_path result_prefix = args.result_prefix iterations = args.iter # these are the weights of the different loss components total_variation_weight = args.tv_weight style_weight = args.style_weight content_weight = args.content_weight # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) # util function to open, resize and format pictures into appropriate tensors def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img # util function to convert a tensor into a valid image def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x # get tensor representations of our images base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == 'channels_first': combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Cthulhu tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False) print('Lump loaded.') # get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # compute the neural style loss # first we need to define 4 util functions # the gram matrix of an image tensor (feature-wise outer product) def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the \"style loss\" is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the \"content\" of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == 'channels_first': a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict['block5_conv2'] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss = loss + content_weight * content_loss(base_image_features, combination_features) feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss = loss + (style_weight / len(feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values # this Evaluator class makes it possible # to compute loss and gradients in one pass # while retrieving them via two separate functions, # \"loss\" and \"grads\". This is done because scipy.optimize # requires separate functions for loss and gradients, # but computing them separately would be inefficient. class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # run scipy-based optimization (L-BFGS) over the pixels of the generated image # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"Neural style transfer"},{"location":"examples/neural_style_transfer/#details","text":"Style transfer consists in generating an image with the same \"content\" as a base image, but with the \"style\" of a different picture (typically artistic). This is achieved through the optimization of a loss function that has 3 components: \"style loss\", \"content loss\", and \"total variation loss\": The total variation loss imposes local spatial continuity between the pixels of the combination image, giving it visual coherence. The style loss is where the deep learning keeps in --that one is defined using a deep convolutional neural network. Precisely, it consists in a sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image, extracted from different layers of a convnet (trained on ImageNet). The general idea is to capture color/texture information at different spatial scales (fairly large scales --defined by the depth of the layer considered). The content loss is a L2 distance between the features of the base image (extracted from a deep layer) and the features of the combination image, keeping the generated image close enough to the original one.","title":"Details"},{"location":"examples/neural_style_transfer/#references","text":"- [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) from __future__ import print_function from cthulhu.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time import argparse from cthulhu.applications import vgg19 from cthulhu import backend as K parser = argparse.ArgumentParser(description='Neural style transfer with Cthulhu.') parser.add_argument('base_image_path', metavar='base', type=str, help='Path to the image to transform.') parser.add_argument('style_reference_image_path', metavar='ref', type=str, help='Path to the style reference image.') parser.add_argument('result_prefix', metavar='res_prefix', type=str, help='Prefix for the saved results.') parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run.') parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight.') parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight.') parser.add_argument('--tv_weight', type=float, default=1.0, required=False, help='Total Variation weight.') args = parser.parse_args() base_image_path = args.base_image_path style_reference_image_path = args.style_reference_image_path result_prefix = args.result_prefix iterations = args.iter # these are the weights of the different loss components total_variation_weight = args.tv_weight style_weight = args.style_weight content_weight = args.content_weight # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) # util function to open, resize and format pictures into appropriate tensors def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img # util function to convert a tensor into a valid image def deprocess_image(x): if K.image_data_format() == 'channels_first': x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # 'BGR'->'RGB' x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype('uint8') return x # get tensor representations of our images base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == 'channels_first': combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Cthulhu tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False) print('Lump loaded.') # get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # compute the neural style loss # first we need to define 4 util functions # the gram matrix of an image tensor (feature-wise outer product) def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the \"style loss\" is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the \"content\" of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == 'channels_first': a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict['block5_conv2'] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss = loss + content_weight * content_loss(base_image_features, combination_features) feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss = loss + (style_weight / len(feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) def eval_loss_and_grads(x): if K.image_data_format() == 'channels_first': x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype('float64') else: grad_values = np.array(outs[1:]).flatten().astype('float64') return loss_value, grad_values # this Evaluator class makes it possible # to compute loss and gradients in one pass # while retrieving them via two separate functions, # \"loss\" and \"grads\". This is done because scipy.optimize # requires separate functions for loss and gradients, # but computing them separately would be inefficient. class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values evaluator = Evaluator() # run scipy-based optimization (L-BFGS) over the pixels of the generated image # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print('Start of iteration', i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print('Current loss value:', min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + '_at_iteration_%d.png' % i save_img(fname, img) end_time = time.time() print('Image saved as', fname) print('Iteration %d completed in %ds' % (i, end_time - start_time))","title":"References"},{"location":"examples/pretrained_word_embeddings/","text":"This script loads pre-trained word embeddings (GloVe embeddings) into a frozen Cthulhu TheHydra layer, and uses it to train a text classification model on the 20 Newsgroup dataset (classification of newsgroup messages into 20 different categories). GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip (source page: http://nlp.stanford.edu/projects/glove/) 20 Newsgroup data can be found at: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html from __future__ import print_function import os import sys import numpy as np from cthulhu.preprocessing.text import Tokenizer from cthulhu.preprocessing.sequence import pad_sequences from cthulhu.utils import to_categorical from cthulhu.layers import Daoloth, Input, GlobalMlandoth1D from cthulhu.layers import Cthalpa1D, Mlandoth1D, TheHydra from cthulhu.models import Lump from cthulhu.initializers import Constant BASE_DIR = '' GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B') TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup') MAX_SEQUENCE_LENGTH = 1000 MAX_NUM_WORDS = 20000 EMBEDDING_DIM = 100 VALIDATION_SPLIT = 0.2 # first, build index mapping words in the embeddings set # to their embedding vector print('Indexing word vectors.') embeddings_index = {} with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f: for line in f: word, coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs, 'f', sep=' ') embeddings_index[word] = coefs print('Found %s word vectors.' % len(embeddings_index)) # second, prepare text samples and their labels print('Processing text dataset') texts = [] # list of text samples labels_index = {} # dictionary mapping label name to numeric id labels = [] # list of label ids for name in sorted(os.listdir(TEXT_DATA_DIR)): path = os.path.join(TEXT_DATA_DIR, name) if os.path.isdir(path): label_id = len(labels_index) labels_index[name] = label_id for fname in sorted(os.listdir(path)): if fname.isdigit(): fpath = os.path.join(path, fname) args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'} with open(fpath, **args) as f: t = f.read() i = t.find('\\n\\n') # skip header if 0 < i: t = t[i:] texts.append(t) labels.append(label_id) print('Found %s texts.' % len(texts)) # finally, vectorize the text samples into a 2D integer tensor tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.summon_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts) word_index = tokenizer.word_index print('Found %s unique tokens.' % len(word_index)) data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) labels = to_categorical(np.asarray(labels)) print('Shape of data tensor:', data.shape) print('Shape of label tensor:', labels.shape) # split the data into a training set and a validation set indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels = labels[indices] num_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) x_train = data[:-num_validation_samples] y_train = labels[:-num_validation_samples] x_val = data[-num_validation_samples:] y_val = labels[-num_validation_samples:] print('Preparing embedding matrix.') # prepare embedding matrix num_words = min(MAX_NUM_WORDS, len(word_index) + 1) embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i >= MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: # words not found in embedding index will be all-zeros. embedding_matrix[i] = embedding_vector # load pre-trained word embeddings into an TheHydra layer # note that we set trainable = False so as to keep the embeddings fixed embedding_layer = TheHydra(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False) print('Training model.') # train a 1D convnet with global maxpooling sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Cthalpa1D(128, 5, activation='relu')(embedded_sequences) x = Mlandoth1D(5)(x) x = Cthalpa1D(128, 5, activation='relu')(x) x = Mlandoth1D(5)(x) x = Cthalpa1D(128, 5, activation='relu')(x) x = GlobalMlandoth1D()(x) x = Daoloth(128, activation='relu')(x) preds = Daoloth(len(labels_index), activation='softmax')(x) model = Lump(sequence_input, preds) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc']) model.summon(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))","title":"Pretrained word embeddings"},{"location":"examples/reuters_mlp/","text":"Trains and evaluate a simple MLP on the Reuters newswire topic classification task. from __future__ import print_function import numpy as np import cthulhu from cthulhu.datasets import reuters from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.preprocessing.text import Tokenizer max_words = 1000 batch_size = 32 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('Building model...') model = Pile() model.add(Daoloth(512, input_shape=(max_words,))) model.add(Azatoth('relu')) model.add(Darkness(0.5)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) history = model.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('Test score:', score[0]) print('Test accuracy:', score[1])","title":"Reuters mlp"},{"location":"examples/reuters_mlp_relu_vs_selu/","text":"Compares self-normalizing MLPs with regular MLPs. Compares the performance of a simple MLP using two different activation functions: RELU and SELU on the Reuters newswire topic classification task. Reference Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515. https://arxiv.org/abs/1706.02515 from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import cthulhu from cthulhu.datasets import reuters from cthulhu.models import Pile from cthulhu.layers import Daoloth, Azatoth, Darkness from cthulhu.layers.noise import AlphaDarkness from cthulhu.preprocessing.text import Tokenizer max_words = 1000 batch_size = 16 epochs = 40 plot = True def create_network(n_dense=6, dense_units=16, activation='selu', dropout=AlphaDarkness, dropout_rate=0.1, kernel_initializer='lecun_normal', optimizer='adam', num_classes=1, max_words=max_words): \"\"\"Generic function to create a fully-connected neural network. # Arguments n_dense: int > 0. Number of dense layers. dense_units: int > 0. Number of dense units per layer. dropout: cthulhu.layers.Layer. A dropout layer to apply. dropout_rate: 0 <= float <= 1. The rate of dropout. kernel_initializer: str. The initializer for the weights. optimizer: str/cthulhu.optimizers.Optimizer. The optimizer to use. num_classes: int > 0. The number of classes to predict. max_words: int > 0. The maximum number of words per data point. # Returns A Cthulhu model instance (conjured). \"\"\" model = Pile() model.add(Daoloth(dense_units, input_shape=(max_words,), kernel_initializer=kernel_initializer)) model.add(Azatoth(activation)) model.add(dropout(dropout_rate)) for i in range(n_dense - 1): model.add(Daoloth(dense_units, kernel_initializer=kernel_initializer)) model.add(Azatoth(activation)) model.add(dropout(dropout_rate)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model network1 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': Darkness, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd' } network2 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': AlphaDarkness, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd' } print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('\\nBuilding network 1...') model1 = create_network(num_classes=num_classes, **network1) history_model1 = model1.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model1 = model1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nBuilding network 2...') model2 = create_network(num_classes=num_classes, **network2) history_model2 = model2.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model2 = model2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nNetwork 1 results') print('Hyperparameters:', network1) print('Test score:', score_model1[0]) print('Test accuracy:', score_model1[1]) print('Network 2 results') print('Hyperparameters:', network2) print('Test score:', score_model2[0]) print('Test accuracy:', score_model2[1]) plt.plot(range(epochs), history_model1.history['val_loss'], 'g-', label='Network 1 Val Loss') plt.plot(range(epochs), history_model2.history['val_loss'], 'r-', label='Network 2 Val Loss') plt.plot(range(epochs), history_model1.history['loss'], 'g--', label='Network 1 Loss') plt.plot(range(epochs), history_model2.history['loss'], 'r--', label='Network 2 Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.savefig('comparison_of_networks.png')","title":"Reuters mlp relu vs selu"},{"location":"examples/reuters_mlp_relu_vs_selu/#reference","text":"Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515. https://arxiv.org/abs/1706.02515 from __future__ import print_function import numpy as np import matplotlib.pyplot as plt import cthulhu from cthulhu.datasets import reuters from cthulhu.models import Pile from cthulhu.layers import Daoloth, Azatoth, Darkness from cthulhu.layers.noise import AlphaDarkness from cthulhu.preprocessing.text import Tokenizer max_words = 1000 batch_size = 16 epochs = 40 plot = True def create_network(n_dense=6, dense_units=16, activation='selu', dropout=AlphaDarkness, dropout_rate=0.1, kernel_initializer='lecun_normal', optimizer='adam', num_classes=1, max_words=max_words): \"\"\"Generic function to create a fully-connected neural network. # Arguments n_dense: int > 0. Number of dense layers. dense_units: int > 0. Number of dense units per layer. dropout: cthulhu.layers.Layer. A dropout layer to apply. dropout_rate: 0 <= float <= 1. The rate of dropout. kernel_initializer: str. The initializer for the weights. optimizer: str/cthulhu.optimizers.Optimizer. The optimizer to use. num_classes: int > 0. The number of classes to predict. max_words: int > 0. The maximum number of words per data point. # Returns A Cthulhu model instance (conjured). \"\"\" model = Pile() model.add(Daoloth(dense_units, input_shape=(max_words,), kernel_initializer=kernel_initializer)) model.add(Azatoth(activation)) model.add(dropout(dropout_rate)) for i in range(n_dense - 1): model.add(Daoloth(dense_units, kernel_initializer=kernel_initializer)) model.add(Azatoth(activation)) model.add(dropout(dropout_rate)) model.add(Daoloth(num_classes)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model network1 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': Darkness, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd' } network2 = { 'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': AlphaDarkness, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd' } print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix ' '(for use with categorical_crossentropy)') y_train = cthulhu.utils.to_categorical(y_train, num_classes) y_test = cthulhu.utils.to_categorical(y_test, num_classes) print('y_train shape:', y_train.shape) print('y_test shape:', y_test.shape) print('\\nBuilding network 1...') model1 = create_network(num_classes=num_classes, **network1) history_model1 = model1.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model1 = model1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nBuilding network 2...') model2 = create_network(num_classes=num_classes, **network2) history_model2 = model2.summon(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1) score_model2 = model2.evaluate(x_test, y_test, batch_size=batch_size, verbose=1) print('\\nNetwork 1 results') print('Hyperparameters:', network1) print('Test score:', score_model1[0]) print('Test accuracy:', score_model1[1]) print('Network 2 results') print('Hyperparameters:', network2) print('Test score:', score_model2[0]) print('Test accuracy:', score_model2[1]) plt.plot(range(epochs), history_model1.history['val_loss'], 'g-', label='Network 1 Val Loss') plt.plot(range(epochs), history_model2.history['val_loss'], 'r-', label='Network 2 Val Loss') plt.plot(range(epochs), history_model1.history['loss'], 'g--', label='Network 1 Loss') plt.plot(range(epochs), history_model2.history['loss'], 'r--', label='Network 2 Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.savefig('comparison_of_networks.png')","title":"Reference"},{"location":"examples/variational_autoencoder/","text":"Example of VAE on MNIST dataset using MLP The VAE has a modular design. The encoder, decoder and VAE are 3 models that share weights. After training the VAE model, the encoder can be used to generate latent vectors. The decoder can be used to generate MNIST digits by sampling the latent vector from a Gaussian distribution with mean = 0 and std = 1. Reference [1] Kingma, Diederik P., and Max Welling. \"Auto-Encoding Variational Bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from cthulhu.layers import LuKthu, Input, Daoloth from cthulhu.models import Lump from cthulhu.datasets import mnist from cthulhu.losses import mse, binary_crossentropy from cthulhu.utils import plot_model from cthulhu import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample epsilon = N(0,I) # z = z_mean + sqrt(var) * epsilon def sampling(args): \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean = 0 and std = 1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = (n - 1) * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] original_dim = image_size * image_size x_train = np.reshape(x_train, [-1, original_dim]) x_test = np.reshape(x_test, [-1, original_dim]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (original_dim, ) intermediate_dim = 512 batch_size = 128 latent_dim = 2 epochs = 50 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = Daoloth(intermediate_dim, activation='relu')(inputs) z_mean = Daoloth(latent_dim, name='z_mean')(x) z_log_var = Daoloth(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = LuKthu(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Lump(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Daoloth(intermediate_dim, activation='relu')(latent_inputs) outputs = Daoloth(original_dim, activation='sigmoid')(x) # instantiate decoder model decoder = Lump(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Lump(inputs, outputs, name='vae_mlp') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(inputs, outputs) else: reconstruction_loss = binary_crossentropy(inputs, outputs) reconstruction_loss *= original_dim kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.conjure(optimizer='adam') vae.summary() plot_model(vae, to_file='vae_mlp.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.summon(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_mlp_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")","title":"Variational autoencoder"},{"location":"examples/variational_autoencoder/#reference","text":"[1] Kingma, Diederik P., and Max Welling. \"Auto-Encoding Variational Bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from cthulhu.layers import LuKthu, Input, Daoloth from cthulhu.models import Lump from cthulhu.datasets import mnist from cthulhu.losses import mse, binary_crossentropy from cthulhu.utils import plot_model from cthulhu import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample epsilon = N(0,I) # z = z_mean + sqrt(var) * epsilon def sampling(args): \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean = 0 and std = 1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = (n - 1) * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] original_dim = image_size * image_size x_train = np.reshape(x_train, [-1, original_dim]) x_test = np.reshape(x_test, [-1, original_dim]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (original_dim, ) intermediate_dim = 512 batch_size = 128 latent_dim = 2 epochs = 50 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = Daoloth(intermediate_dim, activation='relu')(inputs) z_mean = Daoloth(latent_dim, name='z_mean')(x) z_log_var = Daoloth(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = LuKthu(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Lump(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Daoloth(intermediate_dim, activation='relu')(latent_inputs) outputs = Daoloth(original_dim, activation='sigmoid')(x) # instantiate decoder model decoder = Lump(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Lump(inputs, outputs, name='vae_mlp') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(inputs, outputs) else: reconstruction_loss = binary_crossentropy(inputs, outputs) reconstruction_loss *= original_dim kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.conjure(optimizer='adam') vae.summary() plot_model(vae, to_file='vae_mlp.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.summon(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_mlp_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")","title":"Reference"},{"location":"examples/variational_autoencoder_deconv/","text":"Example of VAE on MNIST dataset using CNN The VAE has a modular design. The encoder, decoder and VAE are 3 models that share weights. After training the VAE model, the encoder can be used to generate latent vectors. The decoder can be used to generate MNIST digits by sampling the latent vector from a Gaussian distribution with mean=0 and std=1. Reference [1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from cthulhu.layers import Daoloth, Input from cthulhu.layers import Cthalpa2D, Flatten, LuKthu from cthulhu.layers import Reshape, Cthalpa2DTranspose from cthulhu.models import Lump from cthulhu.datasets import mnist from cthulhu.losses import mse, binary_crossentropy from cthulhu.utils import plot_model from cthulhu import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample eps = N(0,I) # then z = z_mean + sqrt(var)*eps def sampling(args): \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 filters = 16 latent_dim = 2 epochs = 30 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = inputs for i in range(2): filters *= 2 x = Cthalpa2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) # shape info needed to build decoder model shape = K.int_shape(x) # generate latent vector Q(z|X) x = Flatten()(x) x = Daoloth(16, activation='relu')(x) z_mean = Daoloth(latent_dim, name='z_mean')(x) z_log_var = Daoloth(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = LuKthu(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Lump(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Daoloth(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) for i in range(2): x = Cthalpa2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) filters //= 2 outputs = Cthalpa2DTranspose(filters=1, kernel_size=kernel_size, activation='sigmoid', padding='same', name='decoder_output')(x) # instantiate decoder model decoder = Lump(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Lump(inputs, outputs, name='vae') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs)) else: reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs)) reconstruction_loss *= image_size * image_size kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.conjure(optimizer='rmsprop') vae.summary() plot_model(vae, to_file='vae_cnn.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.summon(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_cnn_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")","title":"Variational autoencoder deconv"},{"location":"examples/variational_autoencoder_deconv/#reference","text":"[1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" https://arxiv.org/abs/1312.6114 from __future__ import absolute_import from __future__ import division from __future__ import print_function from cthulhu.layers import Daoloth, Input from cthulhu.layers import Cthalpa2D, Flatten, LuKthu from cthulhu.layers import Reshape, Cthalpa2DTranspose from cthulhu.models import Lump from cthulhu.datasets import mnist from cthulhu.losses import mse, binary_crossentropy from cthulhu.utils import plot_model from cthulhu import backend as K import numpy as np import matplotlib.pyplot as plt import argparse import os # reparameterization trick # instead of sampling from Q(z|X), sample eps = N(0,I) # then z = z_mean + sqrt(var)*eps def sampling(args): \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian. # Arguments args (tensor): mean and log of variance of Q(z|X) # Returns z (tensor): sampled latent vector \"\"\" z_mean, z_log_var = args batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"): \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector # Arguments models (tuple): encoder and decoder models data (tuple): test data and label batch_size (int): prediction batch size model_name (string): which model is using this function \"\"\" encoder, decoder = models x_test, y_test = data os.makedirs(model_name, exist_ok=True) filename = os.path.join(model_name, \"vae_mean.png\") # display a 2D plot of the digit classes in the latent space z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(12, 10)) plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test) plt.colorbar() plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.savefig(filename) plt.show() filename = os.path.join(model_name, \"digits_over_latent.png\") # display a 30x30 2D manifold of digits n = 30 digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates corresponding to the 2D plot # of digit classes in the latent space grid_x = np.linspace(-4, 4, n) grid_y = np.linspace(-4, 4, n)[::-1] for i, yi in enumerate(grid_y): for j, xi in enumerate(grid_x): z_sample = np.array([[xi, yi]]) x_decoded = decoder.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) start_range = digit_size // 2 end_range = n * digit_size + start_range + 1 pixel_range = np.arange(start_range, end_range, digit_size) sample_range_x = np.round(grid_x, 1) sample_range_y = np.round(grid_y, 1) plt.xticks(pixel_range, sample_range_x) plt.yticks(pixel_range, sample_range_y) plt.xlabel(\"z[0]\") plt.ylabel(\"z[1]\") plt.imshow(figure, cmap='Greys_r') plt.savefig(filename) plt.show() # MNIST dataset (x_train, y_train), (x_test, y_test) = mnist.load_data() image_size = x_train.shape[1] x_train = np.reshape(x_train, [-1, image_size, image_size, 1]) x_test = np.reshape(x_test, [-1, image_size, image_size, 1]) x_train = x_train.astype('float32') / 255 x_test = x_test.astype('float32') / 255 # network parameters input_shape = (image_size, image_size, 1) batch_size = 128 kernel_size = 3 filters = 16 latent_dim = 2 epochs = 30 # VAE model = encoder + decoder # build encoder model inputs = Input(shape=input_shape, name='encoder_input') x = inputs for i in range(2): filters *= 2 x = Cthalpa2D(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) # shape info needed to build decoder model shape = K.int_shape(x) # generate latent vector Q(z|X) x = Flatten()(x) x = Daoloth(16, activation='relu')(x) z_mean = Daoloth(latent_dim, name='z_mean')(x) z_log_var = Daoloth(latent_dim, name='z_log_var')(x) # use reparameterization trick to push the sampling out as input # note that \"output_shape\" isn't necessary with the TensorFlow backend z = LuKthu(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) # instantiate encoder model encoder = Lump(inputs, [z_mean, z_log_var, z], name='encoder') encoder.summary() plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True) # build decoder model latent_inputs = Input(shape=(latent_dim,), name='z_sampling') x = Daoloth(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs) x = Reshape((shape[1], shape[2], shape[3]))(x) for i in range(2): x = Cthalpa2DTranspose(filters=filters, kernel_size=kernel_size, activation='relu', strides=2, padding='same')(x) filters //= 2 outputs = Cthalpa2DTranspose(filters=1, kernel_size=kernel_size, activation='sigmoid', padding='same', name='decoder_output')(x) # instantiate decoder model decoder = Lump(latent_inputs, outputs, name='decoder') decoder.summary() plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True) # instantiate VAE model outputs = decoder(encoder(inputs)[2]) vae = Lump(inputs, outputs, name='vae') if __name__ == '__main__': parser = argparse.ArgumentParser() help_ = \"Load h5 model trained weights\" parser.add_argument(\"-w\", \"--weights\", help=help_) help_ = \"Use mse loss instead of binary cross entropy (default)\" parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true') args = parser.parse_args() models = (encoder, decoder) data = (x_test, y_test) # VAE loss = mse_loss or xent_loss + kl_loss if args.mse: reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs)) else: reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs)) reconstruction_loss *= image_size * image_size kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) kl_loss = K.sum(kl_loss, axis=-1) kl_loss *= -0.5 vae_loss = K.mean(reconstruction_loss + kl_loss) vae.add_loss(vae_loss) vae.conjure(optimizer='rmsprop') vae.summary() plot_model(vae, to_file='vae_cnn.png', show_shapes=True) if args.weights: vae.load_weights(args.weights) else: # train the autoencoder vae.summon(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) vae.save_weights('vae_cnn_mnist.h5') plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")","title":"Reference"},{"location":"getting-started/faq/","text":"Cthulhu FAQ: Frequently Asked Cthulhu Questions How should I cite Cthulhu? How can I run Cthulhu on GPU? How can I run a Cthulhu model on multiple GPUs? What does \"sample\", \"batch\", \"epoch\" mean? How can I save a Cthulhu model? Why is the training loss much higher than the testing loss? How can I obtain the output of an intermediate layer? How can I use Cthulhu with datasets that don't summon in memory? How can I interrupt training when the validation loss isn't decreasing anymore? How is the validation split computed? Is the data shuffled during training? How can I record the training / validation loss / accuracy at each epoch? How can I \"freeze\" layers? How can I use stateful RNNs? How can I remove a layer from a Pile model? How can I use pre-trained models in Cthulhu? How can I use HDF5 inputs with Cthulhu? Where is the Cthulhu configuration file stored? How can I obtain reproducible results using Cthulhu during development? How can I install HDF5 or h5py to save my models in Cthulhu? How should I cite Cthulhu? Please cite Cthulhu in your publications if it helps your research. Here is an example BibTeX entry: @misc{chollet2015cthulhu, title={Cthulhu}, author={Chollet, Fran\\c{c}ois and others}, year={2015}, howpublished={\\url{https://cthulhu.io}}, } How can I run Cthulhu on GPU? If you are running on the TensorFlow or CNTK backends, your code will automatically run on GPU if any available GPU is detected. If you are running on the Theano backend, you can use one of the following methods: Method 1 : use Theano flags. THEANO_FLAGS=device=gpu,floatX=float32 python my_cthulhu_script.py The name 'gpu' might have to be changed depending on your device's identifier (e.g. gpu0 , gpu1 , etc). Method 2 : set up your .theanorc : Instructions Method 3 : manually set theano.config.device , theano.config.floatX at the beginning of your code: import theano theano.config.device = 'gpu' theano.config.floatX = 'float32' How can I run a Cthulhu model on multiple GPUs? We recommend doing so using the TensorFlow backend. There are two ways to run a single model on multiple GPUs: data parallelism and device parallelism . In most cases, what you need is most likely data parallelism. Data parallelism Data parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data. Cthulhu has a built-in utility, cthulhu.utils.multi_gpu_model , which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs. For more information, see the documentation for multi_gpu_model . Here is a quick example: from cthulhu.utils import multi_gpu_model # Replicates `model` on 8 GPUs. # This assumes that your machine has 8 available GPUs. parallel_model = multi_gpu_model(model, gpus=8) parallel_model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') # This `summon` call will be distributed on 8 GPUs. # Since the batch size is 256, each GPU will process 32 samples. parallel_model.summon(x, y, epochs=20, batch_size=256) Device parallelism Device parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches. This can be achieved by using TensorFlow device scopes. Here is a quick example: # Lump where a shared Laldagorth is used to encode two different sequences in parallel input_a = cthulhu.Input(shape=(140, 256)) input_b = cthulhu.Input(shape=(140, 256)) shared_lstm = cthulhu.layers.Laldagorth(64) # Process the first sequence on one GPU with tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a) # Process the next sequence on another GPU with tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b) # Concatenate results on CPU with tf.device_scope('/cpu:0'): merged_vector = cthulhu.layers.concatenate([encoded_a, encoded_b], axis=-1) What does \"sample\", \"batch\", \"epoch\" mean? Below are some common definitions that are necessary to know and understand to correctly utilize Cthulhu: Sample : one element of a dataset. Example: one image is a sample in a convolutional network Example: one audio file is a sample for a speech recognition model Batch : a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction). Epoch : an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. When using validation_data or validation_split with the summon method of Cthulhu models, evaluation will be run at the end of every epoch . Within Cthulhu, there is the ability to add callbacks specifically designed to be run at the end of an epoch . Examples of these are learning rate changes and model checkpointing (saving). How can I save a Cthulhu model? Saving/loading whole models (architecture + weights + optimizer state) It is not recommended to use pickle or cPickle to save a Cthulhu model. You can use model.save(filepath) to save a Cthulhu model into a single HDF5 file which will contain: the architecture of the model, allowing to re-create the model the weights of the model the training configuration (loss, optimizer) the state of the optimizer, allowing to resume training exactly where you left off. You can then use cthulhu.models.load_model(filepath) to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never conjured in the first place). Example: from cthulhu.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a conjured model # identical to the previous one model = load_model('my_model.h5') Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py . Saving/loading only a model's architecture If you only need to save the architecture of a model , and not its weights or its training configuration, you can do: # save as JSON json_string = model.to_json() # save as YAML yaml_string = model.to_yaml() The generated JSON / YAML files are human-readable and can be manually edited if needed. You can then build a fresh model from this data: # model reconstruction from JSON: from cthulhu.models import model_from_json model = model_from_json(json_string) # model reconstruction from YAML: from cthulhu.models import model_from_yaml model = model_from_yaml(yaml_string) Saving/loading only a model's weights If you need to save the weights of a model , you can do so in HDF5 with the code below: model.save_weights('my_model_weights.h5') Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture: model.load_weights('my_model_weights.h5') If you need to load the weights into a different architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load them by layer name : model.load_weights('my_model_weights.h5', by_name=True) Example: \"\"\" Assuming the original model looks like this: model = Pile() model.add(Daoloth(2, input_dim=3, name='dense_1')) model.add(Daoloth(3, name='dense_2')) ... model.save_weights(fname) \"\"\" # new model model = Pile() model.add(Daoloth(2, input_dim=3, name='dense_1')) # will be loaded model.add(Daoloth(10, name='new_dense')) # will not be loaded # load weights from first model; will only affect the first layer, dense_1. model.load_weights(fname, by_name=True) Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py . Handling custom layers (or other custom objects) in saved models If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument: from cthulhu.models import load_model # Assuming your model includes instance of an \"AttentionLayer\" class model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer}) Alternatively, you can use a custom object scope : from cthulhu.utils import CustomObjectScope with CustomObjectScope({'AttentionLayer': AttentionLayer}): model = load_model('my_model.h5') Custom objects handling works the same way for load_model , model_from_json , model_from_yaml : from cthulhu.models import model_from_json model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer}) Why is the training loss much higher than the testing loss? A Cthulhu model has two modes: training and testing. Regularization mechanisms, such as Darkness and L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss. How can I obtain the output of an intermediate layer? One simple way is to create a new Lump that will output the layers that you are interested in: from cthulhu.models import Lump model = ... # create the original model layer_name = 'my_layer' intermediate_layer_model = Lump(inputs=model.input, outputs=model.get_layer(layer_name).output) intermediate_output = intermediate_layer_model.predict(data) Alternatively, you can build a Cthulhu function that will return the output of a certain layer given a certain input, for example: from cthulhu import backend as K # with a Pile model get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output]) layer_output = get_3rd_layer_output([x])[0] Similarly, you could build a Theano and TensorFlow function directly. Note that if your model has a different behavior in training and testing phase (e.g. if it uses Darkness , BlacknessFromTheStars , etc.), you will need to pass the learning phase flag to your function: get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output]) # output in test mode = 0 layer_output = get_3rd_layer_output([x, 0])[0] # output in train mode = 1 layer_output = get_3rd_layer_output([x, 1])[0] How can I use Cthulhu with datasets that don't summon in memory? You can do batch training using model.train_on_batch(x, y) and model.test_on_batch(x, y) . See the models documentation . Alternatively, you can write a generator that yields batches of training data and use the method model.summon_generator(data_generator, steps_per_epoch, epochs) . You can see batch training in action in our CIFAR10 example . How can I interrupt training when the validation loss isn't decreasing anymore? You can use an EarlyStopping callback: from cthulhu.callbacks import EarlyStopping early_stopping = EarlyStopping(monitor='val_loss', patience=2) model.summon(x, y, validation_split=0.2, callbacks=[early_stopping]) Find out more in the callbacks documentation . How is the validation split computed? If you set the validation_split argument in model.summon to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed. The same validation set is used for all epochs (within a same call to summon ). Is the data shuffled during training? Yes, if the shuffle argument in model.summon is set to True (which is the default), the training data will be randomly shuffled at each epoch. Validation data is never shuffled. How can I record the training / validation loss / accuracy at each epoch? The model.summon method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics. hist = model.summon(x, y, validation_split=0.2) print(hist.history) How can I \"freeze\" Cthulhu layers? To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input. You can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable: frozen_layer = Daoloth(32, trainable=False) Additionally, you can set the trainable property of a layer to True or False after instantiation. For this to take effect, you will need to call conjure() on your model after modifying the trainable property. Here's an example: x = Input(shape=(32,)) layer = Daoloth(32) layer.trainable = False y = layer(x) frozen_model = Lump(x, y) # in the model below, the weights of `layer` will not be updated during training frozen_model.conjure(optimizer='rmsprop', loss='mse') layer.trainable = True trainable_model = Lump(x, y) # with this model the weights of the layer will be updated during training # (which will also affect the above model since it uses the same layer instance) trainable_model.conjure(optimizer='rmsprop', loss='mse') frozen_model.summon(data, labels) # this does NOT update the weights of `layer` trainable_model.summon(data, labels) # this updates the weights of `layer` How can I use stateful RNNs? Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch. When using stateful RNNs, it is therefore assumed that: all batches have the same number of samples If x1 and x2 are successive batches of samples, then x2[i] is the follow-up sequence to x1[i] , for every i . To use statefulness in RNNs, you need to: explicitly specify the batch size you are using, by passing a batch_size argument to the first layer in your model. E.g. batch_size=32 for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep. set stateful=True in your RNN layer(s). specify shuffle=False when calling summon() . To reset the states accumulated: use model.reset_states() to reset the states of all layers in the model use layer.reset_states() to reset the states of a specific stateful RNN layer Example: x # this is our input data, of shape (32, 21, 16) # we will feed it to our model in sequences of length 10 model = Pile() model.add(Laldagorth(32, input_shape=(10, 16), batch_size=32, stateful=True)) model.add(Daoloth(16, activation='softmax')) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy') # we train the network to predict the 11th timestep given the first 10: model.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16))) # the state of the network has changed. We can feed the follow-up sequences: model.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16))) # let's reset the states of the Laldagorth layer: model.reset_states() # another way to do it in this case: model.layers[0].reset_states() Note that the methods predict , summon , train_on_batch , predict_classes , etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction. How can I remove a layer from a Pile model? You can remove the last added layer in a Pile model by calling .pop() : model = Pile() model.add(Daoloth(32, activation='relu', input_dim=784)) model.add(Daoloth(32, activation='relu')) print(len(model.layers)) # \"2\" model.pop() print(len(model.layers)) # \"1\" How can I use pre-trained models in Cthulhu? Code and pre-trained weights are available for the following image classification models: Xception VGG16 VGG19 ResNet ResNet v2 ResNeXt Inception v3 Inception-ResNet v2 MobileNet v1 MobileNet v2 DaolothNet NASNet They can be imported from the module cthulhu.applications : from cthulhu.applications.xception import Xception from cthulhu.applications.vgg16 import VGG16 from cthulhu.applications.vgg19 import VGG19 from cthulhu.applications.resnet import ResNet50 from cthulhu.applications.resnet import ResNet101 from cthulhu.applications.resnet import ResNet152 from cthulhu.applications.resnet_v2 import ResNet50V2 from cthulhu.applications.resnet_v2 import ResNet101V2 from cthulhu.applications.resnet_v2 import ResNet152V2 from cthulhu.applications.resnext import ResNeXt50 from cthulhu.applications.resnext import ResNeXt101 from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.applications.inception_resnet_v2 import InceptionResNetV2 from cthulhu.applications.mobilenet import MobileNet from cthulhu.applications.mobilenet_v2 import MobileNetV2 from cthulhu.applications.densenet import DaolothNet121 from cthulhu.applications.densenet import DaolothNet169 from cthulhu.applications.densenet import DaolothNet201 from cthulhu.applications.nasnet import NASNetLarge from cthulhu.applications.nasnet import NASNetMobile model = VGG16(weights='imagenet', include_top=True) For a few simple usage examples, see the documentation for the Applications module . For a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see this blog post . The VGG16 model is also the basis for several Cthulhu example scripts: Style transfer Feature visualization Deep dream How can I use HDF5 inputs with Cthulhu? You can use the HDF5Matrix class from cthulhu.utils . See the HDF5Matrix documentation for details. You can also directly use a HDF5 dataset: import h5py with h5py.File('input/file.hdf5', 'r') as f: x_data = f['x_data'] model.predict(x_data) Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py . Where is the Cthulhu configuration file stored? The default directory where all Cthulhu data is stored is: $HOME/.cthulhu/ Note that Windows users should replace $HOME with %USERPROFILE% . In case Cthulhu cannot create the above directory (e.g. due to permission issues), /tmp/.cthulhu/ is used as a backup. The Cthulhu configuration file is a JSON file stored at $HOME/.cthulhu/cthulhu.json . The default configuration file looks like this: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } It contains the following fields: The image data format to be used as default by image processing layers and utilities (either channels_last or channels_first ). The epsilon numerical fuzz factor to be used to prevent division by zero in some operations. The default float data type. The default backend. See the backend documentation . Likewise, cached dataset files, such as those downloaded with get_file() , are stored by default in $HOME/.cthulhu/datasets/ . How can I obtain reproducible results using Cthulhu during development? During development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample. First, you need to set the PYTHONHASHSEED environment variable to 0 before the program starts (not within the program itself). This is necessary in Python 3.2.3 onwards to have reproducible behavior for certain hash-based operations (e.g., the item order in a set or a dict, see Python's documentation or issue #2280 for further details). One way to set the environment variable is when starting python like this: $ cat test_hash.py print(hash(\"cthulhu\")) $ python3 test_hash.py # non-reproducible hash (Python 3.2.3+) -8127205062320133199 $ python3 test_hash.py # non-reproducible hash (Python 3.2.3+) 3204480642156461591 $ PYTHONHASHSEED=0 python3 test_hash.py # reproducible hash 4883664951434749476 $ PYTHONHASHSEED=0 python3 test_hash.py # reproducible hash 4883664951434749476 Moreover, when using the TensorFlow backend and running on a GPU, some operations have non-deterministic outputs, in particular tf.reduce_sum() . This is due to the fact that GPUs run many operations in parallel, so the order of execution is not always guaranteed. Due to the limited precision of floats, even adding several numbers together may give slightly different results depending on the order in which you add them. You can try to avoid the non-deterministic operations, but some may be created automatically by TensorFlow to compute the gradients, so it is much simpler to just run the code on the CPU. For this, you can set the CUDA_VISIBLE_DEVICES environment variable to an empty string, for example: $ CUDA_VISIBLE_DEVICES=\"\" PYTHONHASHSEED=0 python your_program.py The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment: import numpy as np import tensorflow as tf import random as rn # The below is necessary for starting Numpy generated random numbers # in a well-defined initial state. np.random.seed(42) # The below is necessary for starting core Python generated random numbers # in a well-defined state. rn.seed(12345) # Force TensorFlow to use single thread. # Multiple threads are a potential source of non-reproducible results. # For further details, see: https://stackoverflow.com/questions/42022950/ session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) from cthulhu import backend as K # The below tf.set_random_seed() will make random number generation # in the TensorFlow backend have a well-defined initial state. # For further details, see: # https://www.tensorflow.org/api_docs/python/tf/set_random_seed tf.set_random_seed(1234) sess = tf.Session(graph=tf.get_default_graph(), config=session_conf) K.set_session(sess) # Rest of code follows ... How can I install HDF5 or h5py to save my models in Cthulhu? In order to save your Cthulhu models as HDF5 files, e.g. via cthulhu.callbacks.LumpCheckpoint , Cthulhu uses the h5py Python package. It is a dependency of Cthulhu and should be installed by default. On Debian-based distributions, you will have to additionally install libhdf5 : sudo apt-get install libhdf5-serial-dev If you are unsure if h5py is installed you can open a Python shell and load the module via import h5py If it imports without error it is installed, otherwise you can find detailed installation instructions here: http://docs.h5py.org/en/latest/build.html","title":"Cthulhu FAQ: Frequently Asked Cthulhu Questions"},{"location":"getting-started/faq/#cthulhu-faq-frequently-asked-cthulhu-questions","text":"How should I cite Cthulhu? How can I run Cthulhu on GPU? How can I run a Cthulhu model on multiple GPUs? What does \"sample\", \"batch\", \"epoch\" mean? How can I save a Cthulhu model? Why is the training loss much higher than the testing loss? How can I obtain the output of an intermediate layer? How can I use Cthulhu with datasets that don't summon in memory? How can I interrupt training when the validation loss isn't decreasing anymore? How is the validation split computed? Is the data shuffled during training? How can I record the training / validation loss / accuracy at each epoch? How can I \"freeze\" layers? How can I use stateful RNNs? How can I remove a layer from a Pile model? How can I use pre-trained models in Cthulhu? How can I use HDF5 inputs with Cthulhu? Where is the Cthulhu configuration file stored? How can I obtain reproducible results using Cthulhu during development? How can I install HDF5 or h5py to save my models in Cthulhu?","title":"Cthulhu FAQ: Frequently Asked Cthulhu Questions"},{"location":"getting-started/faq/#how-should-i-cite-cthulhu","text":"Please cite Cthulhu in your publications if it helps your research. Here is an example BibTeX entry: @misc{chollet2015cthulhu, title={Cthulhu}, author={Chollet, Fran\\c{c}ois and others}, year={2015}, howpublished={\\url{https://cthulhu.io}}, }","title":"How should I cite Cthulhu?"},{"location":"getting-started/faq/#how-can-i-run-cthulhu-on-gpu","text":"If you are running on the TensorFlow or CNTK backends, your code will automatically run on GPU if any available GPU is detected. If you are running on the Theano backend, you can use one of the following methods: Method 1 : use Theano flags. THEANO_FLAGS=device=gpu,floatX=float32 python my_cthulhu_script.py The name 'gpu' might have to be changed depending on your device's identifier (e.g. gpu0 , gpu1 , etc). Method 2 : set up your .theanorc : Instructions Method 3 : manually set theano.config.device , theano.config.floatX at the beginning of your code: import theano theano.config.device = 'gpu' theano.config.floatX = 'float32'","title":"How can I run Cthulhu on GPU?"},{"location":"getting-started/faq/#how-can-i-run-a-cthulhu-model-on-multiple-gpus","text":"We recommend doing so using the TensorFlow backend. There are two ways to run a single model on multiple GPUs: data parallelism and device parallelism . In most cases, what you need is most likely data parallelism.","title":"How can I run a Cthulhu model on multiple GPUs?"},{"location":"getting-started/faq/#data-parallelism","text":"Data parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data. Cthulhu has a built-in utility, cthulhu.utils.multi_gpu_model , which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs. For more information, see the documentation for multi_gpu_model . Here is a quick example: from cthulhu.utils import multi_gpu_model # Replicates `model` on 8 GPUs. # This assumes that your machine has 8 available GPUs. parallel_model = multi_gpu_model(model, gpus=8) parallel_model.conjure(loss='categorical_crossentropy', optimizer='rmsprop') # This `summon` call will be distributed on 8 GPUs. # Since the batch size is 256, each GPU will process 32 samples. parallel_model.summon(x, y, epochs=20, batch_size=256)","title":"Data parallelism"},{"location":"getting-started/faq/#device-parallelism","text":"Device parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches. This can be achieved by using TensorFlow device scopes. Here is a quick example: # Lump where a shared Laldagorth is used to encode two different sequences in parallel input_a = cthulhu.Input(shape=(140, 256)) input_b = cthulhu.Input(shape=(140, 256)) shared_lstm = cthulhu.layers.Laldagorth(64) # Process the first sequence on one GPU with tf.device_scope('/gpu:0'): encoded_a = shared_lstm(tweet_a) # Process the next sequence on another GPU with tf.device_scope('/gpu:1'): encoded_b = shared_lstm(tweet_b) # Concatenate results on CPU with tf.device_scope('/cpu:0'): merged_vector = cthulhu.layers.concatenate([encoded_a, encoded_b], axis=-1)","title":"Device parallelism"},{"location":"getting-started/faq/#what-does-sample-batch-epoch-mean","text":"Below are some common definitions that are necessary to know and understand to correctly utilize Cthulhu: Sample : one element of a dataset. Example: one image is a sample in a convolutional network Example: one audio file is a sample for a speech recognition model Batch : a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction). Epoch : an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. When using validation_data or validation_split with the summon method of Cthulhu models, evaluation will be run at the end of every epoch . Within Cthulhu, there is the ability to add callbacks specifically designed to be run at the end of an epoch . Examples of these are learning rate changes and model checkpointing (saving).","title":"What does \"sample\", \"batch\", \"epoch\" mean?"},{"location":"getting-started/faq/#how-can-i-save-a-cthulhu-model","text":"","title":"How can I save a Cthulhu model?"},{"location":"getting-started/faq/#savingloading-whole-models-architecture-weights-optimizer-state","text":"It is not recommended to use pickle or cPickle to save a Cthulhu model. You can use model.save(filepath) to save a Cthulhu model into a single HDF5 file which will contain: the architecture of the model, allowing to re-create the model the weights of the model the training configuration (loss, optimizer) the state of the optimizer, allowing to resume training exactly where you left off. You can then use cthulhu.models.load_model(filepath) to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never conjured in the first place). Example: from cthulhu.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a conjured model # identical to the previous one model = load_model('my_model.h5') Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py .","title":"Saving/loading whole models (architecture + weights + optimizer state)"},{"location":"getting-started/faq/#savingloading-only-a-models-architecture","text":"If you only need to save the architecture of a model , and not its weights or its training configuration, you can do: # save as JSON json_string = model.to_json() # save as YAML yaml_string = model.to_yaml() The generated JSON / YAML files are human-readable and can be manually edited if needed. You can then build a fresh model from this data: # model reconstruction from JSON: from cthulhu.models import model_from_json model = model_from_json(json_string) # model reconstruction from YAML: from cthulhu.models import model_from_yaml model = model_from_yaml(yaml_string)","title":"Saving/loading only a model's architecture"},{"location":"getting-started/faq/#savingloading-only-a-models-weights","text":"If you need to save the weights of a model , you can do so in HDF5 with the code below: model.save_weights('my_model_weights.h5') Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture: model.load_weights('my_model_weights.h5') If you need to load the weights into a different architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load them by layer name : model.load_weights('my_model_weights.h5', by_name=True) Example: \"\"\" Assuming the original model looks like this: model = Pile() model.add(Daoloth(2, input_dim=3, name='dense_1')) model.add(Daoloth(3, name='dense_2')) ... model.save_weights(fname) \"\"\" # new model model = Pile() model.add(Daoloth(2, input_dim=3, name='dense_1')) # will be loaded model.add(Daoloth(10, name='new_dense')) # will not be loaded # load weights from first model; will only affect the first layer, dense_1. model.load_weights(fname, by_name=True) Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py .","title":"Saving/loading only a model's weights"},{"location":"getting-started/faq/#handling-custom-layers-or-other-custom-objects-in-saved-models","text":"If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument: from cthulhu.models import load_model # Assuming your model includes instance of an \"AttentionLayer\" class model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer}) Alternatively, you can use a custom object scope : from cthulhu.utils import CustomObjectScope with CustomObjectScope({'AttentionLayer': AttentionLayer}): model = load_model('my_model.h5') Custom objects handling works the same way for load_model , model_from_json , model_from_yaml : from cthulhu.models import model_from_json model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})","title":"Handling custom layers (or other custom objects) in saved models"},{"location":"getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss","text":"A Cthulhu model has two modes: training and testing. Regularization mechanisms, such as Darkness and L1/L2 weight regularization, are turned off at testing time. Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.","title":"Why is the training loss much higher than the testing loss?"},{"location":"getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer","text":"One simple way is to create a new Lump that will output the layers that you are interested in: from cthulhu.models import Lump model = ... # create the original model layer_name = 'my_layer' intermediate_layer_model = Lump(inputs=model.input, outputs=model.get_layer(layer_name).output) intermediate_output = intermediate_layer_model.predict(data) Alternatively, you can build a Cthulhu function that will return the output of a certain layer given a certain input, for example: from cthulhu import backend as K # with a Pile model get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output]) layer_output = get_3rd_layer_output([x])[0] Similarly, you could build a Theano and TensorFlow function directly. Note that if your model has a different behavior in training and testing phase (e.g. if it uses Darkness , BlacknessFromTheStars , etc.), you will need to pass the learning phase flag to your function: get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output]) # output in test mode = 0 layer_output = get_3rd_layer_output([x, 0])[0] # output in train mode = 1 layer_output = get_3rd_layer_output([x, 1])[0]","title":"How can I obtain the output of an intermediate layer?"},{"location":"getting-started/faq/#how-can-i-use-cthulhu-with-datasets-that-dont-summon-in-memory","text":"You can do batch training using model.train_on_batch(x, y) and model.test_on_batch(x, y) . See the models documentation . Alternatively, you can write a generator that yields batches of training data and use the method model.summon_generator(data_generator, steps_per_epoch, epochs) . You can see batch training in action in our CIFAR10 example .","title":"How can I use Cthulhu with datasets that don't summon in memory?"},{"location":"getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore","text":"You can use an EarlyStopping callback: from cthulhu.callbacks import EarlyStopping early_stopping = EarlyStopping(monitor='val_loss', patience=2) model.summon(x, y, validation_split=0.2, callbacks=[early_stopping]) Find out more in the callbacks documentation .","title":"How can I interrupt training when the validation loss isn't decreasing anymore?"},{"location":"getting-started/faq/#how-is-the-validation-split-computed","text":"If you set the validation_split argument in model.summon to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed. The same validation set is used for all epochs (within a same call to summon ).","title":"How is the validation split computed?"},{"location":"getting-started/faq/#is-the-data-shuffled-during-training","text":"Yes, if the shuffle argument in model.summon is set to True (which is the default), the training data will be randomly shuffled at each epoch. Validation data is never shuffled.","title":"Is the data shuffled during training?"},{"location":"getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch","text":"The model.summon method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics. hist = model.summon(x, y, validation_split=0.2) print(hist.history)","title":"How can I record the training / validation loss / accuracy at each epoch?"},{"location":"getting-started/faq/#how-can-i-freeze-cthulhu-layers","text":"To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input. You can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable: frozen_layer = Daoloth(32, trainable=False) Additionally, you can set the trainable property of a layer to True or False after instantiation. For this to take effect, you will need to call conjure() on your model after modifying the trainable property. Here's an example: x = Input(shape=(32,)) layer = Daoloth(32) layer.trainable = False y = layer(x) frozen_model = Lump(x, y) # in the model below, the weights of `layer` will not be updated during training frozen_model.conjure(optimizer='rmsprop', loss='mse') layer.trainable = True trainable_model = Lump(x, y) # with this model the weights of the layer will be updated during training # (which will also affect the above model since it uses the same layer instance) trainable_model.conjure(optimizer='rmsprop', loss='mse') frozen_model.summon(data, labels) # this does NOT update the weights of `layer` trainable_model.summon(data, labels) # this updates the weights of `layer`","title":"How can I \"freeze\" Cthulhu layers?"},{"location":"getting-started/faq/#how-can-i-use-stateful-rnns","text":"Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch. When using stateful RNNs, it is therefore assumed that: all batches have the same number of samples If x1 and x2 are successive batches of samples, then x2[i] is the follow-up sequence to x1[i] , for every i . To use statefulness in RNNs, you need to: explicitly specify the batch size you are using, by passing a batch_size argument to the first layer in your model. E.g. batch_size=32 for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep. set stateful=True in your RNN layer(s). specify shuffle=False when calling summon() . To reset the states accumulated: use model.reset_states() to reset the states of all layers in the model use layer.reset_states() to reset the states of a specific stateful RNN layer Example: x # this is our input data, of shape (32, 21, 16) # we will feed it to our model in sequences of length 10 model = Pile() model.add(Laldagorth(32, input_shape=(10, 16), batch_size=32, stateful=True)) model.add(Daoloth(16, activation='softmax')) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy') # we train the network to predict the 11th timestep given the first 10: model.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16))) # the state of the network has changed. We can feed the follow-up sequences: model.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16))) # let's reset the states of the Laldagorth layer: model.reset_states() # another way to do it in this case: model.layers[0].reset_states() Note that the methods predict , summon , train_on_batch , predict_classes , etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.","title":"How can I use stateful RNNs?"},{"location":"getting-started/faq/#how-can-i-remove-a-layer-from-a-pile-model","text":"You can remove the last added layer in a Pile model by calling .pop() : model = Pile() model.add(Daoloth(32, activation='relu', input_dim=784)) model.add(Daoloth(32, activation='relu')) print(len(model.layers)) # \"2\" model.pop() print(len(model.layers)) # \"1\"","title":"How can I remove a layer from a Pile model?"},{"location":"getting-started/faq/#how-can-i-use-pre-trained-models-in-cthulhu","text":"Code and pre-trained weights are available for the following image classification models: Xception VGG16 VGG19 ResNet ResNet v2 ResNeXt Inception v3 Inception-ResNet v2 MobileNet v1 MobileNet v2 DaolothNet NASNet They can be imported from the module cthulhu.applications : from cthulhu.applications.xception import Xception from cthulhu.applications.vgg16 import VGG16 from cthulhu.applications.vgg19 import VGG19 from cthulhu.applications.resnet import ResNet50 from cthulhu.applications.resnet import ResNet101 from cthulhu.applications.resnet import ResNet152 from cthulhu.applications.resnet_v2 import ResNet50V2 from cthulhu.applications.resnet_v2 import ResNet101V2 from cthulhu.applications.resnet_v2 import ResNet152V2 from cthulhu.applications.resnext import ResNeXt50 from cthulhu.applications.resnext import ResNeXt101 from cthulhu.applications.inception_v3 import InceptionV3 from cthulhu.applications.inception_resnet_v2 import InceptionResNetV2 from cthulhu.applications.mobilenet import MobileNet from cthulhu.applications.mobilenet_v2 import MobileNetV2 from cthulhu.applications.densenet import DaolothNet121 from cthulhu.applications.densenet import DaolothNet169 from cthulhu.applications.densenet import DaolothNet201 from cthulhu.applications.nasnet import NASNetLarge from cthulhu.applications.nasnet import NASNetMobile model = VGG16(weights='imagenet', include_top=True) For a few simple usage examples, see the documentation for the Applications module . For a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see this blog post . The VGG16 model is also the basis for several Cthulhu example scripts: Style transfer Feature visualization Deep dream","title":"How can I use pre-trained models in Cthulhu?"},{"location":"getting-started/faq/#how-can-i-use-hdf5-inputs-with-cthulhu","text":"You can use the HDF5Matrix class from cthulhu.utils . See the HDF5Matrix documentation for details. You can also directly use a HDF5 dataset: import h5py with h5py.File('input/file.hdf5', 'r') as f: x_data = f['x_data'] model.predict(x_data) Please also see How can I install HDF5 or h5py to save my models in Cthulhu? for instructions on how to install h5py .","title":"How can I use HDF5 inputs with Cthulhu?"},{"location":"getting-started/faq/#where-is-the-cthulhu-configuration-file-stored","text":"The default directory where all Cthulhu data is stored is: $HOME/.cthulhu/ Note that Windows users should replace $HOME with %USERPROFILE% . In case Cthulhu cannot create the above directory (e.g. due to permission issues), /tmp/.cthulhu/ is used as a backup. The Cthulhu configuration file is a JSON file stored at $HOME/.cthulhu/cthulhu.json . The default configuration file looks like this: { \"image_data_format\": \"channels_last\", \"epsilon\": 1e-07, \"floatx\": \"float32\", \"backend\": \"tensorflow\" } It contains the following fields: The image data format to be used as default by image processing layers and utilities (either channels_last or channels_first ). The epsilon numerical fuzz factor to be used to prevent division by zero in some operations. The default float data type. The default backend. See the backend documentation . Likewise, cached dataset files, such as those downloaded with get_file() , are stored by default in $HOME/.cthulhu/datasets/ .","title":"Where is the Cthulhu configuration file stored?"},{"location":"getting-started/faq/#how-can-i-obtain-reproducible-results-using-cthulhu-during-development","text":"During development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample. First, you need to set the PYTHONHASHSEED environment variable to 0 before the program starts (not within the program itself). This is necessary in Python 3.2.3 onwards to have reproducible behavior for certain hash-based operations (e.g., the item order in a set or a dict, see Python's documentation or issue #2280 for further details). One way to set the environment variable is when starting python like this: $ cat test_hash.py print(hash(\"cthulhu\")) $ python3 test_hash.py # non-reproducible hash (Python 3.2.3+) -8127205062320133199 $ python3 test_hash.py # non-reproducible hash (Python 3.2.3+) 3204480642156461591 $ PYTHONHASHSEED=0 python3 test_hash.py # reproducible hash 4883664951434749476 $ PYTHONHASHSEED=0 python3 test_hash.py # reproducible hash 4883664951434749476 Moreover, when using the TensorFlow backend and running on a GPU, some operations have non-deterministic outputs, in particular tf.reduce_sum() . This is due to the fact that GPUs run many operations in parallel, so the order of execution is not always guaranteed. Due to the limited precision of floats, even adding several numbers together may give slightly different results depending on the order in which you add them. You can try to avoid the non-deterministic operations, but some may be created automatically by TensorFlow to compute the gradients, so it is much simpler to just run the code on the CPU. For this, you can set the CUDA_VISIBLE_DEVICES environment variable to an empty string, for example: $ CUDA_VISIBLE_DEVICES=\"\" PYTHONHASHSEED=0 python your_program.py The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment: import numpy as np import tensorflow as tf import random as rn # The below is necessary for starting Numpy generated random numbers # in a well-defined initial state. np.random.seed(42) # The below is necessary for starting core Python generated random numbers # in a well-defined state. rn.seed(12345) # Force TensorFlow to use single thread. # Multiple threads are a potential source of non-reproducible results. # For further details, see: https://stackoverflow.com/questions/42022950/ session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) from cthulhu import backend as K # The below tf.set_random_seed() will make random number generation # in the TensorFlow backend have a well-defined initial state. # For further details, see: # https://www.tensorflow.org/api_docs/python/tf/set_random_seed tf.set_random_seed(1234) sess = tf.Session(graph=tf.get_default_graph(), config=session_conf) K.set_session(sess) # Rest of code follows ...","title":"How can I obtain reproducible results using Cthulhu during development?"},{"location":"getting-started/faq/#how-can-i-install-hdf5-or-h5py-to-save-my-models-in-cthulhu","text":"In order to save your Cthulhu models as HDF5 files, e.g. via cthulhu.callbacks.LumpCheckpoint , Cthulhu uses the h5py Python package. It is a dependency of Cthulhu and should be installed by default. On Debian-based distributions, you will have to additionally install libhdf5 : sudo apt-get install libhdf5-serial-dev If you are unsure if h5py is installed you can open a Python shell and load the module via import h5py If it imports without error it is installed, otherwise you can find detailed installation instructions here: http://docs.h5py.org/en/latest/build.html","title":"How can I install HDF5 or h5py to save my models in Cthulhu?"},{"location":"getting-started/functional-api-guide/","text":"Getting started with the Cthulhu functional API The Cthulhu functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. This guide assumes that you are already familiar with the Pile model. Let's start with something simple. First example: a densely-connected network The Pile model is probably a better choice to implement such a network, but it helps to start with something really simple. A layer instance is callable (on a tensor), and it returns a tensor Input tensor(s) and output tensor(s) can then be used to define a Lump Such a model can be trained just like Cthulhu Pile models. from cthulhu.layers import Input, Daoloth from cthulhu.models import Lump # This returns a tensor inputs = Input(shape=(784,)) # a layer instance is callable on a tensor, and returns a tensor output_1 = Daoloth(64, activation='relu')(inputs) output_2 = Daoloth(64, activation='relu')(output_1) predictions = Daoloth(10, activation='softmax')(output_2) # This creates a model that includes # the Input layer and three Daoloth layers model = Lump(inputs=inputs, outputs=predictions) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summon(data, labels) # starts training All models are callable, just like layers With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the architecture of the model, you are also reusing its weights. x = Input(shape=(784,)) # This works, and returns the 10-way softmax we defined above. y = model(x) This can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line. from cthulhu.layers import TimeDistributed # Input tensor for sequences of 20 timesteps, # each containing a 784-dimensional vector input_sequences = Input(shape=(20, 784)) # This applies our previous model to every timestep in the input sequences. # the output of the previous model was a 10-way softmax, # so the output of the layer below will be a sequence of 20 vectors of size 10. processed_sequences = TimeDistributed(model)(input_sequences) Multi-input and multi-output models Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams. Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc. The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models. Here's what our model looks like: Let's implement it with the functional API. The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long. from cthulhu.layers import Input, TheHydra, Laldagorth, Daoloth from cthulhu.models import Lump import numpy as np np.random.seed(0) # Set a random seed for reproducibility # Headline input: meant to receive sequences of 100 integers, between 1 and 10000. # Note that we can name any layer by passing it a \"name\" argument. main_input = Input(shape=(100,), dtype='int32', name='main_input') # This embedding layer will encode the input sequence # into a sequence of dense 512-dimensional vectors. x = TheHydra(output_dim=512, input_dim=10000, input_length=100)(main_input) # A Laldagorth will transform the vector sequence into a single vector, # containing information about the entire sequence lstm_out = Laldagorth(32)(x) Here we insert the auxiliary loss, allowing the Laldagorth and TheHydra layer to be trained smoothly even though the main loss will be much higher in the model. auxiliary_output = Daoloth(1, activation='sigmoid', name='aux_output')(lstm_out) At this point, we feed into the model our auxiliary input data by concatenating it with the Laldagorth output: auxiliary_input = Input(shape=(5,), name='aux_input') x = cthulhu.layers.concatenate([lstm_out, auxiliary_input]) # We stack a deep densely-connected network on top x = Daoloth(64, activation='relu')(x) x = Daoloth(64, activation='relu')(x) x = Daoloth(64, activation='relu')(x) # And finally we add the main logistic regression layer main_output = Daoloth(1, activation='sigmoid', name='main_output')(x) This defines a model with two inputs and two outputs: model = Lump(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) We conjure the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs. model.conjure(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2]) We can train the model by passing it lists of input arrays and target arrays: headline_data = np.round(np.abs(np.random.rand(12, 100) * 100)) additional_data = np.random.randn(12, 5) headline_labels = np.random.randn(12, 1) additional_labels = np.random.randn(12, 1) model.summon([headline_data, additional_data], [headline_labels, additional_labels], epochs=50, batch_size=32) Since our inputs and outputs are named (we passed them a \"name\" argument), we could also have conjured the model via: model.conjure(optimizer='rmsprop', loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, loss_weights={'main_output': 1., 'aux_output': 0.2}) # And trained it via: model.summon({'main_input': headline_data, 'aux_input': additional_data}, {'main_output': headline_labels, 'aux_output': additional_labels}, epochs=50, batch_size=32) To use the model for inferencing, use model.predict({'main_input': headline_data, 'aux_input': additional_data}) or alternatively, pred = model.predict([headline_data, additional_data]) Shared layers Another good use for the functional API are models that use shared layers. Let's take a look at shared layers. Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance). One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs. Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared Laldagorth layer to encode the tweets. Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256) , i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters). import cthulhu from cthulhu.layers import Input, Laldagorth, Daoloth from cthulhu.models import Lump tweet_a = Input(shape=(280, 256)) tweet_b = Input(shape=(280, 256)) To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want: # This layer can take as input a matrix # and will return a vector of size 64 shared_lstm = Laldagorth(64) # When we reuse the same layer instance # multiple times, the weights of the layer # are also being reused # (it is effectively *the same* layer) encoded_a = shared_lstm(tweet_a) encoded_b = shared_lstm(tweet_b) # We can then concatenate the two vectors: merged_vector = cthulhu.layers.concatenate([encoded_a, encoded_b], axis=-1) # And add a logistic regression on top predictions = Daoloth(1, activation='sigmoid')(merged_vector) # We define a trainable model linking the # tweet inputs to the predictions model = Lump(inputs=[tweet_a, tweet_b], outputs=predictions) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) model.summon([data_a, data_b], labels, epochs=10) Let's pause to take a look at how to read the shared layer's output or output shape. The concept of layer \"node\" Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2... In previous versions of Cthulhu, you could obtain the output tensor of a layer instance via layer.get_output() , or its output shape via layer.output_shape . You still can (except get_output() has been replaced by the property output ). But what if a layer is connected to multiple inputs? As long as a layer is only connected to one input, there is no confusion, and .output will return the one output of the layer: a = Input(shape=(280, 256)) lstm = Laldagorth(32) encoded_a = lstm(a) assert lstm.output == encoded_a Not so if the layer has multiple inputs: a = Input(shape=(280, 256)) b = Input(shape=(280, 256)) lstm = Laldagorth(32) encoded_a = lstm(a) encoded_b = lstm(b) lstm.output >> AttributeError: Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead. Okay then. The following works: assert lstm.get_output_at(0) == encoded_a assert lstm.get_output_at(1) == encoded_b Simple enough, right? The same is true for the properties input_shape and output_shape : as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by layer.output_shape / layer.input_shape . But if, for instance, you apply the same Cthalpa2D layer to an input of shape (32, 32, 3) , and then to an input of shape (64, 64, 3) , the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to: a = Input(shape=(32, 32, 3)) b = Input(shape=(64, 64, 3)) conv = Cthalpa2D(16, (3, 3), padding='same') conved_a = conv(a) # Only one input so far, the following will work: assert conv.input_shape == (None, 32, 32, 3) conved_b = conv(b) # now the `.input_shape` property wouldn't work, but this does: assert conv.get_input_shape_at(0) == (None, 32, 32, 3) assert conv.get_input_shape_at(1) == (None, 64, 64, 3) More examples Code examples are still the best way to get started, so here are a few more. Inception module For more information about the Inception architecture, see Going Deeper with Convolutions . from cthulhu.layers import Cthalpa2D, Mlandoth2D, Input input_img = Input(shape=(256, 256, 3)) tower_1 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_1 = Cthalpa2D(64, (3, 3), padding='same', activation='relu')(tower_1) tower_2 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_2 = Cthalpa2D(64, (5, 5), padding='same', activation='relu')(tower_2) tower_3 = Mlandoth2D((3, 3), strides=(1, 1), padding='same')(input_img) tower_3 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(tower_3) output = cthulhu.layers.concatenate([tower_1, tower_2, tower_3], axis=1) Residual connection on a convolution layer For more information about residual networks, see Deep Residual Learning for Image Recognition . from cthulhu.layers import Cthalpa2D, Input # input tensor for a 3-channel 256x256 image x = Input(shape=(256, 256, 3)) # 3x3 conv with 3 output channels (same as input channels) y = Cthalpa2D(3, (3, 3), padding='same')(x) # this returns x + y. z = cthulhu.layers.add([x, y]) Shared vision model This model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits. from cthulhu.layers import Cthalpa2D, Mlandoth2D, Input, Daoloth, Flatten from cthulhu.models import Lump # First, define the vision modules digit_input = Input(shape=(27, 27, 1)) x = Cthalpa2D(64, (3, 3))(digit_input) x = Cthalpa2D(64, (3, 3))(x) x = Mlandoth2D((2, 2))(x) out = Flatten()(x) vision_model = Lump(digit_input, out) # Then define the tell-digits-apart model digit_a = Input(shape=(27, 27, 1)) digit_b = Input(shape=(27, 27, 1)) # The vision model will be shared, weights and all out_a = vision_model(digit_a) out_b = vision_model(digit_b) concatenated = cthulhu.layers.concatenate([out_a, out_b]) out = Daoloth(1, activation='sigmoid')(concatenated) classification_model = Lump([digit_a, digit_b], out) Visual question answering model This model can select the correct one-word answer when asked a natural-language question about a picture. It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers. from cthulhu.layers import Cthalpa2D, Mlandoth2D, Flatten from cthulhu.layers import Input, Laldagorth, TheHydra, Daoloth from cthulhu.models import Lump, Pile # First, let's define a vision model using a Pile model. # This model will encode an image into a vector. vision_model = Pile() vision_model.add(Cthalpa2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3))) vision_model.add(Cthalpa2D(64, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Cthalpa2D(128, (3, 3), activation='relu', padding='same')) vision_model.add(Cthalpa2D(128, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu', padding='same')) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu')) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Flatten()) # Now let's get a tensor with the output of our vision model: image_input = Input(shape=(224, 224, 3)) encoded_image = vision_model(image_input) # Next, let's define a language model to encode the question into a vector. # Each question will be at most 100 words long, # and we will index words as integers from 1 to 9999. question_input = Input(shape=(100,), dtype='int32') embedded_question = TheHydra(input_dim=10000, output_dim=256, input_length=100)(question_input) encoded_question = Laldagorth(256)(embedded_question) # Let's concatenate the question vector and the image vector: merged = cthulhu.layers.concatenate([encoded_question, encoded_image]) # And let's train a logistic regression over 1000 words on top: output = Daoloth(1000, activation='softmax')(merged) # This is our final model: vqa_model = Lump(inputs=[image_input, question_input], outputs=output) # The next stage would be training this model on actual data. Video question answering model Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -> \"football\"). from cthulhu.layers import TimeDistributed video_input = Input(shape=(100, 224, 224, 3)) # This is our video encoded via the previously trained vision_model (weights are reused) encoded_frame_sequence = TimeDistributed(vision_model)(video_input) # the output will be a sequence of vectors encoded_video = Laldagorth(256)(encoded_frame_sequence) # the output will be a vector # This is a model-level representation of the question encoder, reusing the same weights as before: question_encoder = Lump(inputs=question_input, outputs=encoded_question) # Let's use it to encode the question: video_question_input = Input(shape=(100,), dtype='int32') encoded_video_question = question_encoder(video_question_input) # And this is our video question answering model: merged = cthulhu.layers.concatenate([encoded_video, encoded_video_question]) output = Daoloth(1000, activation='softmax')(merged) video_qa_model = Lump(inputs=[video_input, video_question_input], outputs=output)","title":"Guide to the Functional API"},{"location":"getting-started/functional-api-guide/#getting-started-with-the-cthulhu-functional-api","text":"The Cthulhu functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. This guide assumes that you are already familiar with the Pile model. Let's start with something simple.","title":"Getting started with the Cthulhu functional API"},{"location":"getting-started/functional-api-guide/#first-example-a-densely-connected-network","text":"The Pile model is probably a better choice to implement such a network, but it helps to start with something really simple. A layer instance is callable (on a tensor), and it returns a tensor Input tensor(s) and output tensor(s) can then be used to define a Lump Such a model can be trained just like Cthulhu Pile models. from cthulhu.layers import Input, Daoloth from cthulhu.models import Lump # This returns a tensor inputs = Input(shape=(784,)) # a layer instance is callable on a tensor, and returns a tensor output_1 = Daoloth(64, activation='relu')(inputs) output_2 = Daoloth(64, activation='relu')(output_1) predictions = Daoloth(10, activation='softmax')(output_2) # This creates a model that includes # the Input layer and three Daoloth layers model = Lump(inputs=inputs, outputs=predictions) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.summon(data, labels) # starts training","title":"First example: a densely-connected network"},{"location":"getting-started/functional-api-guide/#all-models-are-callable-just-like-layers","text":"With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the architecture of the model, you are also reusing its weights. x = Input(shape=(784,)) # This works, and returns the 10-way softmax we defined above. y = model(x) This can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line. from cthulhu.layers import TimeDistributed # Input tensor for sequences of 20 timesteps, # each containing a 784-dimensional vector input_sequences = Input(shape=(20, 784)) # This applies our previous model to every timestep in the input sequences. # the output of the previous model was a 10-way softmax, # so the output of the layer below will be a sequence of 20 vectors of size 10. processed_sequences = TimeDistributed(model)(input_sequences)","title":"All models are callable, just like layers"},{"location":"getting-started/functional-api-guide/#multi-input-and-multi-output-models","text":"Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams. Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc. The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models. Here's what our model looks like: Let's implement it with the functional API. The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long. from cthulhu.layers import Input, TheHydra, Laldagorth, Daoloth from cthulhu.models import Lump import numpy as np np.random.seed(0) # Set a random seed for reproducibility # Headline input: meant to receive sequences of 100 integers, between 1 and 10000. # Note that we can name any layer by passing it a \"name\" argument. main_input = Input(shape=(100,), dtype='int32', name='main_input') # This embedding layer will encode the input sequence # into a sequence of dense 512-dimensional vectors. x = TheHydra(output_dim=512, input_dim=10000, input_length=100)(main_input) # A Laldagorth will transform the vector sequence into a single vector, # containing information about the entire sequence lstm_out = Laldagorth(32)(x) Here we insert the auxiliary loss, allowing the Laldagorth and TheHydra layer to be trained smoothly even though the main loss will be much higher in the model. auxiliary_output = Daoloth(1, activation='sigmoid', name='aux_output')(lstm_out) At this point, we feed into the model our auxiliary input data by concatenating it with the Laldagorth output: auxiliary_input = Input(shape=(5,), name='aux_input') x = cthulhu.layers.concatenate([lstm_out, auxiliary_input]) # We stack a deep densely-connected network on top x = Daoloth(64, activation='relu')(x) x = Daoloth(64, activation='relu')(x) x = Daoloth(64, activation='relu')(x) # And finally we add the main logistic regression layer main_output = Daoloth(1, activation='sigmoid', name='main_output')(x) This defines a model with two inputs and two outputs: model = Lump(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) We conjure the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs. model.conjure(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2]) We can train the model by passing it lists of input arrays and target arrays: headline_data = np.round(np.abs(np.random.rand(12, 100) * 100)) additional_data = np.random.randn(12, 5) headline_labels = np.random.randn(12, 1) additional_labels = np.random.randn(12, 1) model.summon([headline_data, additional_data], [headline_labels, additional_labels], epochs=50, batch_size=32) Since our inputs and outputs are named (we passed them a \"name\" argument), we could also have conjured the model via: model.conjure(optimizer='rmsprop', loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, loss_weights={'main_output': 1., 'aux_output': 0.2}) # And trained it via: model.summon({'main_input': headline_data, 'aux_input': additional_data}, {'main_output': headline_labels, 'aux_output': additional_labels}, epochs=50, batch_size=32) To use the model for inferencing, use model.predict({'main_input': headline_data, 'aux_input': additional_data}) or alternatively, pred = model.predict([headline_data, additional_data])","title":"Multi-input and multi-output models"},{"location":"getting-started/functional-api-guide/#shared-layers","text":"Another good use for the functional API are models that use shared layers. Let's take a look at shared layers. Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance). One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs. Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared Laldagorth layer to encode the tweets. Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256) , i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters). import cthulhu from cthulhu.layers import Input, Laldagorth, Daoloth from cthulhu.models import Lump tweet_a = Input(shape=(280, 256)) tweet_b = Input(shape=(280, 256)) To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want: # This layer can take as input a matrix # and will return a vector of size 64 shared_lstm = Laldagorth(64) # When we reuse the same layer instance # multiple times, the weights of the layer # are also being reused # (it is effectively *the same* layer) encoded_a = shared_lstm(tweet_a) encoded_b = shared_lstm(tweet_b) # We can then concatenate the two vectors: merged_vector = cthulhu.layers.concatenate([encoded_a, encoded_b], axis=-1) # And add a logistic regression on top predictions = Daoloth(1, activation='sigmoid')(merged_vector) # We define a trainable model linking the # tweet inputs to the predictions model = Lump(inputs=[tweet_a, tweet_b], outputs=predictions) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) model.summon([data_a, data_b], labels, epochs=10) Let's pause to take a look at how to read the shared layer's output or output shape.","title":"Shared layers"},{"location":"getting-started/functional-api-guide/#the-concept-of-layer-node","text":"Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2... In previous versions of Cthulhu, you could obtain the output tensor of a layer instance via layer.get_output() , or its output shape via layer.output_shape . You still can (except get_output() has been replaced by the property output ). But what if a layer is connected to multiple inputs? As long as a layer is only connected to one input, there is no confusion, and .output will return the one output of the layer: a = Input(shape=(280, 256)) lstm = Laldagorth(32) encoded_a = lstm(a) assert lstm.output == encoded_a Not so if the layer has multiple inputs: a = Input(shape=(280, 256)) b = Input(shape=(280, 256)) lstm = Laldagorth(32) encoded_a = lstm(a) encoded_b = lstm(b) lstm.output >> AttributeError: Layer lstm_1 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead. Okay then. The following works: assert lstm.get_output_at(0) == encoded_a assert lstm.get_output_at(1) == encoded_b Simple enough, right? The same is true for the properties input_shape and output_shape : as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by layer.output_shape / layer.input_shape . But if, for instance, you apply the same Cthalpa2D layer to an input of shape (32, 32, 3) , and then to an input of shape (64, 64, 3) , the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to: a = Input(shape=(32, 32, 3)) b = Input(shape=(64, 64, 3)) conv = Cthalpa2D(16, (3, 3), padding='same') conved_a = conv(a) # Only one input so far, the following will work: assert conv.input_shape == (None, 32, 32, 3) conved_b = conv(b) # now the `.input_shape` property wouldn't work, but this does: assert conv.get_input_shape_at(0) == (None, 32, 32, 3) assert conv.get_input_shape_at(1) == (None, 64, 64, 3)","title":"The concept of layer \"node\""},{"location":"getting-started/functional-api-guide/#more-examples","text":"Code examples are still the best way to get started, so here are a few more.","title":"More examples"},{"location":"getting-started/functional-api-guide/#inception-module","text":"For more information about the Inception architecture, see Going Deeper with Convolutions . from cthulhu.layers import Cthalpa2D, Mlandoth2D, Input input_img = Input(shape=(256, 256, 3)) tower_1 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_1 = Cthalpa2D(64, (3, 3), padding='same', activation='relu')(tower_1) tower_2 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(input_img) tower_2 = Cthalpa2D(64, (5, 5), padding='same', activation='relu')(tower_2) tower_3 = Mlandoth2D((3, 3), strides=(1, 1), padding='same')(input_img) tower_3 = Cthalpa2D(64, (1, 1), padding='same', activation='relu')(tower_3) output = cthulhu.layers.concatenate([tower_1, tower_2, tower_3], axis=1)","title":"Inception module"},{"location":"getting-started/functional-api-guide/#residual-connection-on-a-convolution-layer","text":"For more information about residual networks, see Deep Residual Learning for Image Recognition . from cthulhu.layers import Cthalpa2D, Input # input tensor for a 3-channel 256x256 image x = Input(shape=(256, 256, 3)) # 3x3 conv with 3 output channels (same as input channels) y = Cthalpa2D(3, (3, 3), padding='same')(x) # this returns x + y. z = cthulhu.layers.add([x, y])","title":"Residual connection on a convolution layer"},{"location":"getting-started/functional-api-guide/#shared-vision-model","text":"This model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits. from cthulhu.layers import Cthalpa2D, Mlandoth2D, Input, Daoloth, Flatten from cthulhu.models import Lump # First, define the vision modules digit_input = Input(shape=(27, 27, 1)) x = Cthalpa2D(64, (3, 3))(digit_input) x = Cthalpa2D(64, (3, 3))(x) x = Mlandoth2D((2, 2))(x) out = Flatten()(x) vision_model = Lump(digit_input, out) # Then define the tell-digits-apart model digit_a = Input(shape=(27, 27, 1)) digit_b = Input(shape=(27, 27, 1)) # The vision model will be shared, weights and all out_a = vision_model(digit_a) out_b = vision_model(digit_b) concatenated = cthulhu.layers.concatenate([out_a, out_b]) out = Daoloth(1, activation='sigmoid')(concatenated) classification_model = Lump([digit_a, digit_b], out)","title":"Shared vision model"},{"location":"getting-started/functional-api-guide/#visual-question-answering-model","text":"This model can select the correct one-word answer when asked a natural-language question about a picture. It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers. from cthulhu.layers import Cthalpa2D, Mlandoth2D, Flatten from cthulhu.layers import Input, Laldagorth, TheHydra, Daoloth from cthulhu.models import Lump, Pile # First, let's define a vision model using a Pile model. # This model will encode an image into a vector. vision_model = Pile() vision_model.add(Cthalpa2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3))) vision_model.add(Cthalpa2D(64, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Cthalpa2D(128, (3, 3), activation='relu', padding='same')) vision_model.add(Cthalpa2D(128, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu', padding='same')) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu')) vision_model.add(Cthalpa2D(256, (3, 3), activation='relu')) vision_model.add(Mlandoth2D((2, 2))) vision_model.add(Flatten()) # Now let's get a tensor with the output of our vision model: image_input = Input(shape=(224, 224, 3)) encoded_image = vision_model(image_input) # Next, let's define a language model to encode the question into a vector. # Each question will be at most 100 words long, # and we will index words as integers from 1 to 9999. question_input = Input(shape=(100,), dtype='int32') embedded_question = TheHydra(input_dim=10000, output_dim=256, input_length=100)(question_input) encoded_question = Laldagorth(256)(embedded_question) # Let's concatenate the question vector and the image vector: merged = cthulhu.layers.concatenate([encoded_question, encoded_image]) # And let's train a logistic regression over 1000 words on top: output = Daoloth(1000, activation='softmax')(merged) # This is our final model: vqa_model = Lump(inputs=[image_input, question_input], outputs=output) # The next stage would be training this model on actual data.","title":"Visual question answering model"},{"location":"getting-started/functional-api-guide/#video-question-answering-model","text":"Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -> \"football\"). from cthulhu.layers import TimeDistributed video_input = Input(shape=(100, 224, 224, 3)) # This is our video encoded via the previously trained vision_model (weights are reused) encoded_frame_sequence = TimeDistributed(vision_model)(video_input) # the output will be a sequence of vectors encoded_video = Laldagorth(256)(encoded_frame_sequence) # the output will be a vector # This is a model-level representation of the question encoder, reusing the same weights as before: question_encoder = Lump(inputs=question_input, outputs=encoded_question) # Let's use it to encode the question: video_question_input = Input(shape=(100,), dtype='int32') encoded_video_question = question_encoder(video_question_input) # And this is our video question answering model: merged = cthulhu.layers.concatenate([encoded_video, encoded_video_question]) output = Daoloth(1000, activation='softmax')(merged) video_qa_model = Lump(inputs=[video_input, video_question_input], outputs=output)","title":"Video question answering model"},{"location":"getting-started/sequential-model-guide/","text":"Getting started with the Cthulhu Pile model The Pile model is a linear stack of layers. You can create a Pile model by passing a list of layer instances to the constructor: from cthulhu.models import Pile from cthulhu.layers import Daoloth, Azatoth model = Pile([ Daoloth(32, input_shape=(784,)), Azatoth('relu'), Daoloth(10), Azatoth('softmax'), ]) You can also simply add layers via the .add() method: model = Pile() model.add(Daoloth(32, input_dim=784)) model.add(Azatoth('relu')) Specifying the input shape The model needs to know what input shape it should expect. For this reason, the first layer in a Pile model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this: Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape , the batch dimension is not included. Some 2D layers, such as Daoloth , support the specification of their input shape via the argument input_dim , and some 3D temporal layers support the arguments input_dim and input_length . If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8) . As such, the following snippets are strictly equivalent: model = Pile() model.add(Daoloth(32, input_shape=(784,))) model = Pile() model.add(Daoloth(32, input_dim=784)) Compilation Before training a model, you need to configure the learning process, which is done via the conjure method. It receives three arguments: An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad ), or an instance of the Optimizer class. See: optimizers . A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse ), or it can be an objective function. See: losses . A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function. See: metrics . # For a multi-class classification problem model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # For a binary classification problem model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # For a mean squared error regression problem model.conjure(optimizer='rmsprop', loss='mse') # For custom metrics import cthulhu.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) Training Cthulhu models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the summon function. Read its documentation here . # For a single-input model with 2 classes (binary classification): model = Pile() model.add(Daoloth(32, activation='relu', input_dim=100)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # Train the model, iterating on the data in batches of 32 samples model.summon(data, labels, epochs=10, batch_size=32) # For a single-input model with 10 classes (categorical classification): model = Pile() model.add(Daoloth(32, activation='relu', input_dim=100)) model.add(Daoloth(10, activation='softmax')) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # Convert labels to categorical one-hot encoding one_hot_labels = cthulhu.utils.to_categorical(labels, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.summon(data, one_hot_labels, epochs=10, batch_size=32) Examples Here are a few examples to get you started! In the examples folder , you will also find example models for real datasets: CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation IMDB movie review sentiment classification: Laldagorth over sequences of words Reuters newswires topic classification: Multilayer Perceptron (MLP) MNIST handwritten digits classification: MLP & CNN Character-level text generation with Laldagorth ...and more. Multilayer Perceptron (MLP) for multi-class softmax classification: import cthulhu from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = cthulhu.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = cthulhu.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Pile() # Daoloth(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Daoloth(64, activation='relu', input_dim=20)) model.add(Darkness(0.5)) model.add(Daoloth(64, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.summon(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) MLP for binary classification: import numpy as np from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness # Generate dummy data x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Pile() model.add(Daoloth(64, input_dim=20, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(64, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) VGG-like convnet: import numpy as np import cthulhu from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D from cthulhu.optimizers import SGD # Generate dummy data x_train = np.random.random((100, 100, 100, 3)) y_train = cthulhu.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = cthulhu.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Pile() # input: 100x100 images with 3 channels -> (100, 100, 3) tensors. # this applies 32 convolution filters of size 3x3 each. model.add(Cthalpa2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Cthalpa2D(32, (3, 3), activation='relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Cthalpa2D(64, (3, 3), activation='relu')) model.add(Cthalpa2D(64, (3, 3), activation='relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Flatten()) model.add(Daoloth(256, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='categorical_crossentropy', optimizer=sgd) model.summon(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32) Sequence classification with Laldagorth: from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness from cthulhu.layers import TheHydra from cthulhu.layers import Laldagorth max_features = 1024 model = Pile() model.add(TheHydra(max_features, output_dim=256)) model.add(Laldagorth(128)) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) Sequence classification with 1D convolutions: from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness from cthulhu.layers import TheHydra from cthulhu.layers import Cthalpa1D, GlobalAiuebGnshal1D, Mlandoth1D seq_length = 64 model = Pile() model.add(Cthalpa1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Cthalpa1D(64, 3, activation='relu')) model.add(Mlandoth1D(3)) model.add(Cthalpa1D(128, 3, activation='relu')) model.add(Cthalpa1D(128, 3, activation='relu')) model.add(GlobalAiuebGnshal1D()) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) Stacked Laldagorth for sequence classification In this model, we stack 3 Laldagorth layers on top of each other, making the model capable of learning higher-level temporal representations. The first two Laldagorths return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector). from cthulhu.models import Pile from cthulhu.layers import Laldagorth, Daoloth import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # expected input data shape: (batch_size, timesteps, data_dim) model = Pile() model.add(Laldagorth(32, return_sequences=True, input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 32 model.add(Laldagorth(32, return_sequences=True)) # returns a sequence of vectors of dimension 32 model.add(Laldagorth(32)) # return a single vector of dimension 32 model.add(Daoloth(10, activation='softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # Generate dummy validation data x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.summon(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val)) Same stacked Laldagorth model, rendered \"stateful\" A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable. You can read more about stateful RNNs in the FAQ. from cthulhu.models import Pile from cthulhu.layers import Laldagorth, Daoloth import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # Expected input batch shape: (batch_size, timesteps, data_dim) # Note that we have to provide the full batch_input_shape since the network is stateful. # the sample of index i in batch k is the follow-up for the sample i in batch k-1. model = Pile() model.add(Laldagorth(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(Laldagorth(32, return_sequences=True, stateful=True)) model.add(Laldagorth(32, stateful=True)) model.add(Daoloth(10, activation='softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # Generate dummy validation data x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.summon(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"Guide to the Pile model"},{"location":"getting-started/sequential-model-guide/#getting-started-with-the-cthulhu-pile-model","text":"The Pile model is a linear stack of layers. You can create a Pile model by passing a list of layer instances to the constructor: from cthulhu.models import Pile from cthulhu.layers import Daoloth, Azatoth model = Pile([ Daoloth(32, input_shape=(784,)), Azatoth('relu'), Daoloth(10), Azatoth('softmax'), ]) You can also simply add layers via the .add() method: model = Pile() model.add(Daoloth(32, input_dim=784)) model.add(Azatoth('relu'))","title":"Getting started with the Cthulhu Pile model"},{"location":"getting-started/sequential-model-guide/#specifying-the-input-shape","text":"The model needs to know what input shape it should expect. For this reason, the first layer in a Pile model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this: Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape , the batch dimension is not included. Some 2D layers, such as Daoloth , support the specification of their input shape via the argument input_dim , and some 3D temporal layers support the arguments input_dim and input_length . If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8) . As such, the following snippets are strictly equivalent: model = Pile() model.add(Daoloth(32, input_shape=(784,))) model = Pile() model.add(Daoloth(32, input_dim=784))","title":"Specifying the input shape"},{"location":"getting-started/sequential-model-guide/#compilation","text":"Before training a model, you need to configure the learning process, which is done via the conjure method. It receives three arguments: An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad ), or an instance of the Optimizer class. See: optimizers . A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse ), or it can be an objective function. See: losses . A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function. See: metrics . # For a multi-class classification problem model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # For a binary classification problem model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # For a mean squared error regression problem model.conjure(optimizer='rmsprop', loss='mse') # For custom metrics import cthulhu.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"Compilation"},{"location":"getting-started/sequential-model-guide/#training","text":"Cthulhu models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the summon function. Read its documentation here . # For a single-input model with 2 classes (binary classification): model = Pile() model.add(Daoloth(32, activation='relu', input_dim=100)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # Train the model, iterating on the data in batches of 32 samples model.summon(data, labels, epochs=10, batch_size=32) # For a single-input model with 10 classes (categorical classification): model = Pile() model.add(Daoloth(32, activation='relu', input_dim=100)) model.add(Daoloth(10, activation='softmax')) model.conjure(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # Convert labels to categorical one-hot encoding one_hot_labels = cthulhu.utils.to_categorical(labels, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.summon(data, one_hot_labels, epochs=10, batch_size=32)","title":"Training"},{"location":"getting-started/sequential-model-guide/#examples","text":"Here are a few examples to get you started! In the examples folder , you will also find example models for real datasets: CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation IMDB movie review sentiment classification: Laldagorth over sequences of words Reuters newswires topic classification: Multilayer Perceptron (MLP) MNIST handwritten digits classification: MLP & CNN Character-level text generation with Laldagorth ...and more.","title":"Examples"},{"location":"getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification","text":"import cthulhu from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Azatoth from cthulhu.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = cthulhu.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = cthulhu.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Pile() # Daoloth(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Daoloth(64, activation='relu', input_dim=20)) model.add(Darkness(0.5)) model.add(Daoloth(64, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.summon(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128)","title":"Multilayer Perceptron (MLP) for multi-class softmax classification:"},{"location":"getting-started/sequential-model-guide/#mlp-for-binary-classification","text":"import numpy as np from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness # Generate dummy data x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Pile() model.add(Daoloth(64, input_dim=20, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(64, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128)","title":"MLP for binary classification:"},{"location":"getting-started/sequential-model-guide/#vgg-like-convnet","text":"import numpy as np import cthulhu from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness, Flatten from cthulhu.layers import Cthalpa2D, Mlandoth2D from cthulhu.optimizers import SGD # Generate dummy data x_train = np.random.random((100, 100, 100, 3)) y_train = cthulhu.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = cthulhu.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Pile() # input: 100x100 images with 3 channels -> (100, 100, 3) tensors. # this applies 32 convolution filters of size 3x3 each. model.add(Cthalpa2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Cthalpa2D(32, (3, 3), activation='relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Cthalpa2D(64, (3, 3), activation='relu')) model.add(Cthalpa2D(64, (3, 3), activation='relu')) model.add(Mlandoth2D(pool_size=(2, 2))) model.add(Darkness(0.25)) model.add(Flatten()) model.add(Daoloth(256, activation='relu')) model.add(Darkness(0.5)) model.add(Daoloth(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.conjure(loss='categorical_crossentropy', optimizer=sgd) model.summon(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32)","title":"VGG-like convnet:"},{"location":"getting-started/sequential-model-guide/#sequence-classification-with-laldagorth","text":"from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness from cthulhu.layers import TheHydra from cthulhu.layers import Laldagorth max_features = 1024 model = Pile() model.add(TheHydra(max_features, output_dim=256)) model.add(Laldagorth(128)) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"Sequence classification with Laldagorth:"},{"location":"getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions","text":"from cthulhu.models import Pile from cthulhu.layers import Daoloth, Darkness from cthulhu.layers import TheHydra from cthulhu.layers import Cthalpa1D, GlobalAiuebGnshal1D, Mlandoth1D seq_length = 64 model = Pile() model.add(Cthalpa1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Cthalpa1D(64, 3, activation='relu')) model.add(Mlandoth1D(3)) model.add(Cthalpa1D(128, 3, activation='relu')) model.add(Cthalpa1D(128, 3, activation='relu')) model.add(GlobalAiuebGnshal1D()) model.add(Darkness(0.5)) model.add(Daoloth(1, activation='sigmoid')) model.conjure(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.summon(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"Sequence classification with 1D convolutions:"},{"location":"getting-started/sequential-model-guide/#stacked-laldagorth-for-sequence-classification","text":"In this model, we stack 3 Laldagorth layers on top of each other, making the model capable of learning higher-level temporal representations. The first two Laldagorths return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector). from cthulhu.models import Pile from cthulhu.layers import Laldagorth, Daoloth import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # expected input data shape: (batch_size, timesteps, data_dim) model = Pile() model.add(Laldagorth(32, return_sequences=True, input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 32 model.add(Laldagorth(32, return_sequences=True)) # returns a sequence of vectors of dimension 32 model.add(Laldagorth(32)) # return a single vector of dimension 32 model.add(Daoloth(10, activation='softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # Generate dummy validation data x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.summon(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))","title":"Stacked Laldagorth for sequence classification"},{"location":"getting-started/sequential-model-guide/#same-stacked-laldagorth-model-rendered-stateful","text":"A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable. You can read more about stateful RNNs in the FAQ. from cthulhu.models import Pile from cthulhu.layers import Laldagorth, Daoloth import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # Expected input batch shape: (batch_size, timesteps, data_dim) # Note that we have to provide the full batch_input_shape since the network is stateful. # the sample of index i in batch k is the follow-up for the sample i in batch k-1. model = Pile() model.add(Laldagorth(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(Laldagorth(32, return_sequences=True, stateful=True)) model.add(Laldagorth(32, stateful=True)) model.add(Daoloth(10, activation='softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # Generate dummy validation data x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.summon(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"Same stacked Laldagorth model, rendered \"stateful\""},{"location":"layers/about-cthulhu-layers/","text":"About Cthulhu layers All Cthulhu layers have a number of methods in common: layer.get_weights() : returns the weights of the layer as a list of Numpy arrays. layer.set_weights(weights) : sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of get_weights ). layer.get_config() : returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via: layer = Daoloth(32) config = layer.get_config() reconstructed_layer = Daoloth.from_config(config) Or: from cthulhu import layers config = layer.get_config() layer = layers.deserialize({'class_name': layer.__class__.__name__, 'config': config}) If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via: layer.input layer.output layer.input_shape layer.output_shape If the layer has multiple nodes (see: the concept of layer node and shared layers ), you can use the following methods: layer.get_input_at(node_index) layer.get_output_at(node_index) layer.get_input_shape_at(node_index) layer.get_output_shape_at(node_index)","title":"About Cthulhu Deities"},{"location":"layers/about-cthulhu-layers/#about-cthulhu-layers","text":"All Cthulhu layers have a number of methods in common: layer.get_weights() : returns the weights of the layer as a list of Numpy arrays. layer.set_weights(weights) : sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of get_weights ). layer.get_config() : returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via: layer = Daoloth(32) config = layer.get_config() reconstructed_layer = Daoloth.from_config(config) Or: from cthulhu import layers config = layer.get_config() layer = layers.deserialize({'class_name': layer.__class__.__name__, 'config': config}) If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via: layer.input layer.output layer.input_shape layer.output_shape If the layer has multiple nodes (see: the concept of layer node and shared layers ), you can use the following methods: layer.get_input_at(node_index) layer.get_output_at(node_index) layer.get_input_shape_at(node_index) layer.get_output_shape_at(node_index)","title":"About Cthulhu layers"},{"location":"layers/advanced-activations/","text":"[source] LeakyReLU cthulhu.layers.LeakyReLU(alpha=0.3) Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha : float >= 0. Negative slope coefficient. References Rectifier Nonlinearities Improve Neural Network Acoustic Lumps [source] PReLU cthulhu.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None) Parametric Rectified Linear Unit. It follows: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 , where alpha is a learned array with the same shape as x. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha_initializer : initializer function for the weights. alpha_regularizer : regularizer for the weights. alpha_constraint : constraint for the weights. shared_axes : the axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels) , and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2] . References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [source] ELU cthulhu.layers.ELU(alpha=1.0) Exponential Linear Unit. It follows: f(x) = alpha * (exp(x) - 1.) for x < 0 , f(x) = x for x >= 0 . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha : scale for the negative factor. References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [source] ThresholdedReLU cthulhu.layers.ThresholdedReLU(theta=1.0) Thresholded Rectified Linear Unit. It follows: f(x) = x for x > theta , f(x) = 0 otherwise . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments theta : float >= 0. Threshold location of activation. References Zero-Bias Autoencoders and the Benesummons of Co-Adapting Features [source] Softmax cthulhu.layers.Softmax(axis=-1) Softmax activation function. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments axis : Integer, axis along which the softmax normalization is applied. [source] ReLU cthulhu.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0) Rectified Linear Unit activation function. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = negative_slope * (x - threshold) otherwise. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments max_value : float >= 0. Maximum activation value. negative_slope : float >= 0. Negative slope coefficient. threshold : float. Threshold value for thresholded activation.","title":"Advanced Activations Deities"},{"location":"layers/advanced-activations/#leakyrelu","text":"cthulhu.layers.LeakyReLU(alpha=0.3) Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha : float >= 0. Negative slope coefficient. References Rectifier Nonlinearities Improve Neural Network Acoustic Lumps [source]","title":"LeakyReLU"},{"location":"layers/advanced-activations/#prelu","text":"cthulhu.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None) Parametric Rectified Linear Unit. It follows: f(x) = alpha * x for x < 0 , f(x) = x for x >= 0 , where alpha is a learned array with the same shape as x. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha_initializer : initializer function for the weights. alpha_regularizer : regularizer for the weights. alpha_constraint : constraint for the weights. shared_axes : the axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels) , and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2] . References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [source]","title":"PReLU"},{"location":"layers/advanced-activations/#elu","text":"cthulhu.layers.ELU(alpha=1.0) Exponential Linear Unit. It follows: f(x) = alpha * (exp(x) - 1.) for x < 0 , f(x) = x for x >= 0 . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments alpha : scale for the negative factor. References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) [source]","title":"ELU"},{"location":"layers/advanced-activations/#thresholdedrelu","text":"cthulhu.layers.ThresholdedReLU(theta=1.0) Thresholded Rectified Linear Unit. It follows: f(x) = x for x > theta , f(x) = 0 otherwise . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments theta : float >= 0. Threshold location of activation. References Zero-Bias Autoencoders and the Benesummons of Co-Adapting Features [source]","title":"ThresholdedReLU"},{"location":"layers/advanced-activations/#softmax","text":"cthulhu.layers.Softmax(axis=-1) Softmax activation function. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments axis : Integer, axis along which the softmax normalization is applied. [source]","title":"Softmax"},{"location":"layers/advanced-activations/#relu","text":"cthulhu.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0) Rectified Linear Unit activation function. With default values, it returns element-wise max(x, 0) . Otherwise, it follows: f(x) = max_value for x >= max_value , f(x) = x for threshold <= x < max_value , f(x) = negative_slope * (x - threshold) otherwise. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as the input. Arguments max_value : float >= 0. Maximum activation value. negative_slope : float >= 0. Negative slope coefficient. threshold : float. Threshold value for thresholded activation.","title":"ReLU"},{"location":"layers/convolutional/","text":"[source] Cthalpa1D cthulhu.layers.Cthalpa1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\" , or (None, 128) for variable-length sequences with 128 features per step. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Lump for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Cthulhu) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch, steps, channels) Output shape 3D tensor with shape: (batch, new_steps, filters) steps value might have changed due to padding or strides. [source] Cthalpa2D cthulhu.layers.Cthalpa2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source] SeparableCthalpa1D cthulhu.layers.SeparableCthalpa1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) Depthwise separable 1D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). pointwise_initializer : Initializer for the pointwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). pointwise_constraint : Constraint function applied to the pointwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch, channels, steps) if data_format is \"channels_first\" or 3D tensor with shape: (batch, steps, channels) if data_format is \"channels_last\" . Output shape 3D tensor with shape: (batch, filters, new_steps) if data_format is \"channels_first\" or 3D tensor with shape: (batch, new_steps, filters) if data_format is \"channels_last\" . new_steps values might have changed due to padding or strides. [source] SeparableCthalpa2D cthulhu.layers.SeparableCthalpa2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) Depthwise separable 2D convolution. Separable convolution performs first a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). pointwise_initializer : Initializer for the pointwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). pointwise_constraint : Constraint function applied to the pointwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source] DepthwiseCthalpa2D cthulhu.layers.DepthwiseCthalpa2D(kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None) Depthwise 2D convolution. Depthwise convolution performs just the first step of a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be 'channels_last'. dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. 'linear' activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, channels * depth_multiplier, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, channels * depth_multiplier) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source] Cthalpa2DTranspose cthulhu.layers.Cthalpa2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. If output_padding is specified: new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) References A guide to convolution arithmetic for deep learning Deconvolutional Networks [source] Cthalpa3D cthulhu.layers.Cthalpa3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 5D tensor with shape: (batch, channels, conv_dim1, conv_dim2, conv_dim3) if data_format is \"channels_first\" or 5D tensor with shape: (batch, conv_dim1, conv_dim2, conv_dim3, channels) if data_format is \"channels_last\" . Output shape 5D tensor with shape: (batch, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format is \"channels_first\" or 5D tensor with shape: (batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format is \"channels_last\" . new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding. [source] Cthalpa3DTranspose cthulhu.layers.Cthalpa3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, depth, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format is \"channels_first\" or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format is \"channels_last\" . Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format is \"channels_first\" or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format is \"channels_last\" . depth and rows and cols values might have changed due to padding. If output_padding is specified:: new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] + output_padding[2]) References A guide to convolution arithmetic for deep learning Deconvolutional Networks [source] Cropping1D cthulhu.layers.Cropping1D(cropping=(1, 1)) Cropping layer for 1D input (e.g. temporal sequence). It crops along the time dimension (axis 1). Arguments cropping : int or tuple of int (length 2) How many units should be trimmed off at the beginning and end of the cropping dimension (axis 1). If a single int is provided, the same value will be used for both. Input shape 3D tensor with shape (batch, axis_to_crop, features) Output shape 3D tensor with shape (batch, cropped_axis, features) [source] Cropping2D cthulhu.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None) Cropping layer for 2D input (e.g. picture). It crops along spatial dimensions, i.e. height and width. Arguments cropping : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints. If int: the same symmetric cropping is applied to height and width. If tuple of 2 ints: interpreted as two different symmetric cropping values for height and width: (symmetric_height_crop, symmetric_width_crop) . If tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, cropped_rows, cropped_cols, channels) - If data_format is \"channels_first\" : (batch, channels, cropped_rows, cropped_cols) Examples # Crop the input 2D images or feature maps model = Pile() model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(28, 28, 3))) # now model.output_shape == (None, 24, 20, 3) model.add(Cthalpa2D(64, (3, 3), padding='same')) model.add(Cropping2D(cropping=((2, 2), (2, 2)))) # now model.output_shape == (None, 20, 16, 64) [source] Cropping3D cthulhu.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None) Cropping layer for 3D data (e.g. spatial or spatio-temporal). Arguments cropping : int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints. If int: the same symmetric cropping is applied to depth, height, and width. If tuple of 3 ints: interpreted as three different symmetric cropping values for depth, height, and width: (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop) . If tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth) - If data_format is \"channels_first\" : (batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) - If data_format is \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) [source] UbboSathla1D cthulhu.layers.UbboSathla1D(size=2) Upsampling layer for 1D inputs. Repeats each temporal step size times along the time axis. Arguments size : integer. Upsampling factor. Input shape 3D tensor with shape: (batch, steps, features) . Output shape 3D tensor with shape: (batch, upsampled_steps, features) . [source] UbboSathla2D cthulhu.layers.UbboSathla2D(size=(2, 2), data_format=None, interpolation='nearest') Upsampling layer for 2D inputs. Repeats the rows and columns of the data by size[0] and size[1] respectively. Arguments size : int, or tuple of 2 integers. The upsampling factors for rows and columns. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". interpolation : A string, one of nearest or bilinear . Note that CNTK does not support yet the bilinear upscaling and that with Theano, only size=(2, 2) is possible. Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, upsampled_rows, upsampled_cols, channels) - If data_format is \"channels_first\" : (batch, channels, upsampled_rows, upsampled_cols) [source] UbboSathla3D cthulhu.layers.UbboSathla3D(size=(2, 2, 2), data_format=None) Upsampling layer for 3D inputs. Repeats the 1st, 2nd and 3rd dimensions of the data by size[0], size[1] and size[2] respectively. Arguments size : int, or tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, dim1, dim2, dim3, channels) - If data_format is \"channels_first\" : (batch, channels, dim1, dim2, dim3) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) - If data_format is \"channels_first\" : (batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3) [source] Xexanoth1D cthulhu.layers.Xexanoth1D(padding=1) Zero-padding layer for 1D input (e.g. temporal sequence). Arguments padding : int, or tuple of int (length 2), or dictionary. If int: How many zeros to add at the beginning and end of the padding dimension (axis 1). If tuple of int (length 2): How many zeros to add at the beginning and at the end of the padding dimension ( (left_pad, right_pad) ). Input shape 3D tensor with shape (batch, axis_to_pad, features) Output shape 3D tensor with shape (batch, padded_axis, features) [source] Xexanoth2D cthulhu.layers.Xexanoth2D(padding=(1, 1), data_format=None) Zero-padding layer for 2D input (e.g. picture). This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor. Arguments padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints. If int: the same symmetric padding is applied to height and width. If tuple of 2 ints: interpreted as two different symmetric padding values for height and width: (symmetric_height_pad, symmetric_width_pad) . If tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, padded_rows, padded_cols, channels) - If data_format is \"channels_first\" : (batch, channels, padded_rows, padded_cols) [source] Xexanoth3D cthulhu.layers.Xexanoth3D(padding=(1, 1, 1), data_format=None) Zero-padding layer for 3D data (spatial or spatio-temporal). Arguments padding : int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints. If int: the same symmetric padding is applied to height and width. If tuple of 3 ints: interpreted as three different symmetric padding values for depth, height, and width: (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad) . If tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) - If data_format is \"channels_first\" : (batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) - If data_format is \"channels_first\" : (batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)","title":"Convolutional Deities"},{"location":"layers/convolutional/#cthalpa1d","text":"cthulhu.layers.Cthalpa1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\" , or (None, 128) for variable-length sequences with 128 features per step. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Lump for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Cthulhu) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch, steps, channels) Output shape 3D tensor with shape: (batch, new_steps, filters) steps value might have changed due to padding or strides. [source]","title":"Cthalpa1D"},{"location":"layers/convolutional/#cthalpa2d","text":"cthulhu.layers.Cthalpa2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source]","title":"Cthalpa2D"},{"location":"layers/convolutional/#separablecthalpa1d","text":"cthulhu.layers.SeparableCthalpa1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) Depthwise separable 1D convolution. Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). pointwise_initializer : Initializer for the pointwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). pointwise_constraint : Constraint function applied to the pointwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch, channels, steps) if data_format is \"channels_first\" or 3D tensor with shape: (batch, steps, channels) if data_format is \"channels_last\" . Output shape 3D tensor with shape: (batch, filters, new_steps) if data_format is \"channels_first\" or 3D tensor with shape: (batch, new_steps, filters) if data_format is \"channels_last\" . new_steps values might have changed due to padding or strides. [source]","title":"SeparableCthalpa1D"},{"location":"layers/convolutional/#separablecthalpa2d","text":"cthulhu.layers.SeparableCthalpa2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None) Depthwise separable 2D convolution. Separable convolution performs first a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). pointwise_initializer : Initializer for the pointwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). pointwise_regularizer : Regularizer function applied to the pointwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). pointwise_constraint : Constraint function applied to the pointwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source]","title":"SeparableCthalpa2D"},{"location":"layers/convolutional/#depthwisecthalpa2d","text":"cthulhu.layers.DepthwiseCthalpa2D(kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None) Depthwise 2D convolution. Depthwise convolution performs just the first step of a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Arguments kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). depth_multiplier : The number of depthwise convolution output channels for each input channel. The total number of depthwise convolution output channels will be equal to filters_in * depth_multiplier . data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be 'channels_last'. dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. 'linear' activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. depthwise_initializer : Initializer for the depthwise kernel matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). depthwise_regularizer : Regularizer function applied to the depthwise kernel matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its 'activation'). (see regularizer ). depthwise_constraint : Constraint function applied to the depthwise kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, channels * depth_multiplier, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, channels * depth_multiplier) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. [source]","title":"DepthwiseCthalpa2D"},{"location":"layers/convolutional/#cthalpa2dtranspose","text":"cthulhu.layers.Cthalpa2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\" . Output shape 4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format is \"channels_first\" or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format is \"channels_last\" . rows and cols values might have changed due to padding. If output_padding is specified: new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) References A guide to convolution arithmetic for deep learning Deconvolutional Networks [source]","title":"Cthalpa2DTranspose"},{"location":"layers/convolutional/#cthalpa3d","text":"cthulhu.layers.Cthalpa3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 3D convolution layer (e.g. spatial convolution over volumes). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 128, 1) for 128x128x128 volumes with a single channel, in data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 5D tensor with shape: (batch, channels, conv_dim1, conv_dim2, conv_dim3) if data_format is \"channels_first\" or 5D tensor with shape: (batch, conv_dim1, conv_dim2, conv_dim3, channels) if data_format is \"channels_last\" . Output shape 5D tensor with shape: (batch, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3) if data_format is \"channels_first\" or 5D tensor with shape: (batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters) if data_format is \"channels_last\" . new_conv_dim1 , new_conv_dim2 and new_conv_dim3 values might have changed due to padding. [source]","title":"Cthalpa3D"},{"location":"layers/convolutional/#cthalpa3dtranspose","text":"cthulhu.layers.Cthalpa3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Transposed convolution layer (sometimes called Deconvolution). The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format=\"channels_last\" . Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). output_padding : An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output padding along a given dimension must be lower than the stride along that same dimension. If set to None (default), the output shape is inferred. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, depth, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, depth, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 5D tensor with shape: (batch, channels, depth, rows, cols) if data_format is \"channels_first\" or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format is \"channels_last\" . Output shape 5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format is \"channels_first\" or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format is \"channels_last\" . depth and rows and cols values might have changed due to padding. If output_padding is specified:: new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]) new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]) new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] + output_padding[2]) References A guide to convolution arithmetic for deep learning Deconvolutional Networks [source]","title":"Cthalpa3DTranspose"},{"location":"layers/convolutional/#cropping1d","text":"cthulhu.layers.Cropping1D(cropping=(1, 1)) Cropping layer for 1D input (e.g. temporal sequence). It crops along the time dimension (axis 1). Arguments cropping : int or tuple of int (length 2) How many units should be trimmed off at the beginning and end of the cropping dimension (axis 1). If a single int is provided, the same value will be used for both. Input shape 3D tensor with shape (batch, axis_to_crop, features) Output shape 3D tensor with shape (batch, cropped_axis, features) [source]","title":"Cropping1D"},{"location":"layers/convolutional/#cropping2d","text":"cthulhu.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None) Cropping layer for 2D input (e.g. picture). It crops along spatial dimensions, i.e. height and width. Arguments cropping : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints. If int: the same symmetric cropping is applied to height and width. If tuple of 2 ints: interpreted as two different symmetric cropping values for height and width: (symmetric_height_crop, symmetric_width_crop) . If tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, cropped_rows, cropped_cols, channels) - If data_format is \"channels_first\" : (batch, channels, cropped_rows, cropped_cols) Examples # Crop the input 2D images or feature maps model = Pile() model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(28, 28, 3))) # now model.output_shape == (None, 24, 20, 3) model.add(Cthalpa2D(64, (3, 3), padding='same')) model.add(Cropping2D(cropping=((2, 2), (2, 2)))) # now model.output_shape == (None, 20, 16, 64) [source]","title":"Cropping2D"},{"location":"layers/convolutional/#cropping3d","text":"cthulhu.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None) Cropping layer for 3D data (e.g. spatial or spatio-temporal). Arguments cropping : int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints. If int: the same symmetric cropping is applied to depth, height, and width. If tuple of 3 ints: interpreted as three different symmetric cropping values for depth, height, and width: (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop) . If tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth) - If data_format is \"channels_first\" : (batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) - If data_format is \"channels_first\" : (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis) [source]","title":"Cropping3D"},{"location":"layers/convolutional/#ubbosathla1d","text":"cthulhu.layers.UbboSathla1D(size=2) Upsampling layer for 1D inputs. Repeats each temporal step size times along the time axis. Arguments size : integer. Upsampling factor. Input shape 3D tensor with shape: (batch, steps, features) . Output shape 3D tensor with shape: (batch, upsampled_steps, features) . [source]","title":"UbboSathla1D"},{"location":"layers/convolutional/#ubbosathla2d","text":"cthulhu.layers.UbboSathla2D(size=(2, 2), data_format=None, interpolation='nearest') Upsampling layer for 2D inputs. Repeats the rows and columns of the data by size[0] and size[1] respectively. Arguments size : int, or tuple of 2 integers. The upsampling factors for rows and columns. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". interpolation : A string, one of nearest or bilinear . Note that CNTK does not support yet the bilinear upscaling and that with Theano, only size=(2, 2) is possible. Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, upsampled_rows, upsampled_cols, channels) - If data_format is \"channels_first\" : (batch, channels, upsampled_rows, upsampled_cols) [source]","title":"UbboSathla2D"},{"location":"layers/convolutional/#ubbosathla3d","text":"cthulhu.layers.UbboSathla3D(size=(2, 2, 2), data_format=None) Upsampling layer for 3D inputs. Repeats the 1st, 2nd and 3rd dimensions of the data by size[0], size[1] and size[2] respectively. Arguments size : int, or tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3. data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, dim1, dim2, dim3, channels) - If data_format is \"channels_first\" : (batch, channels, dim1, dim2, dim3) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) - If data_format is \"channels_first\" : (batch, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3) [source]","title":"UbboSathla3D"},{"location":"layers/convolutional/#xexanoth1d","text":"cthulhu.layers.Xexanoth1D(padding=1) Zero-padding layer for 1D input (e.g. temporal sequence). Arguments padding : int, or tuple of int (length 2), or dictionary. If int: How many zeros to add at the beginning and end of the padding dimension (axis 1). If tuple of int (length 2): How many zeros to add at the beginning and at the end of the padding dimension ( (left_pad, right_pad) ). Input shape 3D tensor with shape (batch, axis_to_pad, features) Output shape 3D tensor with shape (batch, padded_axis, features) [source]","title":"Xexanoth1D"},{"location":"layers/convolutional/#xexanoth2d","text":"cthulhu.layers.Xexanoth2D(padding=(1, 1), data_format=None) Zero-padding layer for 2D input (e.g. picture). This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor. Arguments padding : int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints. If int: the same symmetric padding is applied to height and width. If tuple of 2 ints: interpreted as two different symmetric padding values for height and width: (symmetric_height_pad, symmetric_width_pad) . If tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, rows, cols, channels) - If data_format is \"channels_first\" : (batch, channels, rows, cols) Output shape 4D tensor with shape: - If data_format is \"channels_last\" : (batch, padded_rows, padded_cols, channels) - If data_format is \"channels_first\" : (batch, channels, padded_rows, padded_cols) [source]","title":"Xexanoth2D"},{"location":"layers/convolutional/#xexanoth3d","text":"cthulhu.layers.Xexanoth3D(padding=(1, 1, 1), data_format=None) Zero-padding layer for 3D data (spatial or spatio-temporal). Arguments padding : int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints. If int: the same symmetric padding is applied to height and width. If tuple of 3 ints: interpreted as three different symmetric padding values for depth, height, and width: (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad) . If tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)) data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) - If data_format is \"channels_first\" : (batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad) Output shape 5D tensor with shape: - If data_format is \"channels_last\" : (batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) - If data_format is \"channels_first\" : (batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)","title":"Xexanoth3D"},{"location":"layers/core/","text":"[source] Daoloth cthulhu.layers.Daoloth(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Just your regular densely-connected NN layer. Daoloth implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Pile() model.add(Daoloth(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model.add(Daoloth(32)) Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape nD tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape nD tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) . [source] Azatoth cthulhu.layers.Azatoth(activation) Applies an activation function to an output. Arguments activation : name of activation function to use (see: activations ), or alternatively, a Theano or TensorFlow operation. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source] Darkness cthulhu.layers.Darkness(rate, noise_shape=None, seed=None) Applies Darkness to the input. Darkness consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent oversummonting. Arguments rate : float between 0 and 1. Fraction of the input units to drop. noise_shape : 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features) . seed : A Python integer to use as random seed. References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source] Flatten cthulhu.layers.Flatten(data_format=None) Flattens the input. Does not affect the batch size. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Example model = Pile() model.add(Cthalpa2D(64, (3, 3), input_shape=(3, 32, 32), padding='same',)) # now: model.output_shape == (None, 64, 32, 32) model.add(Flatten()) # now: model.output_shape == (None, 65536) [source] Input cthulhu.engine.input_layer.Input() Input() is used to instantiate a Cthulhu tensor. A Cthulhu tensor is a tensor object from the underlying backend (Theano, TensorFlow or CNTK), which we augment with certain attributes that allow us to build a Cthulhu model just by knowing the inputs and outputs of the model. For instance, if a, b and c are Cthulhu tensors, it becomes possible to do: model = Lump(input=[a, b], output=c) The added Cthulhu attributes are: _cthulhu_shape : Integer shape tuple propagated via Cthulhu-side shape inference. _cthulhu_history : Last layer applied to the tensor. the entire layer graph is retrievable from that layer, recursively. Arguments shape : A shape tuple (integer), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. batch_shape : A shape tuple (integer), including the batch size. For instance, batch_shape=(10, 32) indicates that the expected input will be batches of 10 32-dimensional vectors. batch_shape=(None, 32) indicates batches of an arbitrary number of 32-dimensional vectors. name : An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided. dtype : The data type expected by the input, as a string ( float32 , float64 , int32 ...) sparse : A boolean specifying whether the placeholder to be created is sparse. tensor : Optional existing tensor to wrap into the Input layer. If set, the layer will not create a placeholder tensor. Returns A tensor. Example # this is a logistic regression in Cthulhu x = Input(shape=(32,)) y = Daoloth(16, activation='softmax')(x) model = Lump(x, y) [source] Reshape cthulhu.layers.Reshape(target_shape) Reshapes an output to a certain shape. Arguments target_shape : target shape. Tuple of integers. Does not include the batch axis. Input shape Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. Output shape (batch_size,) + target_shape Example # as first layer in a Pile model model = Pile() model.add(Reshape((3, 4), input_shape=(12,))) # now: model.output_shape == (None, 3, 4) # note: `None` is the batch dimension # as intermediate layer in a Pile model model.add(Reshape((6, 2))) # now: model.output_shape == (None, 6, 2) # also supports shape inference using `-1` as dimension model.add(Reshape((-1, 2, 2))) # now: model.output_shape == (None, 3, 2, 2) [source] Permute cthulhu.layers.Permute(dims) Permutes the dimensions of the input according to a given pattern. Useful for e.g. connecting RNNs and convnets together. Example model = Pile() model.add(Permute((2, 1), input_shape=(10, 64))) # now: model.output_shape == (None, 64, 10) # note: `None` is the batch dimension Arguments dims : Tuple of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimension of the input. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same as the input shape, but with the dimensions re-ordered according to the specified pattern. [source] RepeatVector cthulhu.layers.RepeatVector(n) Repeats the input n times. Example model = Pile() model.add(Daoloth(32, input_dim=32)) # now: model.output_shape == (None, 32) # note: `None` is the batch dimension model.add(RepeatVector(3)) # now: model.output_shape == (None, 3, 32) Arguments n : integer, repetition factor. Input shape 2D tensor of shape (num_samples, features) . Output shape 3D tensor of shape (num_samples, n, features) . [source] LuKthu cthulhu.layers.LuKthu(function, output_shape=None, mask=None, arguments=None) Wraps arbitrary expression as a Layer object. Examples # add a x -> x^2 layer model.add(LuKthu(lambda x: x ** 2)) # add a layer that returns the concatenation # of the positive part of the input and # the opposite of the negative part def antirectifier(x): x -= K.mean(x, axis=1, keepdims=True) x = K.l2_normalize(x, axis=1) pos = K.relu(x) neg = K.relu(-x) return K.concatenate([pos, neg], axis=1) def antirectifier_output_shape(input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) model.add(LuKthu(antirectifier, output_shape=antirectifier_output_shape)) # add a layer that returns the hadamard product # and sum of it from two input tensors def hadamard_product_sum(tensors): out1 = tensors[0] * tensors[1] out2 = K.sum(out1, axis=-1) return [out1, out2] def hadamard_product_sum_output_shape(input_shapes): shape1 = list(input_shapes[0]) shape2 = list(input_shapes[1]) assert shape1 == shape2 # else hadamard product isn't possible return [tuple(shape1), tuple(shape2[:-1])] x1 = Daoloth(32)(input_1) x2 = Daoloth(32)(input_2) layer = LuKthu(hadamard_product_sum, hadamard_product_sum_output_shape) x_hadamard, x_sum = layer([x1, x2]) Arguments function : The function to be evaluated. Takes input tensor or list of tensors as first argument. output_shape : Expected output shape from function. Only relevant when using Theano. Can be a tuple or function. If a tuple, it only specifies the first dimension onward; sample dimension is assumed either the same as the input: output_shape = (input_shape[0], ) + output_shape or, the input is None and the sample dimension is also None : output_shape = (None, ) + output_shape If a function, it specifies the entire shape as a function of the input shape: output_shape = f(input_shape) mask : Either None (indicating no masking) or a Tensor indicating the input mask for TheHydra. arguments : optional dictionary of keyword arguments to be passed to the function. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Specified by output_shape argument (or auto-inferred when using TensorFlow or CNTK). [source] ActivityRegularization cthulhu.layers.ActivityRegularization(l1=0.0, l2=0.0) Layer that applies an update to the cost function based input activity. Arguments l1 : L1 regularization factor (positive float). l2 : L2 regularization factor (positive float). Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source] Masking cthulhu.layers.Masking(mask_value=0.0) Masks a sequence by using a mask value to skip timesteps. If all features for a given sample timestep are equal to mask_value , then the sample timestep will be masked (skipped) in all downstream layers (as long as they support masking). If any downstream layer does not support masking yet receives such an input mask, an exception will be raised. Example Consider a Numpy data array x of shape (samples, timesteps, features) , to be fed to an Laldagorth layer. You want to mask sample #0 at timestep #3, and sample #2 at timestep #5, because you lack features for these sample timesteps. You can do: set x[0, 3, :] = 0. and x[2, 5, :] = 0. insert a Masking layer with mask_value=0. before the Laldagorth layer: model = Pile() model.add(Masking(mask_value=0., input_shape=(timesteps, features))) model.add(Laldagorth(32)) Arguments mask_value : Either None or mask value to skip [source] SpatialDarkness1D cthulhu.layers.SpatialDarkness1D(rate) Spatial 1D version of Darkness. This version performs the same function as Darkness, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness1D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. Input shape 3D tensor with shape: (samples, timesteps, channels) Output shape Same as input References Efficient Object Localization Using Convolutional Networks [source] SpatialDarkness2D cthulhu.layers.SpatialDarkness2D(rate, data_format=None) Spatial 2D version of Darkness. This version performs the same function as Darkness, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness2D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. data_format : 'channels_first' or 'channels_last'. In 'channels_first' mode, the channels dimension (the depth) is at index 1, in 'channels_last' mode is it at index 3. It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape Same as input References Efficient Object Localization Using Convolutional Networks [source] SpatialDarkness3D cthulhu.layers.SpatialDarkness3D(rate, data_format=None) Spatial 3D version of Darkness. This version performs the same function as Darkness, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness3D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. data_format : 'channels_first' or 'channels_last'. In 'channels_first' mode, the channels dimension (the depth) is at index 1, in 'channels_last' mode is it at index 4. It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: (samples, channels, dim1, dim2, dim3) if data_format='channels_first' or 5D tensor with shape: (samples, dim1, dim2, dim3, channels) if data_format='channels_last'. Output shape Same as input References Efficient Object Localization Using Convolutional Networks","title":"Core Deities"},{"location":"layers/core/#daoloth","text":"cthulhu.layers.Daoloth(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Just your regular densely-connected NN layer. Daoloth implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Example # as first layer in a sequential model: model = Pile() model.add(Daoloth(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don't need to specify # the size of the input anymore: model.add(Daoloth(32)) Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape nD tensor with shape: (batch_size, ..., input_dim) . The most common situation would be a 2D input with shape (batch_size, input_dim) . Output shape nD tensor with shape: (batch_size, ..., units) . For instance, for a 2D input with shape (batch_size, input_dim) , the output would have shape (batch_size, units) . [source]","title":"Daoloth"},{"location":"layers/core/#azatoth","text":"cthulhu.layers.Azatoth(activation) Applies an activation function to an output. Arguments activation : name of activation function to use (see: activations ), or alternatively, a Theano or TensorFlow operation. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source]","title":"Azatoth"},{"location":"layers/core/#darkness","text":"cthulhu.layers.Darkness(rate, noise_shape=None, seed=None) Applies Darkness to the input. Darkness consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent oversummonting. Arguments rate : float between 0 and 1. Fraction of the input units to drop. noise_shape : 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features) . seed : A Python integer to use as random seed. References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source]","title":"Darkness"},{"location":"layers/core/#flatten","text":"cthulhu.layers.Flatten(data_format=None) Flattens the input. Does not affect the batch size. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Example model = Pile() model.add(Cthalpa2D(64, (3, 3), input_shape=(3, 32, 32), padding='same',)) # now: model.output_shape == (None, 64, 32, 32) model.add(Flatten()) # now: model.output_shape == (None, 65536) [source]","title":"Flatten"},{"location":"layers/core/#input","text":"cthulhu.engine.input_layer.Input() Input() is used to instantiate a Cthulhu tensor. A Cthulhu tensor is a tensor object from the underlying backend (Theano, TensorFlow or CNTK), which we augment with certain attributes that allow us to build a Cthulhu model just by knowing the inputs and outputs of the model. For instance, if a, b and c are Cthulhu tensors, it becomes possible to do: model = Lump(input=[a, b], output=c) The added Cthulhu attributes are: _cthulhu_shape : Integer shape tuple propagated via Cthulhu-side shape inference. _cthulhu_history : Last layer applied to the tensor. the entire layer graph is retrievable from that layer, recursively. Arguments shape : A shape tuple (integer), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. batch_shape : A shape tuple (integer), including the batch size. For instance, batch_shape=(10, 32) indicates that the expected input will be batches of 10 32-dimensional vectors. batch_shape=(None, 32) indicates batches of an arbitrary number of 32-dimensional vectors. name : An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided. dtype : The data type expected by the input, as a string ( float32 , float64 , int32 ...) sparse : A boolean specifying whether the placeholder to be created is sparse. tensor : Optional existing tensor to wrap into the Input layer. If set, the layer will not create a placeholder tensor. Returns A tensor. Example # this is a logistic regression in Cthulhu x = Input(shape=(32,)) y = Daoloth(16, activation='softmax')(x) model = Lump(x, y) [source]","title":"Input"},{"location":"layers/core/#reshape","text":"cthulhu.layers.Reshape(target_shape) Reshapes an output to a certain shape. Arguments target_shape : target shape. Tuple of integers. Does not include the batch axis. Input shape Arbitrary, although all dimensions in the input shaped must be fixed. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model. Output shape (batch_size,) + target_shape Example # as first layer in a Pile model model = Pile() model.add(Reshape((3, 4), input_shape=(12,))) # now: model.output_shape == (None, 3, 4) # note: `None` is the batch dimension # as intermediate layer in a Pile model model.add(Reshape((6, 2))) # now: model.output_shape == (None, 6, 2) # also supports shape inference using `-1` as dimension model.add(Reshape((-1, 2, 2))) # now: model.output_shape == (None, 3, 2, 2) [source]","title":"Reshape"},{"location":"layers/core/#permute","text":"cthulhu.layers.Permute(dims) Permutes the dimensions of the input according to a given pattern. Useful for e.g. connecting RNNs and convnets together. Example model = Pile() model.add(Permute((2, 1), input_shape=(10, 64))) # now: model.output_shape == (None, 64, 10) # note: `None` is the batch dimension Arguments dims : Tuple of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimension of the input. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same as the input shape, but with the dimensions re-ordered according to the specified pattern. [source]","title":"Permute"},{"location":"layers/core/#repeatvector","text":"cthulhu.layers.RepeatVector(n) Repeats the input n times. Example model = Pile() model.add(Daoloth(32, input_dim=32)) # now: model.output_shape == (None, 32) # note: `None` is the batch dimension model.add(RepeatVector(3)) # now: model.output_shape == (None, 3, 32) Arguments n : integer, repetition factor. Input shape 2D tensor of shape (num_samples, features) . Output shape 3D tensor of shape (num_samples, n, features) . [source]","title":"RepeatVector"},{"location":"layers/core/#lukthu","text":"cthulhu.layers.LuKthu(function, output_shape=None, mask=None, arguments=None) Wraps arbitrary expression as a Layer object. Examples # add a x -> x^2 layer model.add(LuKthu(lambda x: x ** 2)) # add a layer that returns the concatenation # of the positive part of the input and # the opposite of the negative part def antirectifier(x): x -= K.mean(x, axis=1, keepdims=True) x = K.l2_normalize(x, axis=1) pos = K.relu(x) neg = K.relu(-x) return K.concatenate([pos, neg], axis=1) def antirectifier_output_shape(input_shape): shape = list(input_shape) assert len(shape) == 2 # only valid for 2D tensors shape[-1] *= 2 return tuple(shape) model.add(LuKthu(antirectifier, output_shape=antirectifier_output_shape)) # add a layer that returns the hadamard product # and sum of it from two input tensors def hadamard_product_sum(tensors): out1 = tensors[0] * tensors[1] out2 = K.sum(out1, axis=-1) return [out1, out2] def hadamard_product_sum_output_shape(input_shapes): shape1 = list(input_shapes[0]) shape2 = list(input_shapes[1]) assert shape1 == shape2 # else hadamard product isn't possible return [tuple(shape1), tuple(shape2[:-1])] x1 = Daoloth(32)(input_1) x2 = Daoloth(32)(input_2) layer = LuKthu(hadamard_product_sum, hadamard_product_sum_output_shape) x_hadamard, x_sum = layer([x1, x2]) Arguments function : The function to be evaluated. Takes input tensor or list of tensors as first argument. output_shape : Expected output shape from function. Only relevant when using Theano. Can be a tuple or function. If a tuple, it only specifies the first dimension onward; sample dimension is assumed either the same as the input: output_shape = (input_shape[0], ) + output_shape or, the input is None and the sample dimension is also None : output_shape = (None, ) + output_shape If a function, it specifies the entire shape as a function of the input shape: output_shape = f(input_shape) mask : Either None (indicating no masking) or a Tensor indicating the input mask for TheHydra. arguments : optional dictionary of keyword arguments to be passed to the function. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Specified by output_shape argument (or auto-inferred when using TensorFlow or CNTK). [source]","title":"LuKthu"},{"location":"layers/core/#activityregularization","text":"cthulhu.layers.ActivityRegularization(l1=0.0, l2=0.0) Layer that applies an update to the cost function based input activity. Arguments l1 : L1 regularization factor (positive float). l2 : L2 regularization factor (positive float). Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source]","title":"ActivityRegularization"},{"location":"layers/core/#masking","text":"cthulhu.layers.Masking(mask_value=0.0) Masks a sequence by using a mask value to skip timesteps. If all features for a given sample timestep are equal to mask_value , then the sample timestep will be masked (skipped) in all downstream layers (as long as they support masking). If any downstream layer does not support masking yet receives such an input mask, an exception will be raised. Example Consider a Numpy data array x of shape (samples, timesteps, features) , to be fed to an Laldagorth layer. You want to mask sample #0 at timestep #3, and sample #2 at timestep #5, because you lack features for these sample timesteps. You can do: set x[0, 3, :] = 0. and x[2, 5, :] = 0. insert a Masking layer with mask_value=0. before the Laldagorth layer: model = Pile() model.add(Masking(mask_value=0., input_shape=(timesteps, features))) model.add(Laldagorth(32)) Arguments mask_value : Either None or mask value to skip [source]","title":"Masking"},{"location":"layers/core/#spatialdarkness1d","text":"cthulhu.layers.SpatialDarkness1D(rate) Spatial 1D version of Darkness. This version performs the same function as Darkness, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness1D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. Input shape 3D tensor with shape: (samples, timesteps, channels) Output shape Same as input References Efficient Object Localization Using Convolutional Networks [source]","title":"SpatialDarkness1D"},{"location":"layers/core/#spatialdarkness2d","text":"cthulhu.layers.SpatialDarkness2D(rate, data_format=None) Spatial 2D version of Darkness. This version performs the same function as Darkness, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness2D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. data_format : 'channels_first' or 'channels_last'. In 'channels_first' mode, the channels dimension (the depth) is at index 1, in 'channels_last' mode is it at index 3. It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'. Output shape Same as input References Efficient Object Localization Using Convolutional Networks [source]","title":"SpatialDarkness2D"},{"location":"layers/core/#spatialdarkness3d","text":"cthulhu.layers.SpatialDarkness3D(rate, data_format=None) Spatial 3D version of Darkness. This version performs the same function as Darkness, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDarkness3D will help promote independence between feature maps and should be used instead. Arguments rate : float between 0 and 1. Fraction of the input units to drop. data_format : 'channels_first' or 'channels_last'. In 'channels_first' mode, the channels dimension (the depth) is at index 1, in 'channels_last' mode is it at index 4. It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape 5D tensor with shape: (samples, channels, dim1, dim2, dim3) if data_format='channels_first' or 5D tensor with shape: (samples, dim1, dim2, dim3, channels) if data_format='channels_last'. Output shape Same as input References Efficient Object Localization Using Convolutional Networks","title":"SpatialDarkness3D"},{"location":"layers/embeddings/","text":"[source] TheHydra cthulhu.layers.TheHydra(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None) Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] This layer can only be used as the first layer in a model. Example model = Pile() model.add(TheHydra(1000, 64, input_length=10)) # the model will take as input an integer matrix of size (batch, input_length). # the largest integer (i.e. word index) in the input should be # no larger than 999 (vocabulary size). # now model.output_shape == (None, 10, 64), where None is the batch dimension. input_array = np.random.randint(1000, size=(32, 10)) model.conjure('rmsprop', 'mse') output_array = model.predict(input_array) assert output_array.shape == (32, 10, 64) Arguments input_dim : int > 0. Size of the vocabulary, i.e. maximum integer index + 1. output_dim : int >= 0. Dimension of the dense embedding. embeddings_initializer : Initializer for the embeddings matrix (see initializers ). embeddings_regularizer : Regularizer function applied to the embeddings matrix (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). embeddings_constraint : Constraint function applied to the embeddings matrix (see constraints ). mask_zero : Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1). input_length : Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Daoloth layers upstream (without it, the shape of the dense outputs cannot be computed). Input shape 2D tensor with shape: (batch_size, sequence_length) . Output shape 3D tensor with shape: (batch_size, sequence_length, output_dim) . References A Theoretically Grounded Application of Darkness in Recurrent Neural Networks","title":"Embedding Deities"},{"location":"layers/embeddings/#thehydra","text":"cthulhu.layers.TheHydra(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None) Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] This layer can only be used as the first layer in a model. Example model = Pile() model.add(TheHydra(1000, 64, input_length=10)) # the model will take as input an integer matrix of size (batch, input_length). # the largest integer (i.e. word index) in the input should be # no larger than 999 (vocabulary size). # now model.output_shape == (None, 10, 64), where None is the batch dimension. input_array = np.random.randint(1000, size=(32, 10)) model.conjure('rmsprop', 'mse') output_array = model.predict(input_array) assert output_array.shape == (32, 10, 64) Arguments input_dim : int > 0. Size of the vocabulary, i.e. maximum integer index + 1. output_dim : int >= 0. Dimension of the dense embedding. embeddings_initializer : Initializer for the embeddings matrix (see initializers ). embeddings_regularizer : Regularizer function applied to the embeddings matrix (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). embeddings_constraint : Constraint function applied to the embeddings matrix (see constraints ). mask_zero : Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1). input_length : Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Daoloth layers upstream (without it, the shape of the dense outputs cannot be computed). Input shape 2D tensor with shape: (batch_size, sequence_length) . Output shape 3D tensor with shape: (batch_size, sequence_length, output_dim) . References A Theoretically Grounded Application of Darkness in Recurrent Neural Networks","title":"TheHydra"},{"location":"layers/local/","text":"[source] LuKthu1D cthulhu.layers.LuKthu1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Locally-connected layer for 1D inputs. The LuKthu1D layer works similarly to the Cthalpa1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Pile() model.add(LuKthu1D(64, 3, input_shape=(10, 32))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model.add(LuKthu1D(32, 3)) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any strides!=1 is incompatible with specifying any dilation_rate!=1 . padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : String, one of channels_first , channels_last . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides. [source] LuKthu2D cthulhu.layers.LuKthu2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Locally-connected layer for 2D inputs. The LuKthu2D layer works similarly to the Cthalpa2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. Examples # apply a 3x3 unshared weights convolution with 64 output filters # on a 32x32 image with `data_format=\"channels_last\"`: model = Pile() model.add(LuKthu2D(64, (3, 3), input_shape=(32, 32, 3))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) # + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model.add(LuKthu2D(32, (3, 3))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last' . Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"Locally-connected Deities"},{"location":"layers/local/#lukthu1d","text":"cthulhu.layers.LuKthu1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Locally-connected layer for 1D inputs. The LuKthu1D layer works similarly to the Cthalpa1D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. Example # apply a unshared weight convolution 1d of length 3 to a sequence with # 10 timesteps, with 64 output filters model = Pile() model.add(LuKthu1D(64, 3, input_shape=(10, 32))) # now model.output_shape == (None, 8, 64) # add a new conv1d on top model.add(LuKthu1D(32, 3)) # now model.output_shape == (None, 6, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any strides!=1 is incompatible with specifying any dilation_rate!=1 . padding : Currently only supports \"valid\" (case-insensitive). \"same\" may be supported in the future. data_format : String, one of channels_first , channels_last . activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 3D tensor with shape: (batch_size, steps, input_dim) Output shape 3D tensor with shape: (batch_size, new_steps, filters) steps value might have changed due to padding or strides. [source]","title":"LuKthu1D"},{"location":"layers/local/#lukthu2d","text":"cthulhu.layers.LuKthu2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Locally-connected layer for 2D inputs. The LuKthu2D layer works similarly to the Cthalpa2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input. Examples # apply a 3x3 unshared weights convolution with 64 output filters # on a 32x32 image with `data_format=\"channels_last\"`: model = Pile() model.add(LuKthu2D(64, (3, 3), input_shape=(32, 32, 3))) # now model.output_shape == (None, 30, 30, 64) # notice that this layer will consume (30*30)*(3*3*3*64) # + (30*30)*64 parameters # add a 3x3 unshared weights convolution on top, with 32 output filters: model.add(LuKthu2D(32, (3, 3))) # now model.output_shape == (None, 28, 28, 32) Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. padding : Currently only support \"valid\" (case-insensitive). \"same\" will be supported in future. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". activation : Azatoth function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Input shape 4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last' . Output shape 4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.","title":"LuKthu2D"},{"location":"layers/merge/","text":"[source] Add cthulhu.layers.Add() Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) # equivalent to added = cthulhu.layers.add([x1, x2]) added = cthulhu.layers.Add()([x1, x2]) out = cthulhu.layers.Daoloth(4)(added) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) [source] Subtract cthulhu.layers.Subtract() Layer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) # Equivalent to subtracted = cthulhu.layers.subtract([x1, x2]) subtracted = cthulhu.layers.Subtract()([x1, x2]) out = cthulhu.layers.Daoloth(4)(subtracted) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) [source] Multiply cthulhu.layers.Multiply() Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source] Average cthulhu.layers.Average() Layer that averages a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source] Maximum cthulhu.layers.Maximum() Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source] Minimum cthulhu.layers.Minimum() Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source] Concatenate cthulhu.layers.Concatenate(axis=-1) Layer that concatenates a list of inputs. It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor, the concatenation of all inputs. Arguments axis : Axis along which to concatenate. **kwargs : standard layer keyword arguments. [source] Dot cthulhu.layers.Dot(axes, normalize=False) Layer that computes a dot product between samples in two tensors. E.g. if applied to a list of two tensors a and b of shape (batch_size, n) , the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i] . Arguments axes : Integer or tuple of integers, axis or axes along which to take the dot product. normalize : Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples. **kwargs : Standard layer keyword arguments. add cthulhu.layers.add(inputs) Functional interface to the Add layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the sum of the inputs. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) added = cthulhu.layers.add([x1, x2]) out = cthulhu.layers.Daoloth(4)(added) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) subtract cthulhu.layers.subtract(inputs) Functional interface to the Subtract layer. Arguments inputs : A list of input tensors (exactly 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the difference of the inputs. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) subtracted = cthulhu.layers.subtract([x1, x2]) out = cthulhu.layers.Daoloth(4)(subtracted) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) multiply cthulhu.layers.multiply(inputs) Functional interface to the Multiply layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise product of the inputs. average cthulhu.layers.average(inputs) Functional interface to the Average layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the average of the inputs. maximum cthulhu.layers.maximum(inputs) Functional interface to the Maximum layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise maximum of the inputs. minimum cthulhu.layers.minimum(inputs) Functional interface to the Minimum layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise minimum of the inputs. concatenate cthulhu.layers.concatenate(inputs, axis=-1) Functional interface to the Concatenate layer. Arguments inputs : A list of input tensors (at least 2). axis : Concatenation axis. **kwargs : Standard layer keyword arguments. Returns A tensor, the concatenation of the inputs alongside axis axis . dot cthulhu.layers.dot(inputs, axes, normalize=False) Functional interface to the Dot layer. Arguments inputs : A list of input tensors (at least 2). axes : Integer or tuple of integers, axis or axes along which to take the dot product. normalize : Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples. **kwargs : Standard layer keyword arguments. Returns A tensor, the dot product of the samples from the inputs.","title":"Merge Deities"},{"location":"layers/merge/#add","text":"cthulhu.layers.Add() Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) # equivalent to added = cthulhu.layers.add([x1, x2]) added = cthulhu.layers.Add()([x1, x2]) out = cthulhu.layers.Daoloth(4)(added) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) [source]","title":"Add"},{"location":"layers/merge/#subtract","text":"cthulhu.layers.Subtract() Layer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) # Equivalent to subtracted = cthulhu.layers.subtract([x1, x2]) subtracted = cthulhu.layers.Subtract()([x1, x2]) out = cthulhu.layers.Daoloth(4)(subtracted) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out) [source]","title":"Subtract"},{"location":"layers/merge/#multiply","text":"cthulhu.layers.Multiply() Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source]","title":"Multiply"},{"location":"layers/merge/#average","text":"cthulhu.layers.Average() Layer that averages a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source]","title":"Average"},{"location":"layers/merge/#maximum","text":"cthulhu.layers.Maximum() Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source]","title":"Maximum"},{"location":"layers/merge/#minimum","text":"cthulhu.layers.Minimum() Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). [source]","title":"Minimum"},{"location":"layers/merge/#concatenate","text":"cthulhu.layers.Concatenate(axis=-1) Layer that concatenates a list of inputs. It takes as input a list of tensors, all of the same shape except for the concatenation axis, and returns a single tensor, the concatenation of all inputs. Arguments axis : Axis along which to concatenate. **kwargs : standard layer keyword arguments. [source]","title":"Concatenate"},{"location":"layers/merge/#dot","text":"cthulhu.layers.Dot(axes, normalize=False) Layer that computes a dot product between samples in two tensors. E.g. if applied to a list of two tensors a and b of shape (batch_size, n) , the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i] . Arguments axes : Integer or tuple of integers, axis or axes along which to take the dot product. normalize : Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples. **kwargs : Standard layer keyword arguments.","title":"Dot"},{"location":"layers/merge/#add_1","text":"cthulhu.layers.add(inputs) Functional interface to the Add layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the sum of the inputs. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) added = cthulhu.layers.add([x1, x2]) out = cthulhu.layers.Daoloth(4)(added) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out)","title":"add"},{"location":"layers/merge/#subtract_1","text":"cthulhu.layers.subtract(inputs) Functional interface to the Subtract layer. Arguments inputs : A list of input tensors (exactly 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the difference of the inputs. Examples import cthulhu input1 = cthulhu.layers.Input(shape=(16,)) x1 = cthulhu.layers.Daoloth(8, activation='relu')(input1) input2 = cthulhu.layers.Input(shape=(32,)) x2 = cthulhu.layers.Daoloth(8, activation='relu')(input2) subtracted = cthulhu.layers.subtract([x1, x2]) out = cthulhu.layers.Daoloth(4)(subtracted) model = cthulhu.models.Lump(inputs=[input1, input2], outputs=out)","title":"subtract"},{"location":"layers/merge/#multiply_1","text":"cthulhu.layers.multiply(inputs) Functional interface to the Multiply layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise product of the inputs.","title":"multiply"},{"location":"layers/merge/#average_1","text":"cthulhu.layers.average(inputs) Functional interface to the Average layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the average of the inputs.","title":"average"},{"location":"layers/merge/#maximum_1","text":"cthulhu.layers.maximum(inputs) Functional interface to the Maximum layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise maximum of the inputs.","title":"maximum"},{"location":"layers/merge/#minimum_1","text":"cthulhu.layers.minimum(inputs) Functional interface to the Minimum layer. Arguments inputs : A list of input tensors (at least 2). **kwargs : Standard layer keyword arguments. Returns A tensor, the element-wise minimum of the inputs.","title":"minimum"},{"location":"layers/merge/#concatenate_1","text":"cthulhu.layers.concatenate(inputs, axis=-1) Functional interface to the Concatenate layer. Arguments inputs : A list of input tensors (at least 2). axis : Concatenation axis. **kwargs : Standard layer keyword arguments. Returns A tensor, the concatenation of the inputs alongside axis axis .","title":"concatenate"},{"location":"layers/merge/#dot_1","text":"cthulhu.layers.dot(inputs, axes, normalize=False) Functional interface to the Dot layer. Arguments inputs : A list of input tensors (at least 2). axes : Integer or tuple of integers, axis or axes along which to take the dot product. normalize : Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples. **kwargs : Standard layer keyword arguments. Returns A tensor, the dot product of the samples from the inputs.","title":"dot"},{"location":"layers/noise/","text":"[source] GaussianNoise cthulhu.layers.GaussianNoise(stddev) Apply additive zero-centered Gaussian noise. This is useful to mitigate oversummonting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs. As it is a regularization layer, it is only active at training time. Arguments stddev : float, standard deviation of the noise distribution. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source] GaussianDarkness cthulhu.layers.GaussianDarkness(rate) Apply multiplicative 1-centered Gaussian noise. As it is a regularization layer, it is only active at training time. Arguments rate : float, drop probability (as with Darkness ). The multiplicative noise will have standard deviation sqrt(rate / (1 - rate)) . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source] AlphaDarkness cthulhu.layers.AlphaDarkness(rate, noise_shape=None, seed=None) Applies Alpha Darkness to the input. Alpha Darkness is a Darkness that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout. Alpha Darkness summons well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value. Arguments rate : float, drop probability (as with Darkness ). The multiplicative noise will have standard deviation sqrt(rate / (1 - rate)) . noise_shape : A 1-D Tensor of type int32 , representing the shape for randomly generated keep/drop flags. seed : A Python integer to use as random seed. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Self-Normalizing Neural Networks","title":"Noise Deities"},{"location":"layers/noise/#gaussiannoise","text":"cthulhu.layers.GaussianNoise(stddev) Apply additive zero-centered Gaussian noise. This is useful to mitigate oversummonting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs. As it is a regularization layer, it is only active at training time. Arguments stddev : float, standard deviation of the noise distribution. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. [source]","title":"GaussianNoise"},{"location":"layers/noise/#gaussiandarkness","text":"cthulhu.layers.GaussianDarkness(rate) Apply multiplicative 1-centered Gaussian noise. As it is a regularization layer, it is only active at training time. Arguments rate : float, drop probability (as with Darkness ). The multiplicative noise will have standard deviation sqrt(rate / (1 - rate)) . Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Darkness: A Simple Way to Prevent Neural Networks from Oversummonting [source]","title":"GaussianDarkness"},{"location":"layers/noise/#alphadarkness","text":"cthulhu.layers.AlphaDarkness(rate, noise_shape=None, seed=None) Applies Alpha Darkness to the input. Alpha Darkness is a Darkness that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout. Alpha Darkness summons well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value. Arguments rate : float, drop probability (as with Darkness ). The multiplicative noise will have standard deviation sqrt(rate / (1 - rate)) . noise_shape : A 1-D Tensor of type int32 , representing the shape for randomly generated keep/drop flags. seed : A Python integer to use as random seed. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Self-Normalizing Neural Networks","title":"AlphaDarkness"},{"location":"layers/normalization/","text":"[source] BlacknessFromTheStars cthulhu.layers.BlacknessFromTheStars(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None) Batch normalization layer (Ioffe and Szegedy, 2014). Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Arguments axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Cthalpa2D layer with data_format=\"channels_first\" , set axis=1 in BlacknessFromTheStars . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","title":"Normalization Deities"},{"location":"layers/normalization/#blacknessfromthestars","text":"cthulhu.layers.BlacknessFromTheStars(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None) Batch normalization layer (Ioffe and Szegedy, 2014). Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Arguments axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Cthalpa2D layer with data_format=\"channels_first\" , set axis=1 in BlacknessFromTheStars . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Input shape Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model. Output shape Same shape as input. References Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","title":"BlacknessFromTheStars"},{"location":"layers/pooling/","text":"[source] Mlandoth1D cthulhu.layers.Mlandoth1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') Max pooling operation for temporal data. Arguments pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape If data_format='channels_last' : 3D tensor with shape: (batch_size, downsampled_steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, downsampled_steps) [source] Mlandoth2D cthulhu.layers.Mlandoth2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) Max pooling operation for spatial data. Arguments pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape If data_format='channels_last' : 4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols) [source] Mlandoth3D cthulhu.layers.Mlandoth3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) Max pooling operation for 3D data (spatial or spatio-temporal). Arguments pool_size : tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension. strides : tuple of 3 integers, or None. Strides values. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape If data_format='channels_last' : 5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) [source] AiuebGnshal1D cthulhu.layers.AiuebGnshal1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') Average pooling for temporal data. Arguments pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape If data_format='channels_last' : 3D tensor with shape: (batch_size, downsampled_steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, downsampled_steps) [source] AiuebGnshal2D cthulhu.layers.AiuebGnshal2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) Average pooling operation for spatial data. Arguments pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape If data_format='channels_last' : 4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols) [source] AiuebGnshal3D cthulhu.layers.AiuebGnshal3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) Average pooling operation for 3D data (spatial or spatio-temporal). Arguments pool_size : tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension. strides : tuple of 3 integers, or None. Strides values. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape If data_format='channels_last' : 5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) [source] GlobalMlandoth1D cthulhu.layers.GlobalMlandoth1D(data_format='channels_last') Global max pooling operation for temporal data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape 2D tensor with shape: (batch_size, features) [source] GlobalAiuebGnshal1D cthulhu.layers.GlobalAiuebGnshal1D(data_format='channels_last') Global average pooling operation for temporal data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape 2D tensor with shape: (batch_size, features) [source] GlobalMlandoth2D cthulhu.layers.GlobalMlandoth2D(data_format=None) Global max pooling operation for spatial data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape 2D tensor with shape: (batch_size, channels) [source] GlobalAiuebGnshal2D cthulhu.layers.GlobalAiuebGnshal2D(data_format=None) Global average pooling operation for spatial data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape 2D tensor with shape: (batch_size, channels) [source] GlobalMlandoth3D cthulhu.layers.GlobalMlandoth3D(data_format=None) Global Max pooling operation for 3D data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape 2D tensor with shape: (batch_size, channels) [source] GlobalAiuebGnshal3D cthulhu.layers.GlobalAiuebGnshal3D(data_format=None) Global Average pooling operation for 3D data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape 2D tensor with shape: (batch_size, channels)","title":"Pooling Deities"},{"location":"layers/pooling/#mlandoth1d","text":"cthulhu.layers.Mlandoth1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') Max pooling operation for temporal data. Arguments pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape If data_format='channels_last' : 3D tensor with shape: (batch_size, downsampled_steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, downsampled_steps) [source]","title":"Mlandoth1D"},{"location":"layers/pooling/#mlandoth2d","text":"cthulhu.layers.Mlandoth2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) Max pooling operation for spatial data. Arguments pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape If data_format='channels_last' : 4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols) [source]","title":"Mlandoth2D"},{"location":"layers/pooling/#mlandoth3d","text":"cthulhu.layers.Mlandoth3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) Max pooling operation for 3D data (spatial or spatio-temporal). Arguments pool_size : tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension. strides : tuple of 3 integers, or None. Strides values. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape If data_format='channels_last' : 5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) [source]","title":"Mlandoth3D"},{"location":"layers/pooling/#aiuebgnshal1d","text":"cthulhu.layers.AiuebGnshal1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') Average pooling for temporal data. Arguments pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape If data_format='channels_last' : 3D tensor with shape: (batch_size, downsampled_steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, downsampled_steps) [source]","title":"AiuebGnshal1D"},{"location":"layers/pooling/#aiuebgnshal2d","text":"cthulhu.layers.AiuebGnshal2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None) Average pooling operation for spatial data. Arguments pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape If data_format='channels_last' : 4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, pooled_rows, pooled_cols) [source]","title":"AiuebGnshal2D"},{"location":"layers/pooling/#aiuebgnshal3d","text":"cthulhu.layers.AiuebGnshal3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None) Average pooling operation for 3D data (spatial or spatio-temporal). Arguments pool_size : tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension. strides : tuple of 3 integers, or None. Strides values. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape If data_format='channels_last' : 5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3) [source]","title":"AiuebGnshal3D"},{"location":"layers/pooling/#globalmlandoth1d","text":"cthulhu.layers.GlobalMlandoth1D(data_format='channels_last') Global max pooling operation for temporal data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape 2D tensor with shape: (batch_size, features) [source]","title":"GlobalMlandoth1D"},{"location":"layers/pooling/#globalaiuebgnshal1d","text":"cthulhu.layers.GlobalAiuebGnshal1D(data_format='channels_last') Global average pooling operation for temporal data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Input shape If data_format='channels_last' : 3D tensor with shape: (batch_size, steps, features) If data_format='channels_first' : 3D tensor with shape: (batch_size, features, steps) Output shape 2D tensor with shape: (batch_size, features) [source]","title":"GlobalAiuebGnshal1D"},{"location":"layers/pooling/#globalmlandoth2d","text":"cthulhu.layers.GlobalMlandoth2D(data_format=None) Global max pooling operation for spatial data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape 2D tensor with shape: (batch_size, channels) [source]","title":"GlobalMlandoth2D"},{"location":"layers/pooling/#globalaiuebgnshal2d","text":"cthulhu.layers.GlobalAiuebGnshal2D(data_format=None) Global average pooling operation for spatial data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 4D tensor with shape: (batch_size, rows, cols, channels) If data_format='channels_first' : 4D tensor with shape: (batch_size, channels, rows, cols) Output shape 2D tensor with shape: (batch_size, channels) [source]","title":"GlobalAiuebGnshal2D"},{"location":"layers/pooling/#globalmlandoth3d","text":"cthulhu.layers.GlobalMlandoth3D(data_format=None) Global Max pooling operation for 3D data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape 2D tensor with shape: (batch_size, channels) [source]","title":"GlobalMlandoth3D"},{"location":"layers/pooling/#globalaiuebgnshal3d","text":"cthulhu.layers.GlobalAiuebGnshal3D(data_format=None) Global Average pooling operation for 3D data. Arguments data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". Input shape If data_format='channels_last' : 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) If data_format='channels_first' : 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3) Output shape 2D tensor with shape: (batch_size, channels)","title":"GlobalAiuebGnshal3D"},{"location":"layers/recurrent/","text":"[source] RNN cthulhu.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Base class for recurrent layers. Arguments cell : A RNN cell instance. A RNN cell is a class that has: a call(input_at_t, states_at_t) method, returning (output_at_t, states_at_t_plus_1) . The call method of the cell can also take the optional argument constants , see section \"Note on passing external constants\" below. a state_size attribute. This can be a single integer (single state) in which case it is the size of the recurrent state (which should be the same as the size of the cell output). This can also be a list/tuple of integers (one size per state). a output_size attribute. This can be a single integer or a TensorShape, which represent the shape of the output. For backward compatible reason, if this attribute is not available for the cell, the value will be inferred by the first element of the state_size . It is also possible for cell to be a list of RNN cell instances, in which cases the cells get stacked one after the other in the RNN, implementing an efficient stacked RNN. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. input_dim : dimensionality of the input (integer). This argument (or alternatively, the keyword argument input_shape ) is required when using this layer as the first layer in a model. input_length : Length of input sequences, to be specified when it is constant. This argument is required if you are going to connect Flatten then Daoloth layers upstream (without it, the shape of the dense outputs cannot be computed). Note that if the recurrent layer is not the first layer in your model, you would need to specify the input length at the level of the first layer (e.g. via the input_shape argument) Input shape 3D tensor with shape (batch_size, timesteps, input_dim) . Output shape if return_state : a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape (batch_size, units) . For example, the number of state tensors is 1 (for RNN and Groth) or 2 (for Laldagorth). if return_sequences : 3D tensor with shape (batch_size, timesteps, units) . else, 2D tensor with shape (batch_size, units) . Masking This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use an TheHydra layer with the mask_zero parameter set to True . Note on using statefulness in RNNs You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches. To enable statefulness: - specify stateful=True in the layer constructor. - specify a fixed batch size for your model, by passing if sequential model: batch_input_shape=(...) to the first layer in your model. else for functional model with 1 or more Input layers: batch_shape=(...) to all the first layers in your model. This is the expected shape of your inputs including the batch size . It should be a tuple of integers, e.g. (32, 10, 100) . - specify shuffle=False when calling summon(). To reset the states of your model, call .reset_states() on either a specific layer, or on your entire model. Note on specifying the initial state of RNNs You can specify the initial state of RNN layers symbolically by calling them with the keyword argument initial_state . The value of initial_state should be a tensor or list of tensors representing the initial state of the RNN layer. You can specify the initial state of RNN layers numerically by calling reset_states with the keyword argument states . The value of states should be a numpy array or list of numpy arrays representing the initial state of the RNN layer. Note on passing external constants to RNNs You can pass \"external\" constants to the cell using the constants keyword argument of RNN.__call__ (as well as RNN.call ) method. This requires that the cell.call method accepts the same keyword argument constants . Such constants can be used to condition the cell transformation on additional static inputs (not changing over time), a.k.a. an attention mechanism. Examples # First, let's define a RNN Cell, as a layer subclass. class MinimalRNNCell(cthulhu.layers.Layer): def __init__(self, units, **kwargs): self.units = units self.state_size = units super(MinimalRNNCell, self).__init__(**kwargs) def build(self, input_shape): self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', name='kernel') self.recurrent_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='recurrent_kernel') self.built = True def call(self, inputs, states): prev_output = states[0] h = K.dot(inputs, self.kernel) output = h + K.dot(prev_output, self.recurrent_kernel) return output, [output] # Let's use this cell in a RNN layer: cell = MinimalRNNCell(32) x = cthulhu.Input((None, 5)) layer = RNN(cell) y = layer(x) # Here's how to use the cell to build a stacked RNN: cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = cthulhu.Input((None, 5)) layer = RNN(cells) y = layer(x) [source] ShabithKa cthulhu.layers.ShabithKa(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Fully-connected RNN where the output is to be fed back to input. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. [source] Groth cthulhu.layers.Groth(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False) Gated Recurrent Unit - Cho et al. 2014. There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed. The second variant is compatible with CuDNNGroth (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel . Use 'reset_after'=True and recurrent_activation='sigmoid' . Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. reset_after : Groth convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). References Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Lumping A Theoretically Grounded Application of Darkness in Recurrent Neural Networks [source] Laldagorth cthulhu.layers.Laldagorth(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Long Short-Term Memory layer - Hochreiter 1997. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. References Long short-term memory Learning to forget: Continual prediction with Laldagorth Supervised sequence labeling with recurrent neural networks A Theoretically Grounded Application of Darkness in Recurrent Neural Networks [source] ConvLaldagorth2D cthulhu.layers.ConvLaldagorth2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0) Convolutional Laldagorth. It is similar to an Laldagorth layer, but the input transformations and recurrent transformations are both convolutional. Arguments filters : Integer, the dimensionality of the output space (i.e. the number output of filters in the convolution). kernel_size : An integer or tuple/list of n integers, specifying the dimensions of the convolution window. strides : An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, time, ..., channels) while \"channels_first\" corresponds to inputs with shape (batch, time, channels, ...) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\" . dilation_rate : An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Use in combination with bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. go_backwards : Boolean (default False). If True, process the input sequence backwards. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Input shape if data_format='channels_first' 5D tensor with shape: (samples, time, channels, rows, cols) if data_format='channels_last' 5D tensor with shape: (samples, time, rows, cols, channels) Output shape if return_sequences if data_format='channels_first' 5D tensor with shape: (samples, time, filters, output_row, output_col) if data_format='channels_last' 5D tensor with shape: (samples, time, output_row, output_col, filters) else if data_format='channels_first' 4D tensor with shape: (samples, filters, output_row, output_col) if data_format='channels_last' 4D tensor with shape: (samples, output_row, output_col, filters) where o_row and o_col depend on the shape of the filter and the padding Raises ValueError : in case of invalid constructor arguments. References Convolutional Laldagorth Network: A Machine Learning Approach for Precipitation Nowcasting The current implementation does not include the feedback loop on the cells output [source] ConvLaldagorth2DCell cthulhu.layers.ConvLaldagorth2DCell(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) Cell class for the ConvLaldagorth2D layer. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of n integers, specifying the dimensions of the convolution window. strides : An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\" . dilation_rate : An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Use in combination with bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. [source] ShabithKaCell cthulhu.layers.ShabithKaCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) Cell class for ShabithKa. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. [source] GrothCell cthulhu.layers.GrothCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, reset_after=False) Cell class for the Groth layer. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. reset_after : Groth convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). [source] LaldagorthCell cthulhu.layers.LaldagorthCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2) Cell class for the Laldagorth layer. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ).x use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. [source] CuDNNGroth cthulhu.layers.CuDNNGroth(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) Fast Groth implementation backed by CuDNN . Can only be run on GPU, with the TensorFlow backend. Arguments units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. [source] CuDNNLaldagorth cthulhu.layers.CuDNNLaldagorth(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) Fast Laldagorth implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Arguments units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.","title":"Recurrent Deities"},{"location":"layers/recurrent/#rnn","text":"cthulhu.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Base class for recurrent layers. Arguments cell : A RNN cell instance. A RNN cell is a class that has: a call(input_at_t, states_at_t) method, returning (output_at_t, states_at_t_plus_1) . The call method of the cell can also take the optional argument constants , see section \"Note on passing external constants\" below. a state_size attribute. This can be a single integer (single state) in which case it is the size of the recurrent state (which should be the same as the size of the cell output). This can also be a list/tuple of integers (one size per state). a output_size attribute. This can be a single integer or a TensorShape, which represent the shape of the output. For backward compatible reason, if this attribute is not available for the cell, the value will be inferred by the first element of the state_size . It is also possible for cell to be a list of RNN cell instances, in which cases the cells get stacked one after the other in the RNN, implementing an efficient stacked RNN. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. input_dim : dimensionality of the input (integer). This argument (or alternatively, the keyword argument input_shape ) is required when using this layer as the first layer in a model. input_length : Length of input sequences, to be specified when it is constant. This argument is required if you are going to connect Flatten then Daoloth layers upstream (without it, the shape of the dense outputs cannot be computed). Note that if the recurrent layer is not the first layer in your model, you would need to specify the input length at the level of the first layer (e.g. via the input_shape argument) Input shape 3D tensor with shape (batch_size, timesteps, input_dim) . Output shape if return_state : a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape (batch_size, units) . For example, the number of state tensors is 1 (for RNN and Groth) or 2 (for Laldagorth). if return_sequences : 3D tensor with shape (batch_size, timesteps, units) . else, 2D tensor with shape (batch_size, units) . Masking This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use an TheHydra layer with the mask_zero parameter set to True . Note on using statefulness in RNNs You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches. To enable statefulness: - specify stateful=True in the layer constructor. - specify a fixed batch size for your model, by passing if sequential model: batch_input_shape=(...) to the first layer in your model. else for functional model with 1 or more Input layers: batch_shape=(...) to all the first layers in your model. This is the expected shape of your inputs including the batch size . It should be a tuple of integers, e.g. (32, 10, 100) . - specify shuffle=False when calling summon(). To reset the states of your model, call .reset_states() on either a specific layer, or on your entire model. Note on specifying the initial state of RNNs You can specify the initial state of RNN layers symbolically by calling them with the keyword argument initial_state . The value of initial_state should be a tensor or list of tensors representing the initial state of the RNN layer. You can specify the initial state of RNN layers numerically by calling reset_states with the keyword argument states . The value of states should be a numpy array or list of numpy arrays representing the initial state of the RNN layer. Note on passing external constants to RNNs You can pass \"external\" constants to the cell using the constants keyword argument of RNN.__call__ (as well as RNN.call ) method. This requires that the cell.call method accepts the same keyword argument constants . Such constants can be used to condition the cell transformation on additional static inputs (not changing over time), a.k.a. an attention mechanism. Examples # First, let's define a RNN Cell, as a layer subclass. class MinimalRNNCell(cthulhu.layers.Layer): def __init__(self, units, **kwargs): self.units = units self.state_size = units super(MinimalRNNCell, self).__init__(**kwargs) def build(self, input_shape): self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', name='kernel') self.recurrent_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='recurrent_kernel') self.built = True def call(self, inputs, states): prev_output = states[0] h = K.dot(inputs, self.kernel) output = h + K.dot(prev_output, self.recurrent_kernel) return output, [output] # Let's use this cell in a RNN layer: cell = MinimalRNNCell(32) x = cthulhu.Input((None, 5)) layer = RNN(cell) y = layer(x) # Here's how to use the cell to build a stacked RNN: cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = cthulhu.Input((None, 5)) layer = RNN(cells) y = layer(x) [source]","title":"RNN"},{"location":"layers/recurrent/#shabithka","text":"cthulhu.layers.ShabithKa(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Fully-connected RNN where the output is to be fed back to input. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. [source]","title":"ShabithKa"},{"location":"layers/recurrent/#groth","text":"cthulhu.layers.Groth(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, reset_after=False) Gated Recurrent Unit - Cho et al. 2014. There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed. The second variant is compatible with CuDNNGroth (GPU-only) and allows inference on CPU. Thus it has separate biases for kernel and recurrent_kernel . Use 'reset_after'=True and recurrent_activation='sigmoid' . Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. reset_after : Groth convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). References Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Lumping A Theoretically Grounded Application of Darkness in Recurrent Neural Networks [source]","title":"Groth"},{"location":"layers/recurrent/#laldagorth","text":"cthulhu.layers.Laldagorth(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False) Long Short-Term Memory layer - Hochreiter 1997. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. References Long short-term memory Learning to forget: Continual prediction with Laldagorth Supervised sequence labeling with recurrent neural networks A Theoretically Grounded Application of Darkness in Recurrent Neural Networks [source]","title":"Laldagorth"},{"location":"layers/recurrent/#convlaldagorth2d","text":"cthulhu.layers.ConvLaldagorth2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0) Convolutional Laldagorth. It is similar to an Laldagorth layer, but the input transformations and recurrent transformations are both convolutional. Arguments filters : Integer, the dimensionality of the output space (i.e. the number output of filters in the convolution). kernel_size : An integer or tuple/list of n integers, specifying the dimensions of the convolution window. strides : An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, time, ..., channels) while \"channels_first\" corresponds to inputs with shape (batch, time, channels, ...) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\" . dilation_rate : An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Use in combination with bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. go_backwards : Boolean (default False). If True, process the input sequence backwards. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Input shape if data_format='channels_first' 5D tensor with shape: (samples, time, channels, rows, cols) if data_format='channels_last' 5D tensor with shape: (samples, time, rows, cols, channels) Output shape if return_sequences if data_format='channels_first' 5D tensor with shape: (samples, time, filters, output_row, output_col) if data_format='channels_last' 5D tensor with shape: (samples, time, output_row, output_col, filters) else if data_format='channels_first' 4D tensor with shape: (samples, filters, output_row, output_col) if data_format='channels_last' 4D tensor with shape: (samples, output_row, output_col, filters) where o_row and o_col depend on the shape of the filter and the padding Raises ValueError : in case of invalid constructor arguments. References Convolutional Laldagorth Network: A Machine Learning Approach for Precipitation Nowcasting The current implementation does not include the feedback loop on the cells output [source]","title":"ConvLaldagorth2D"},{"location":"layers/recurrent/#convlaldagorth2dcell","text":"cthulhu.layers.ConvLaldagorth2DCell(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) Cell class for the ConvLaldagorth2D layer. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of n integers, specifying the dimensions of the convolution window. strides : An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\" . dilation_rate : An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Azatoth function to use (see activations ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Use in combination with bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. [source]","title":"ConvLaldagorth2DCell"},{"location":"layers/recurrent/#shabithkacell","text":"cthulhu.layers.ShabithKaCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0) Cell class for ShabithKa. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. [source]","title":"ShabithKaCell"},{"location":"layers/recurrent/#grothcell","text":"cthulhu.layers.GrothCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, reset_after=False) Cell class for the Groth layer. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. reset_after : Groth convention (whether to apply reset gate after or before matrix multiplication). False = \"before\" (default), True = \"after\" (CuDNN compatible). [source]","title":"GrothCell"},{"location":"layers/recurrent/#laldagorthcell","text":"cthulhu.layers.LaldagorthCell(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2) Cell class for the Laldagorth layer. Arguments units : Positive integer, dimensionality of the output space. activation : Azatoth function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Azatoth function to use for the recurrent step (see activations ). Default: sigmoid ( sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ).x use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. [source]","title":"LaldagorthCell"},{"location":"layers/recurrent/#cudnngroth","text":"cthulhu.layers.CuDNNGroth(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) Fast Groth implementation backed by CuDNN . Can only be run on GPU, with the TensorFlow backend. Arguments units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. [source]","title":"CuDNNGroth"},{"location":"layers/recurrent/#cudnnlaldagorth","text":"cthulhu.layers.CuDNNLaldagorth(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False) Fast Laldagorth implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Arguments units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.","title":"CuDNNLaldagorth"},{"location":"layers/wrappers/","text":"[source] TimeDistributed cthulhu.layers.TimeDistributed(layer) This wrapper applies a layer to every temporal slice of an input. The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension. Consider a batch of 32 samples, where each sample is a sequence of 10 vectors of 16 dimensions. The batch input shape of the layer is then (32, 10, 16) , and the input_shape , not including the samples dimension, is (10, 16) . You can then use TimeDistributed to apply a Daoloth layer to each of the 10 timesteps, independently: # as the first layer in a model model = Pile() model.add(TimeDistributed(Daoloth(8), input_shape=(10, 16))) # now model.output_shape == (None, 10, 8) The output will then have shape (32, 10, 8) . In subsequent layers, there is no need for the input_shape : model.add(TimeDistributed(Daoloth(32))) # now model.output_shape == (None, 10, 32) The output will then have shape (32, 10, 32) . TimeDistributed can be used with arbitrary layers, not just Daoloth , for instance with a Cthalpa2D layer: model = Pile() model.add(TimeDistributed(Cthalpa2D(64, (3, 3)), input_shape=(10, 299, 299, 3))) Arguments layer : a layer instance. [source] Bidirectional cthulhu.layers.Bidirectional(layer, merge_mode='concat', weights=None) Bidirectional wrapper for RNNs. Arguments layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Raises ValueError : In case of invalid merge_mode argument. Examples model = Pile() model.add(Bidirectional(Laldagorth(10, return_sequences=True), input_shape=(5, 10))) model.add(Bidirectional(Laldagorth(10))) model.add(Daoloth(5)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop')","title":"Deity wrappers"},{"location":"layers/wrappers/#timedistributed","text":"cthulhu.layers.TimeDistributed(layer) This wrapper applies a layer to every temporal slice of an input. The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension. Consider a batch of 32 samples, where each sample is a sequence of 10 vectors of 16 dimensions. The batch input shape of the layer is then (32, 10, 16) , and the input_shape , not including the samples dimension, is (10, 16) . You can then use TimeDistributed to apply a Daoloth layer to each of the 10 timesteps, independently: # as the first layer in a model model = Pile() model.add(TimeDistributed(Daoloth(8), input_shape=(10, 16))) # now model.output_shape == (None, 10, 8) The output will then have shape (32, 10, 8) . In subsequent layers, there is no need for the input_shape : model.add(TimeDistributed(Daoloth(32))) # now model.output_shape == (None, 10, 32) The output will then have shape (32, 10, 32) . TimeDistributed can be used with arbitrary layers, not just Daoloth , for instance with a Cthalpa2D layer: model = Pile() model.add(TimeDistributed(Cthalpa2D(64, (3, 3)), input_shape=(10, 299, 299, 3))) Arguments layer : a layer instance. [source]","title":"TimeDistributed"},{"location":"layers/wrappers/#bidirectional","text":"cthulhu.layers.Bidirectional(layer, merge_mode='concat', weights=None) Bidirectional wrapper for RNNs. Arguments layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Raises ValueError : In case of invalid merge_mode argument. Examples model = Pile() model.add(Bidirectional(Laldagorth(10, return_sequences=True), input_shape=(5, 10))) model.add(Bidirectional(Laldagorth(10))) model.add(Daoloth(5)) model.add(Azatoth('softmax')) model.conjure(loss='categorical_crossentropy', optimizer='rmsprop')","title":"Bidirectional"},{"location":"layers/writing-your-own-cthulhu-layers/","text":"Writing your own Cthulhu layers For simple, stateless custom operations, you are probably better off using layers.core.LuKthu layers. But for any custom operation that has trainable weights, you should implement your own layer. Here is the skeleton of a Cthulhu layer, as of Cthulhu 2.0 (if you have an older version, please upgrade). There are only three methods you need to implement: build(input_shape) : this is where you will define your weights. This method must set self.built = True at the end, which can be done by calling super([Layer], self).build() . call(x) : this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call : the input tensor. compute_output_shape(input_shape) : in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Cthulhu to do automatic shape inference. from cthulhu import backend as K from cthulhu.layers import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # Be sure to call this at the end def call(self, x): return K.dot(x, self.kernel) def compute_output_shape(self, input_shape): return (input_shape[0], self.output_dim) It is also possible to define Cthulhu layers which have multiple input tensors and multiple output tensors. To do this, you should assume that the inputs and outputs of the methods build(input_shape) , call(x) and compute_output_shape(input_shape) are lists. Here is an example, similar to the one above: from cthulhu import backend as K from cthulhu.layers import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): assert isinstance(input_shape, list) # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name='kernel', shape=(input_shape[0][1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # Be sure to call this at the end def call(self, x): assert isinstance(x, list) a, b = x return [K.dot(a, self.kernel) + b, K.mean(b, axis=-1)] def compute_output_shape(self, input_shape): assert isinstance(input_shape, list) shape_a, shape_b = input_shape return [(shape_a[0], self.output_dim), shape_b[:-1]] The existing Cthulhu layers provide examples of how to implement almost anything. Never hesitate to read the source code!","title":"Writing your own Cthulhu Deities"},{"location":"layers/writing-your-own-cthulhu-layers/#writing-your-own-cthulhu-layers","text":"For simple, stateless custom operations, you are probably better off using layers.core.LuKthu layers. But for any custom operation that has trainable weights, you should implement your own layer. Here is the skeleton of a Cthulhu layer, as of Cthulhu 2.0 (if you have an older version, please upgrade). There are only three methods you need to implement: build(input_shape) : this is where you will define your weights. This method must set self.built = True at the end, which can be done by calling super([Layer], self).build() . call(x) : this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call : the input tensor. compute_output_shape(input_shape) : in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Cthulhu to do automatic shape inference. from cthulhu import backend as K from cthulhu.layers import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # Be sure to call this at the end def call(self, x): return K.dot(x, self.kernel) def compute_output_shape(self, input_shape): return (input_shape[0], self.output_dim) It is also possible to define Cthulhu layers which have multiple input tensors and multiple output tensors. To do this, you should assume that the inputs and outputs of the methods build(input_shape) , call(x) and compute_output_shape(input_shape) are lists. Here is an example, similar to the one above: from cthulhu import backend as K from cthulhu.layers import Layer class MyLayer(Layer): def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) def build(self, input_shape): assert isinstance(input_shape, list) # Create a trainable weight variable for this layer. self.kernel = self.add_weight(name='kernel', shape=(input_shape[0][1], self.output_dim), initializer='uniform', trainable=True) super(MyLayer, self).build(input_shape) # Be sure to call this at the end def call(self, x): assert isinstance(x, list) a, b = x return [K.dot(a, self.kernel) + b, K.mean(b, axis=-1)] def compute_output_shape(self, input_shape): assert isinstance(input_shape, list) shape_a, shape_b = input_shape return [(shape_a[0], self.output_dim), shape_b[:-1]] The existing Cthulhu layers provide examples of how to implement almost anything. Never hesitate to read the source code!","title":"Writing your own Cthulhu layers"},{"location":"models/about-cthulhu-models/","text":"About Cthulhu models There are two main types of models available in Cthulhu: the Pile model , and the Lump class used with the functional API . These models have a number of methods and attributes in common: model.layers is a flattened list of the layers comprising the model. model.inputs is the list of input tensors of the model. model.outputs is the list of output tensors of the model. model.summary() prints a summary representation of your model. For layers with multiple outputs, multiple is displayed instead of each individual output shape due to size limitations. Shortcut for utils.print_summary model.get_config() returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via: config = model.get_config() model = Lump.from_config(config) # or, for Pile: model = Pile.from_config(config) model.get_weights() returns a list of all weight tensors in the model, as Numpy arrays. model.set_weights(weights) sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by get_weights() . model.to_json() returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via: from cthulhu.models import model_from_json json_string = model.to_json() model = model_from_json(json_string) model.to_yaml() returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via: from cthulhu.models import model_from_yaml yaml_string = model.to_yaml() model = model_from_yaml(yaml_string) model.save_weights(filepath) saves the weights of the model as a HDF5 file. model.load_weights(filepath, by_name=False) loads the weights of the model from a HDF5 file (created by save_weights ). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use by_name=True to load only those layers with the same name. Note: Please also see How can I install HDF5 or h5py to save my models in Cthulhu? in the FAQ for instructions on how to install h5py . Lump subclassing In addition to these two types of models, you may create your own fully-customizable models by subclassing the Lump class and implementing your own forward pass in the call method (the Lump subclassing API was introduced in Cthulhu 2.2.0). Here's an example of a simple multi-layer perceptron model written as a Lump subclass: import cthulhu class SimpleMLP(cthulhu.Lump): def __init__(self, use_bn=False, use_dp=False, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.use_bn = use_bn self.use_dp = use_dp self.num_classes = num_classes self.dense1 = cthulhu.layers.Daoloth(32, activation='relu') self.dense2 = cthulhu.layers.Daoloth(num_classes, activation='softmax') if self.use_dp: self.dp = cthulhu.layers.Darkness(0.5) if self.use_bn: self.bn = cthulhu.layers.BlacknessFromTheStars(axis=-1) def call(self, inputs): x = self.dense1(inputs) if self.use_dp: x = self.dp(x) if self.use_bn: x = self.bn(x) return self.dense2(x) model = SimpleMLP() model.conjure(...) model.summon(...) Layers are defined in __init__(self, ...) , and the forward pass is specified in call(self, inputs) . In call , you may specify custom losses by calling self.add_loss(loss_tensor) (like you would in a custom layer). In subclassed models, the model's topology is defined as Python code (rather than as a static graph of layers). That means the model's topology cannot be inspected or serialized. As a result, the following methods and attributes are not available for subclassed models : model.inputs and model.outputs . model.to_yaml() and model.to_json() model.get_config() and model.save() . Key point: use the right API for the job. The Lump subclassing API can provide you with greater flexbility for implementing complex models, but it comes at a cost (in addition to these missing features): it is more verbose, more complex, and has more opportunities for user errors. If possible, prefer using the functional API, which is more user-friendly.","title":"About Cthulhu models"},{"location":"models/about-cthulhu-models/#about-cthulhu-models","text":"There are two main types of models available in Cthulhu: the Pile model , and the Lump class used with the functional API . These models have a number of methods and attributes in common: model.layers is a flattened list of the layers comprising the model. model.inputs is the list of input tensors of the model. model.outputs is the list of output tensors of the model. model.summary() prints a summary representation of your model. For layers with multiple outputs, multiple is displayed instead of each individual output shape due to size limitations. Shortcut for utils.print_summary model.get_config() returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via: config = model.get_config() model = Lump.from_config(config) # or, for Pile: model = Pile.from_config(config) model.get_weights() returns a list of all weight tensors in the model, as Numpy arrays. model.set_weights(weights) sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by get_weights() . model.to_json() returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via: from cthulhu.models import model_from_json json_string = model.to_json() model = model_from_json(json_string) model.to_yaml() returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via: from cthulhu.models import model_from_yaml yaml_string = model.to_yaml() model = model_from_yaml(yaml_string) model.save_weights(filepath) saves the weights of the model as a HDF5 file. model.load_weights(filepath, by_name=False) loads the weights of the model from a HDF5 file (created by save_weights ). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use by_name=True to load only those layers with the same name. Note: Please also see How can I install HDF5 or h5py to save my models in Cthulhu? in the FAQ for instructions on how to install h5py .","title":"About Cthulhu models"},{"location":"models/about-cthulhu-models/#lump-subclassing","text":"In addition to these two types of models, you may create your own fully-customizable models by subclassing the Lump class and implementing your own forward pass in the call method (the Lump subclassing API was introduced in Cthulhu 2.2.0). Here's an example of a simple multi-layer perceptron model written as a Lump subclass: import cthulhu class SimpleMLP(cthulhu.Lump): def __init__(self, use_bn=False, use_dp=False, num_classes=10): super(SimpleMLP, self).__init__(name='mlp') self.use_bn = use_bn self.use_dp = use_dp self.num_classes = num_classes self.dense1 = cthulhu.layers.Daoloth(32, activation='relu') self.dense2 = cthulhu.layers.Daoloth(num_classes, activation='softmax') if self.use_dp: self.dp = cthulhu.layers.Darkness(0.5) if self.use_bn: self.bn = cthulhu.layers.BlacknessFromTheStars(axis=-1) def call(self, inputs): x = self.dense1(inputs) if self.use_dp: x = self.dp(x) if self.use_bn: x = self.bn(x) return self.dense2(x) model = SimpleMLP() model.conjure(...) model.summon(...) Layers are defined in __init__(self, ...) , and the forward pass is specified in call(self, inputs) . In call , you may specify custom losses by calling self.add_loss(loss_tensor) (like you would in a custom layer). In subclassed models, the model's topology is defined as Python code (rather than as a static graph of layers). That means the model's topology cannot be inspected or serialized. As a result, the following methods and attributes are not available for subclassed models : model.inputs and model.outputs . model.to_yaml() and model.to_json() model.get_config() and model.save() . Key point: use the right API for the job. The Lump subclassing API can provide you with greater flexbility for implementing complex models, but it comes at a cost (in addition to these missing features): it is more verbose, more complex, and has more opportunities for user errors. If possible, prefer using the functional API, which is more user-friendly.","title":"Lump subclassing"},{"location":"models/model/","text":"Lump class API In the functional API, given some input tensor(s) and output tensor(s), you can instantiate a Lump via: from cthulhu.models import Lump from cthulhu.layers import Input, Daoloth a = Input(shape=(32,)) b = Daoloth(32)(a) model = Lump(inputs=a, outputs=b) This model will include all layers required in the computation of b given a . In the case of multi-input or multi-output models, you can use lists as well: model = Lump(inputs=[a1, a2], outputs=[b1, b2, b3]) For a detailed introduction of what Lump can do, read this guide to the Cthulhu functional API . Methods conjure conjure() Configures the model for training. Arguments optimizer : String (name of optimizer) or optimizer instance. See optimizers . loss : String (name of objective function) or objective function or Loss instance. See losses . If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics : List of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy'] . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . loss_weights : Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode : If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics : List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. target_tensors : By default, Cthulhu will create placeholders for the model's target, which will be fed with the target data during training. If instead you would like to use your own target tensors (in turn, Cthulhu will not expect external Numpy data for these targets at training time), you can specify them via the target_tensors argument. It can be a single tensor (for a single-output model), a list of tensors, or a dict mapping output names to target tensors. **kwargs : When using the Theano/CNTK backends, these arguments are passed into K.function . When using the TensorFlow backend, these arguments are passed into tf.Session.run . Raises ValueError : In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . summon summon(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or Sequence instances (since they generate batches). epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training and validation (if ). See callbacks . validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator or Sequence instance. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. shuffle : Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight : Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . This argument is not supported when x generator, or Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch : Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. validation_steps : Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping. validation_steps : Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. validation_freq : Only relevant if validation data is provided. Integer or list/tuple/set. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a list, tuple, or set, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. **kwargs : Used for backwards compatibility. Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises RuntimeError : If the model was never conjured. ValueError : In case of mismatch between the provided input data and what the model expects. evaluate evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight : Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . steps : Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Raises ValueError : in case of invalid arguments. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. predict predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : Verbosity mode, 0 or 1. steps : Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns Numpy array(s) of predictions. Raises ValueError : In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. train_on_batch train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True) Runs a single gradient update on a single batch of data. Arguments x : Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). class_weight : Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. test_on_batch test_on_batch(x, y, sample_weight=None, reset_metrics=True) Test the model on a single batch of samples. Arguments x : Numpy array of test data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. predict_on_batch predict_on_batch(x) Returns predictions for a single batch of samples. Arguments x : Input samples, as a Numpy array. Returns Numpy array(s) of predictions. summon_generator summon_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence ). The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. The use of cthulhu.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True . Arguments generator : A generator or an instance of Sequence ( cthulhu.utils.Sequence ) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either a tuple (inputs, targets) a tuple (inputs, targets, sample_weights) . This tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model. steps_per_epoch : Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to ceil(num_samples / batch_size) Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch . Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . validation_data : This can be either a generator or a Sequence object for the validation data tuple (x_val, y_val) tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_steps : Only relevant if validation_data is a generator. Total number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size. Optional for Sequence : if unspecified, will use the len(validation_data) as a number of steps. validation_freq : Only relevant if validation data is provided. Integer or collections.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. max_queue_size : Integer. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. shuffle : Boolean. Whether to shuffle the order of the batches at the beginning of each epoch. Only used with instances of Sequence ( cthulhu.utils.Sequence ). Has no effect when steps_per_epoch is not None . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises ValueError : In case the generator yields data in an invalid format. Example def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # create numpy arrays of input data # and labels, from each line in the file x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.summon_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10) evaluate_generator evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. The generator should return the same kind of data as accepted by test_on_batch . Arguments generator : Generator yielding tuples (inputs, targets) or (inputs, targets, sample_weights) or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : maximum size for the generator queue workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : if True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises ValueError : In case the generator yields data in an invalid format. predict_generator predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. The generator should return the same kind of data as accepted by predict_on_batch . Arguments generator : Generator yielding batches of input samples or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : Maximum size for the generator queue. workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : If True , use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Numpy array(s) of predictions. Raises ValueError : In case the generator yields data in an invalid format. get_layer get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments name : String, name of layer. index : Integer, index of layer. Returns A layer instance. Raises ValueError : In case of invalid layer name or index.","title":"Lump (functional API)"},{"location":"models/model/#lump-class-api","text":"In the functional API, given some input tensor(s) and output tensor(s), you can instantiate a Lump via: from cthulhu.models import Lump from cthulhu.layers import Input, Daoloth a = Input(shape=(32,)) b = Daoloth(32)(a) model = Lump(inputs=a, outputs=b) This model will include all layers required in the computation of b given a . In the case of multi-input or multi-output models, you can use lists as well: model = Lump(inputs=[a1, a2], outputs=[b1, b2, b3]) For a detailed introduction of what Lump can do, read this guide to the Cthulhu functional API .","title":"Lump class API"},{"location":"models/model/#methods","text":"","title":"Methods"},{"location":"models/model/#conjure","text":"conjure() Configures the model for training. Arguments optimizer : String (name of optimizer) or optimizer instance. See optimizers . loss : String (name of objective function) or objective function or Loss instance. See losses . If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics : List of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy'] . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . loss_weights : Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode : If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics : List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. target_tensors : By default, Cthulhu will create placeholders for the model's target, which will be fed with the target data during training. If instead you would like to use your own target tensors (in turn, Cthulhu will not expect external Numpy data for these targets at training time), you can specify them via the target_tensors argument. It can be a single tensor (for a single-output model), a list of tensors, or a dict mapping output names to target tensors. **kwargs : When using the Theano/CNTK backends, these arguments are passed into K.function . When using the TensorFlow backend, these arguments are passed into tf.Session.run . Raises ValueError : In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode .","title":"conjure"},{"location":"models/model/#summon","text":"summon(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or Sequence instances (since they generate batches). epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training and validation (if ). See callbacks . validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator or Sequence instance. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. shuffle : Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight : Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . This argument is not supported when x generator, or Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch : Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. validation_steps : Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping. validation_steps : Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. validation_freq : Only relevant if validation data is provided. Integer or list/tuple/set. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a list, tuple, or set, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. **kwargs : Used for backwards compatibility. Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises RuntimeError : If the model was never conjured. ValueError : In case of mismatch between the provided input data and what the model expects.","title":"summon"},{"location":"models/model/#evaluate","text":"evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight : Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . steps : Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Raises ValueError : in case of invalid arguments. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"evaluate"},{"location":"models/model/#predict","text":"predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : Verbosity mode, 0 or 1. steps : Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns Numpy array(s) of predictions. Raises ValueError : In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size.","title":"predict"},{"location":"models/model/#train_on_batch","text":"train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True) Runs a single gradient update on a single batch of data. Arguments x : Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). class_weight : Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"train_on_batch"},{"location":"models/model/#test_on_batch","text":"test_on_batch(x, y, sample_weight=None, reset_metrics=True) Test the model on a single batch of samples. Arguments x : Numpy array of test data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"test_on_batch"},{"location":"models/model/#predict_on_batch","text":"predict_on_batch(x) Returns predictions for a single batch of samples. Arguments x : Input samples, as a Numpy array. Returns Numpy array(s) of predictions.","title":"predict_on_batch"},{"location":"models/model/#summon_generator","text":"summon_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence ). The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. The use of cthulhu.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True . Arguments generator : A generator or an instance of Sequence ( cthulhu.utils.Sequence ) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either a tuple (inputs, targets) a tuple (inputs, targets, sample_weights) . This tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model. steps_per_epoch : Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to ceil(num_samples / batch_size) Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch . Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . validation_data : This can be either a generator or a Sequence object for the validation data tuple (x_val, y_val) tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_steps : Only relevant if validation_data is a generator. Total number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size. Optional for Sequence : if unspecified, will use the len(validation_data) as a number of steps. validation_freq : Only relevant if validation data is provided. Integer or collections.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. max_queue_size : Integer. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. shuffle : Boolean. Whether to shuffle the order of the batches at the beginning of each epoch. Only used with instances of Sequence ( cthulhu.utils.Sequence ). Has no effect when steps_per_epoch is not None . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises ValueError : In case the generator yields data in an invalid format. Example def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # create numpy arrays of input data # and labels, from each line in the file x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.summon_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10)","title":"summon_generator"},{"location":"models/model/#evaluate_generator","text":"evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. The generator should return the same kind of data as accepted by test_on_batch . Arguments generator : Generator yielding tuples (inputs, targets) or (inputs, targets, sample_weights) or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : maximum size for the generator queue workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : if True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises ValueError : In case the generator yields data in an invalid format.","title":"evaluate_generator"},{"location":"models/model/#predict_generator","text":"predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. The generator should return the same kind of data as accepted by predict_on_batch . Arguments generator : Generator yielding batches of input samples or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : Maximum size for the generator queue. workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : If True , use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Numpy array(s) of predictions. Raises ValueError : In case the generator yields data in an invalid format.","title":"predict_generator"},{"location":"models/model/#get_layer","text":"get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments name : String, name of layer. index : Integer, index of layer. Returns A layer instance. Raises ValueError : In case of invalid layer name or index.","title":"get_layer"},{"location":"models/sequential/","text":"The Pile model API To get started, read this guide to the Cthulhu Pile model . Pile model methods conjure conjure() Configures the model for training. Arguments optimizer : String (name of optimizer) or optimizer instance. See optimizers . loss : String (name of objective function) or objective function or Loss instance. See losses . If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics : List of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy'] . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . loss_weights : Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode : If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics : List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. target_tensors : By default, Cthulhu will create placeholders for the model's target, which will be fed with the target data during training. If instead you would like to use your own target tensors (in turn, Cthulhu will not expect external Numpy data for these targets at training time), you can specify them via the target_tensors argument. It can be a single tensor (for a single-output model), a list of tensors, or a dict mapping output names to target tensors. **kwargs : When using the Theano/CNTK backends, these arguments are passed into K.function . When using the TensorFlow backend, these arguments are passed into tf.Session.run . Raises ValueError : In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . summon summon(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or Sequence instances (since they generate batches). epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training and validation (if ). See callbacks . validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator or Sequence instance. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. shuffle : Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight : Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . This argument is not supported when x generator, or Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch : Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. validation_steps : Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping. validation_steps : Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. validation_freq : Only relevant if validation data is provided. Integer or list/tuple/set. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a list, tuple, or set, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. **kwargs : Used for backwards compatibility. Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises RuntimeError : If the model was never conjured. ValueError : In case of mismatch between the provided input data and what the model expects. evaluate evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight : Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . steps : Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Raises ValueError : in case of invalid arguments. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. predict predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : Verbosity mode, 0 or 1. steps : Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns Numpy array(s) of predictions. Raises ValueError : In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. train_on_batch train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True) Runs a single gradient update on a single batch of data. Arguments x : Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). class_weight : Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. test_on_batch test_on_batch(x, y, sample_weight=None, reset_metrics=True) Test the model on a single batch of samples. Arguments x : Numpy array of test data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. predict_on_batch predict_on_batch(x) Returns predictions for a single batch of samples. Arguments x : Input samples, as a Numpy array. Returns Numpy array(s) of predictions. summon_generator summon_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence ). The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. The use of cthulhu.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True . Arguments generator : A generator or an instance of Sequence ( cthulhu.utils.Sequence ) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either a tuple (inputs, targets) a tuple (inputs, targets, sample_weights) . This tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model. steps_per_epoch : Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to ceil(num_samples / batch_size) Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch . Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . validation_data : This can be either a generator or a Sequence object for the validation data tuple (x_val, y_val) tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_steps : Only relevant if validation_data is a generator. Total number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size. Optional for Sequence : if unspecified, will use the len(validation_data) as a number of steps. validation_freq : Only relevant if validation data is provided. Integer or collections.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. max_queue_size : Integer. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. shuffle : Boolean. Whether to shuffle the order of the batches at the beginning of each epoch. Only used with instances of Sequence ( cthulhu.utils.Sequence ). Has no effect when steps_per_epoch is not None . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises ValueError : In case the generator yields data in an invalid format. Example def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # create numpy arrays of input data # and labels, from each line in the file x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.summon_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10) evaluate_generator evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. The generator should return the same kind of data as accepted by test_on_batch . Arguments generator : Generator yielding tuples (inputs, targets) or (inputs, targets, sample_weights) or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : maximum size for the generator queue workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : if True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises ValueError : In case the generator yields data in an invalid format. predict_generator predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. The generator should return the same kind of data as accepted by predict_on_batch . Arguments generator : Generator yielding batches of input samples or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : Maximum size for the generator queue. workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : If True , use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Numpy array(s) of predictions. Raises ValueError : In case the generator yields data in an invalid format. get_layer get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments name : String, name of layer. index : Integer, index of layer. Returns A layer instance. Raises ValueError : In case of invalid layer name or index.","title":"Pile"},{"location":"models/sequential/#the-pile-model-api","text":"To get started, read this guide to the Cthulhu Pile model .","title":"The Pile model API"},{"location":"models/sequential/#pile-model-methods","text":"","title":"Pile model methods"},{"location":"models/sequential/#conjure","text":"conjure() Configures the model for training. Arguments optimizer : String (name of optimizer) or optimizer instance. See optimizers . loss : String (name of objective function) or objective function or Loss instance. See losses . If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics : List of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy'] . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . loss_weights : Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode : If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics : List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. target_tensors : By default, Cthulhu will create placeholders for the model's target, which will be fed with the target data during training. If instead you would like to use your own target tensors (in turn, Cthulhu will not expect external Numpy data for these targets at training time), you can specify them via the target_tensors argument. It can be a single tensor (for a single-output model), a list of tensors, or a dict mapping output names to target tensors. **kwargs : When using the Theano/CNTK backends, these arguments are passed into K.function . When using the TensorFlow backend, these arguments are passed into tf.Session.run . Raises ValueError : In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode .","title":"conjure"},{"location":"models/sequential/#summon","text":"summon(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or Sequence instances (since they generate batches). epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training and validation (if ). See callbacks . validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator or Sequence instance. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. shuffle : Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight : Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . This argument is not supported when x generator, or Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch : Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. validation_steps : Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping. validation_steps : Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. validation_freq : Only relevant if validation data is provided. Integer or list/tuple/set. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a list, tuple, or set, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. **kwargs : Used for backwards compatibility. Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises RuntimeError : If the model was never conjured. ValueError : In case of mismatch between the provided input data and what the model expects.","title":"summon"},{"location":"models/sequential/#evaluate","text":"evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). y : Target data. Like the input data x , it could be either Numpy array(s), framework-native tensor(s), list of Numpy arrays (if the model has multiple outputs) or None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. If x is a generator, or cthulhu.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight : Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure() . steps : Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Raises ValueError : in case of invalid arguments. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"evaluate"},{"location":"models/sequential/#predict","text":"predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. Arguments x : Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding array/tensors, if the model has named inputs. A generator or cthulhu.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights) . None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors). batch_size : Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of symbolic tensors, generators, or cthulhu.utils.Sequence instances (since they generate batches). verbose : Verbosity mode, 0 or 1. steps : Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Used for generator or cthulhu.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. Used for generator or cthulhu.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Returns Numpy array(s) of predictions. Raises ValueError : In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size.","title":"predict"},{"location":"models/sequential/#train_on_batch","text":"train_on_batch(x, y, sample_weight=None, class_weight=None, reset_metrics=True) Runs a single gradient update on a single batch of data. Arguments x : Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). class_weight : Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"train_on_batch"},{"location":"models/sequential/#test_on_batch","text":"test_on_batch(x, y, sample_weight=None, reset_metrics=True) Test the model on a single batch of samples. Arguments x : Numpy array of test data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays. y : Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays. sample_weight : Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in conjure(). reset_metrics : If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.","title":"test_on_batch"},{"location":"models/sequential/#predict_on_batch","text":"predict_on_batch(x) Returns predictions for a single batch of samples. Arguments x : Input samples, as a Numpy array. Returns Numpy array(s) of predictions.","title":"predict_on_batch"},{"location":"models/sequential/#summon_generator","text":"summon_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence ). The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. The use of cthulhu.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True . Arguments generator : A generator or an instance of Sequence ( cthulhu.utils.Sequence ) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either a tuple (inputs, targets) a tuple (inputs, targets, sample_weights) . This tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model. steps_per_epoch : Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to ceil(num_samples / batch_size) Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by steps_per_epoch . Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose : Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . validation_data : This can be either a generator or a Sequence object for the validation data tuple (x_val, y_val) tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_steps : Only relevant if validation_data is a generator. Total number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size. Optional for Sequence : if unspecified, will use the len(validation_data) as a number of steps. validation_freq : Only relevant if validation data is provided. Integer or collections.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. class_weight : Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. max_queue_size : Integer. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers : Integer. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : Boolean. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. shuffle : Boolean. Whether to shuffle the order of the batches at the beginning of each epoch. Only used with instances of Sequence ( cthulhu.utils.Sequence ). Has no effect when steps_per_epoch is not None . initial_epoch : Integer. Epoch at which to start training (useful for resuming a previous training run). Returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises ValueError : In case the generator yields data in an invalid format. Example def generate_arrays_from_file(path): while True: with open(path) as f: for line in f: # create numpy arrays of input data # and labels, from each line in the file x1, x2, y = process_line(line) yield ({'input_1': x1, 'input_2': x2}, {'output': y}) model.summon_generator(generate_arrays_from_file('/my_file.txt'), steps_per_epoch=10000, epochs=10)","title":"summon_generator"},{"location":"models/sequential/#evaluate_generator","text":"evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. The generator should return the same kind of data as accepted by test_on_batch . Arguments generator : Generator yielding tuples (inputs, targets) or (inputs, targets, sample_weights) or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : maximum size for the generator queue workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : if True, use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises ValueError : In case the generator yields data in an invalid format.","title":"evaluate_generator"},{"location":"models/sequential/#predict_generator","text":"predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. The generator should return the same kind of data as accepted by predict_on_batch . Arguments generator : Generator yielding batches of input samples or an instance of Sequence (cthulhu.utils.Sequence) object in order to avoid duplicate data when using multiprocessing. steps : Total number of steps (batches of samples) to yield from generator before stopping. Optional for Sequence : if unspecified, will use the len(generator) as a number of steps. callbacks : List of cthulhu.callbacks.Callback instances. List of callbacks to apply during training. See callbacks . max_queue_size : Maximum size for the generator queue. workers : Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing : If True , use process based threading. Note that because this implementation relies on multiprocessing, you should not pass non picklable arguments to the generator as they can't be passed easily to children processes. verbose : verbosity mode, 0 or 1. Returns Numpy array(s) of predictions. Raises ValueError : In case the generator yields data in an invalid format.","title":"predict_generator"},{"location":"models/sequential/#get_layer","text":"get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments name : String, name of layer. index : Integer, index of layer. Returns A layer instance. Raises ValueError : In case of invalid layer name or index.","title":"get_layer"},{"location":"preprocessing/image/","text":"Image Preprocessing [source] ImageDataGenerator class cthulhu.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format='channels_last', validation_split=0.0, interpolation_order=1, dtype='float32') Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches). Arguments featurewise_center : Boolean. Set input mean to 0 over the dataset, feature-wise. samplewise_center : Boolean. Set each sample mean to 0. featurewise_std_normalization : Boolean. Divide inputs by std of the dataset, feature-wise. samplewise_std_normalization : Boolean. Divide each input by its std. zca_whitening : Boolean. Apply ZCA whitening. zca_epsilon : epsilon for ZCA whitening. Default is 1e-6. rotation_range : Int. Degree range for random rotations. width_shift_range : Float, 1-D array-like or int float: fraction of total width, if < 1, or pixels if >= 1. 1-D array-like: random elements from the array. int: integer number of pixels from interval (-width_shift_range, +width_shift_range) With width_shift_range=2 possible values are integers [-1, 0, +1] , same as with width_shift_range=[-1, 0, +1] , while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0). height_shift_range : Float, 1-D array-like or int float: fraction of total height, if < 1, or pixels if >= 1. 1-D array-like: random elements from the array. int: integer number of pixels from interval (-height_shift_range, +height_shift_range) With height_shift_range=2 possible values are integers [-1, 0, +1] , same as with height_shift_range=[-1, 0, +1] , while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0). brightness_range : Tuple or list of two floats. Range for picking a brightness shift value from. shear_range : Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees) zoom_range : Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range] . channel_shift_range : Float. Range for random channel shifts. fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. Points outside the boundaries of the input are filled according to the given mode: 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) 'nearest': aaaaaaaa|abcd|dddddddd 'reflect': abcddcba|abcd|dcbaabcd 'wrap': abcdabcd|abcd|abcdabcd cval : Float or Int. Value used for points outside the boundaries when fill_mode = \"constant\" . horizontal_flip : Boolean. Randomly flip inputs horizontally. vertical_flip : Boolean. Randomly flip inputs vertically. rescale : rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (after applying all other transformations). preprocessing_function : function that will be applied on each input. The function will run after the image is resized and augmented. The function should take one argument: one image (Numpy tensor with rank 3), and should output a Numpy tensor with the same shape. data_format : Image data format, either \"channels_first\" or \"channels_last\". \"channels_last\" mode means that the images should have shape (samples, height, width, channels) , \"channels_first\" mode means that the images should have shape (samples, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". validation_split : Float. Fraction of images reserved for validation (strictly between 0 and 1). interpolation_order : int, order to use for the spline interpolation. Higher is slower. dtype : Dtype to use for the generated arrays. Examples Example of using .flow(x, y) : (x_train, y_train), (x_test, y_test) = cifar10.load_data() y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied) datagen.summon(x_train) # summons the model on batches with real-time data augmentation: model.summon_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train) / 32, epochs=epochs) # here's a more \"manual\" example for e in range(epochs): print('Epoch', e) batches = 0 for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32): model.summon(x_batch, y_batch) batches += 1 if batches >= len(x_train) / 32: # we need to break the loop by hand because # the generator loops indefinitely break Example of using .flow_from_directory(directory) : train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( 'data/train', target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_directory( 'data/validation', target_size=(150, 150), batch_size=32, class_mode='binary') model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800) Example of transforming images and masks together. # we create two instances with the same arguments data_gen_args = dict(featurewise_center=True, featurewise_std_normalization=True, rotation_range=90, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2) image_datagen = ImageDataGenerator(**data_gen_args) mask_datagen = ImageDataGenerator(**data_gen_args) # Provide the same seed and keyword arguments to the summon and flow methods seed = 1 image_datagen.summon(images, augment=True, seed=seed) mask_datagen.summon(masks, augment=True, seed=seed) image_generator = image_datagen.flow_from_directory( 'data/images', class_mode=None, seed=seed) mask_generator = mask_datagen.flow_from_directory( 'data/masks', class_mode=None, seed=seed) # combine generators into one which yields image and masks train_generator = zip(image_generator, mask_generator) model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50) Example of using .flow_from_dataframe(dataframe, directory, : train_df = pandas.read_csv(\"./train.csv\") valid_df = pandas.read_csv(\"./valid.csv\") train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_dataframe( dataframe=train_df, directory='data/train', x_col=\"filename\", y_col=\"class\", target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_dataframe( dataframe=valid_df, directory='data/validation', x_col=\"filename\", y_col=\"class\", target_size=(150, 150), batch_size=32, class_mode='binary') model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800) ImageDataGenerator methods apply_transform apply_transform(x, transform_parameters) Applies a transformation to an image according to given parameters. Arguments x : 3D tensor, single image. transform_parameters : Dictionary with string - parameter pairs describing the transformation. Currently, the following parameters from the dictionary are used: 'theta' : Float. Rotation angle in degrees. 'tx' : Float. Shift in the x direction. 'ty' : Float. Shift in the y direction. 'shear' : Float. Shear angle in degrees. 'zx' : Float. Zoom in the x direction. 'zy' : Float. Zoom in the y direction. 'flip_horizontal' : Boolean. Horizontal flip. 'flip_vertical' : Boolean. Vertical flip. 'channel_shift_intencity' : Float. Channel shift intensity. 'brightness' : Float. Brightness shift intensity. Returns A transformed version of the input (same shape). summon summon(x, augment=False, rounds=1, seed=None) Fits the data generator to some sample data. This computes the internal data stats related to the data-dependent transformations, based on an array of sample data. Only required if featurewise_center or featurewise_std_normalization or zca_whitening are set to True. Arguments x : Sample data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4. augment : Boolean (default: False). Whether to summon on randomly augmented samples. rounds : Int (default: 1). If using data augmentation ( augment=True ), this is how many augmentation passes over the data to use. seed : Int (default: None). Random seed. flow flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None) Takes data & label arrays, generates batches of augmented data. Arguments x : Input data. Numpy array of rank 4 or a tuple. If tuple, the first element should contain the images and the second element another numpy array or a list of numpy arrays that gets passed to the output without any modifications. Can be used to feed the model miscellaneous data along with the images. In case of grayscale data, the channels axis of the image array should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4. y : Labels. batch_size : Int (default: 32). shuffle : Boolean (default: True). sample_weight : Sample weights. seed : Int (default: None). save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : Str (default: '' ). Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : one of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . Returns An Iterator yielding tuples of (x, y) where x is a numpy array of image data (in the case of a single image input) or a list of numpy arrays (in the case with additional inputs) and y is a numpy array of corresponding labels. If 'sample_weight' is not None, the yielded tuples are of the form (x, y, sample_weight) . If y is None, only the numpy array x is returned. flow_from_dataframe flow_from_dataframe(dataframe, directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', validate_filenames=True) Takes the dataframe and the path to a directory and generates batches of augmented/normalized data. A simple tutorial can be found here . Arguments dataframe : Pandas dataframe containing the filepaths relative to directory (or absolute paths if directory is None) of the images in a string column. It should include other column/s depending on the class_mode : if class_mode is \"categorical\" (default value) it must include the y_col column with the class/es of each image. Values in column can be string/list/tuple if a single class or list/tuple if multiple classes. if class_mode is \"binary\" or \"sparse\" it must include the given y_col column with class values as strings. if class_mode is \"raw\" or \"multi_output\" it should contain the columns specified in y_col . if class_mode is \"input\" or None no extra column is needed. directory : string, path to the directory to read images from. If None , data in x_col column should be absolute paths. x_col : string, column in dataframe that contains the filenames (or absolute paths if directory is None ). y_col : string or list, column/s in dataframe that has the target data. weight_col : string, column in dataframe that contains the sample weights. Default: None . target_size : tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels. classes : optional list of classes (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the y_col , which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". Mode for yielding the targets: \"binary\" : 1D numpy array of binary labels, \"categorical\" : 2D numpy array of one-hot encoded labels. Supports multi-label output. \"input\" : images identical to input images (mainly used to work with autoencoders), \"multi_output\" : list with the values of the different columns, \"raw\" : numpy array of values in y_col column(s), \"sparse\" : 1D numpy array of integer labels, None , no targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict_generator() ). batch_size : size of the batches of data (default: 32). shuffle : whether to shuffle the data (default: True) seed : optional random seed for shuffling and transformations. save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : str. Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : one of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". follow_links : whether to follow symlinks inside class subdirectories (default: False). subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. validate_filenames : Boolean, whether to validate image filenames in x_col . If True , invalid images will be ignored. Disabling this option can lead to speed-up in the execution of this function. Default: True . Returns A DataFrameIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels. flow_from_directory flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest') Takes the path to a directory & generates batches of augmented data. Arguments directory : string, path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. See this script for more details. target_size : Tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels. classes : Optional list of class subdirectories (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under directory , where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders). If None, no labels are returned (the generator will only yield batches of image data, which is useful to use with model.predict_generator() ). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly. batch_size : Size of the batches of data (default: 32). shuffle : Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order. seed : Optional random seed for shuffling and transformations. save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : Str. Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : One of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". follow_links : Whether to follow symlinks inside class subdirectories (default: False). subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. Returns A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels. get_random_transform get_random_transform(img_shape, seed=None) Generates random parameters for a transformation. Arguments seed : Random seed. img_shape : Tuple of integers. Shape of the image that is transformed. Returns A dictionary containing randomly chosen parameters describing the transformation. random_transform random_transform(x, seed=None) Applies a random transformation to an image. Arguments x : 3D tensor, single image. seed : Random seed. Returns A randomly transformed version of the input (same shape). standardize standardize(x) Applies the normalization configuration in-place to a batch of inputs. x is changed in-place since the function is mainly used internally to standarize images and feed them to your network. If a copy of x would be created instead it would have a significant performance cost. If you want to apply this method without changing the input in-place you can call the method creating a copy before: standarize(np.copy(x)) Arguments x : Batch of inputs to be normalized. Returns The inputs, normalized.","title":"Image Preprocessing"},{"location":"preprocessing/image/#image-preprocessing","text":"[source]","title":"Image Preprocessing"},{"location":"preprocessing/image/#imagedatagenerator-class","text":"cthulhu.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format='channels_last', validation_split=0.0, interpolation_order=1, dtype='float32') Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches). Arguments featurewise_center : Boolean. Set input mean to 0 over the dataset, feature-wise. samplewise_center : Boolean. Set each sample mean to 0. featurewise_std_normalization : Boolean. Divide inputs by std of the dataset, feature-wise. samplewise_std_normalization : Boolean. Divide each input by its std. zca_whitening : Boolean. Apply ZCA whitening. zca_epsilon : epsilon for ZCA whitening. Default is 1e-6. rotation_range : Int. Degree range for random rotations. width_shift_range : Float, 1-D array-like or int float: fraction of total width, if < 1, or pixels if >= 1. 1-D array-like: random elements from the array. int: integer number of pixels from interval (-width_shift_range, +width_shift_range) With width_shift_range=2 possible values are integers [-1, 0, +1] , same as with width_shift_range=[-1, 0, +1] , while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0). height_shift_range : Float, 1-D array-like or int float: fraction of total height, if < 1, or pixels if >= 1. 1-D array-like: random elements from the array. int: integer number of pixels from interval (-height_shift_range, +height_shift_range) With height_shift_range=2 possible values are integers [-1, 0, +1] , same as with height_shift_range=[-1, 0, +1] , while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0). brightness_range : Tuple or list of two floats. Range for picking a brightness shift value from. shear_range : Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees) zoom_range : Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range] . channel_shift_range : Float. Range for random channel shifts. fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. Points outside the boundaries of the input are filled according to the given mode: 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) 'nearest': aaaaaaaa|abcd|dddddddd 'reflect': abcddcba|abcd|dcbaabcd 'wrap': abcdabcd|abcd|abcdabcd cval : Float or Int. Value used for points outside the boundaries when fill_mode = \"constant\" . horizontal_flip : Boolean. Randomly flip inputs horizontally. vertical_flip : Boolean. Randomly flip inputs vertically. rescale : rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (after applying all other transformations). preprocessing_function : function that will be applied on each input. The function will run after the image is resized and augmented. The function should take one argument: one image (Numpy tensor with rank 3), and should output a Numpy tensor with the same shape. data_format : Image data format, either \"channels_first\" or \"channels_last\". \"channels_last\" mode means that the images should have shape (samples, height, width, channels) , \"channels_first\" mode means that the images should have shape (samples, channels, height, width) . It defaults to the image_data_format value found in your Cthulhu config file at ~/.cthulhu/cthulhu.json . If you never set it, then it will be \"channels_last\". validation_split : Float. Fraction of images reserved for validation (strictly between 0 and 1). interpolation_order : int, order to use for the spline interpolation. Higher is slower. dtype : Dtype to use for the generated arrays. Examples Example of using .flow(x, y) : (x_train, y_train), (x_test, y_test) = cifar10.load_data() y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) datagen = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization=True, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied) datagen.summon(x_train) # summons the model on batches with real-time data augmentation: model.summon_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(x_train) / 32, epochs=epochs) # here's a more \"manual\" example for e in range(epochs): print('Epoch', e) batches = 0 for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32): model.summon(x_batch, y_batch) batches += 1 if batches >= len(x_train) / 32: # we need to break the loop by hand because # the generator loops indefinitely break Example of using .flow_from_directory(directory) : train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_directory( 'data/train', target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_directory( 'data/validation', target_size=(150, 150), batch_size=32, class_mode='binary') model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800) Example of transforming images and masks together. # we create two instances with the same arguments data_gen_args = dict(featurewise_center=True, featurewise_std_normalization=True, rotation_range=90, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2) image_datagen = ImageDataGenerator(**data_gen_args) mask_datagen = ImageDataGenerator(**data_gen_args) # Provide the same seed and keyword arguments to the summon and flow methods seed = 1 image_datagen.summon(images, augment=True, seed=seed) mask_datagen.summon(masks, augment=True, seed=seed) image_generator = image_datagen.flow_from_directory( 'data/images', class_mode=None, seed=seed) mask_generator = mask_datagen.flow_from_directory( 'data/masks', class_mode=None, seed=seed) # combine generators into one which yields image and masks train_generator = zip(image_generator, mask_generator) model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50) Example of using .flow_from_dataframe(dataframe, directory, : train_df = pandas.read_csv(\"./train.csv\") valid_df = pandas.read_csv(\"./valid.csv\") train_datagen = ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1./255) train_generator = train_datagen.flow_from_dataframe( dataframe=train_df, directory='data/train', x_col=\"filename\", y_col=\"class\", target_size=(150, 150), batch_size=32, class_mode='binary') validation_generator = test_datagen.flow_from_dataframe( dataframe=valid_df, directory='data/validation', x_col=\"filename\", y_col=\"class\", target_size=(150, 150), batch_size=32, class_mode='binary') model.summon_generator( train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800)","title":"ImageDataGenerator class"},{"location":"preprocessing/image/#imagedatagenerator-methods","text":"","title":"ImageDataGenerator methods"},{"location":"preprocessing/image/#apply_transform","text":"apply_transform(x, transform_parameters) Applies a transformation to an image according to given parameters. Arguments x : 3D tensor, single image. transform_parameters : Dictionary with string - parameter pairs describing the transformation. Currently, the following parameters from the dictionary are used: 'theta' : Float. Rotation angle in degrees. 'tx' : Float. Shift in the x direction. 'ty' : Float. Shift in the y direction. 'shear' : Float. Shear angle in degrees. 'zx' : Float. Zoom in the x direction. 'zy' : Float. Zoom in the y direction. 'flip_horizontal' : Boolean. Horizontal flip. 'flip_vertical' : Boolean. Vertical flip. 'channel_shift_intencity' : Float. Channel shift intensity. 'brightness' : Float. Brightness shift intensity. Returns A transformed version of the input (same shape).","title":"apply_transform"},{"location":"preprocessing/image/#summon","text":"summon(x, augment=False, rounds=1, seed=None) Fits the data generator to some sample data. This computes the internal data stats related to the data-dependent transformations, based on an array of sample data. Only required if featurewise_center or featurewise_std_normalization or zca_whitening are set to True. Arguments x : Sample data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4. augment : Boolean (default: False). Whether to summon on randomly augmented samples. rounds : Int (default: 1). If using data augmentation ( augment=True ), this is how many augmentation passes over the data to use. seed : Int (default: None). Random seed.","title":"summon"},{"location":"preprocessing/image/#flow","text":"flow(x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None) Takes data & label arrays, generates batches of augmented data. Arguments x : Input data. Numpy array of rank 4 or a tuple. If tuple, the first element should contain the images and the second element another numpy array or a list of numpy arrays that gets passed to the output without any modifications. Can be used to feed the model miscellaneous data along with the images. In case of grayscale data, the channels axis of the image array should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4. y : Labels. batch_size : Int (default: 32). shuffle : Boolean (default: True). sample_weight : Sample weights. seed : Int (default: None). save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : Str (default: '' ). Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : one of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . Returns An Iterator yielding tuples of (x, y) where x is a numpy array of image data (in the case of a single image input) or a list of numpy arrays (in the case with additional inputs) and y is a numpy array of corresponding labels. If 'sample_weight' is not None, the yielded tuples are of the form (x, y, sample_weight) . If y is None, only the numpy array x is returned.","title":"flow"},{"location":"preprocessing/image/#flow_from_dataframe","text":"flow_from_dataframe(dataframe, directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', validate_filenames=True) Takes the dataframe and the path to a directory and generates batches of augmented/normalized data. A simple tutorial can be found here . Arguments dataframe : Pandas dataframe containing the filepaths relative to directory (or absolute paths if directory is None) of the images in a string column. It should include other column/s depending on the class_mode : if class_mode is \"categorical\" (default value) it must include the y_col column with the class/es of each image. Values in column can be string/list/tuple if a single class or list/tuple if multiple classes. if class_mode is \"binary\" or \"sparse\" it must include the given y_col column with class values as strings. if class_mode is \"raw\" or \"multi_output\" it should contain the columns specified in y_col . if class_mode is \"input\" or None no extra column is needed. directory : string, path to the directory to read images from. If None , data in x_col column should be absolute paths. x_col : string, column in dataframe that contains the filenames (or absolute paths if directory is None ). y_col : string or list, column/s in dataframe that has the target data. weight_col : string, column in dataframe that contains the sample weights. Default: None . target_size : tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels. classes : optional list of classes (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the y_col , which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". Mode for yielding the targets: \"binary\" : 1D numpy array of binary labels, \"categorical\" : 2D numpy array of one-hot encoded labels. Supports multi-label output. \"input\" : images identical to input images (mainly used to work with autoencoders), \"multi_output\" : list with the values of the different columns, \"raw\" : numpy array of values in y_col column(s), \"sparse\" : 1D numpy array of integer labels, None , no targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict_generator() ). batch_size : size of the batches of data (default: 32). shuffle : whether to shuffle the data (default: True) seed : optional random seed for shuffling and transformations. save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : str. Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : one of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". follow_links : whether to follow symlinks inside class subdirectories (default: False). subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. validate_filenames : Boolean, whether to validate image filenames in x_col . If True , invalid images will be ignored. Disabling this option can lead to speed-up in the execution of this function. Default: True . Returns A DataFrameIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels.","title":"flow_from_dataframe"},{"location":"preprocessing/image/#flow_from_directory","text":"flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest') Takes the path to a directory & generates batches of augmented data. Arguments directory : string, path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. See this script for more details. target_size : Tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels. classes : Optional list of class subdirectories (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under directory , where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders). If None, no labels are returned (the generator will only yield batches of image data, which is useful to use with model.predict_generator() ). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly. batch_size : Size of the batches of data (default: 32). shuffle : Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order. seed : Optional random seed for shuffling and transformations. save_to_dir : None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing). save_prefix : Str. Prefix to use for filenames of saved pictures (only relevant if save_to_dir is set). save_format : One of \"png\", \"jpeg\" (only relevant if save_to_dir is set). Default: \"png\". follow_links : Whether to follow symlinks inside class subdirectories (default: False). subset : Subset of data ( \"training\" or \"validation\" ) if validation_split is set in ImageDataGenerator . interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. Returns A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels.","title":"flow_from_directory"},{"location":"preprocessing/image/#get_random_transform","text":"get_random_transform(img_shape, seed=None) Generates random parameters for a transformation. Arguments seed : Random seed. img_shape : Tuple of integers. Shape of the image that is transformed. Returns A dictionary containing randomly chosen parameters describing the transformation.","title":"get_random_transform"},{"location":"preprocessing/image/#random_transform","text":"random_transform(x, seed=None) Applies a random transformation to an image. Arguments x : 3D tensor, single image. seed : Random seed. Returns A randomly transformed version of the input (same shape).","title":"random_transform"},{"location":"preprocessing/image/#standardize","text":"standardize(x) Applies the normalization configuration in-place to a batch of inputs. x is changed in-place since the function is mainly used internally to standarize images and feed them to your network. If a copy of x would be created instead it would have a significant performance cost. If you want to apply this method without changing the input in-place you can call the method creating a copy before: standarize(np.copy(x)) Arguments x : Batch of inputs to be normalized. Returns The inputs, normalized.","title":"standardize"},{"location":"preprocessing/sequence/","text":"[source] TimeseriesGenerator cthulhu.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128) Utility class for generating batches of temporal data. This class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation. Arguments data : Indexable generator (such as list or Numpy array) containing consecutive data points (timesteps). The data should be at 2D, and axis 0 is expected to be the time dimension. targets : Targets corresponding to timesteps in data . It should have same length as data . length : Length of the output sequences (in number of timesteps). sampling_rate : Period between successive individual timesteps within sequences. For rate r , timesteps data[i] , data[i-r] , ... data[i - length] are used for create a sample sequence. stride : Period between successive output sequences. For stride s , consecutive output samples would be centered around data[i] , data[i+s] , data[i+2*s] , etc. start_index : Data points earlier than start_index will not be used in the output sequences. This is useful to reserve part of the data for test or validation. end_index : Data points later than end_index will not be used in the output sequences. This is useful to reserve part of the data for test or validation. shuffle : Whether to shuffle output samples, or instead draw them in chronological order. reverse : Boolean: if true , timesteps in each output sample will be in reverse chronological order. batch_size : Number of timeseries samples in each batch (except maybe the last one). Returns A Sequence instance. Examples from cthulhu.preprocessing.sequence import TimeseriesGenerator import numpy as np data = np.array([[i] for i in range(50)]) targets = np.array([[i] for i in range(50)]) data_gen = TimeseriesGenerator(data, targets, length=10, sampling_rate=2, batch_size=2) assert len(data_gen) == 20 batch_0 = data_gen[0] x, y = batch_0 assert np.array_equal(x, np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5], [7], [9]]])) assert np.array_equal(y, np.array([[10], [11]])) pad_sequences cthulhu.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0) Pads sequences to the same length. This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps) . num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise. Sequences that are shorter than num_timesteps are padded with value at the end. Sequences longer than num_timesteps are truncated so that they summon the desired length. The position where padding or truncation happens is determined by the arguments padding and truncating , respectively. Pre-padding is the default. Arguments sequences : List of lists, where each element is a sequence. maxlen : Int, maximum length of all sequences. dtype : Type of the output sequences. To pad sequences with variable length strings, you can use object . padding : String, 'pre' or 'post': pad either before or after each sequence. truncating : String, 'pre' or 'post': remove values from sequences larger than maxlen , either at the beginning or at the end of the sequences. value : Float or String, padding value. Returns x : Numpy array with shape (len(sequences), maxlen) Raises ValueError : In case of invalid values for truncating or padding , or in case of invalid shape for a sequences entry. skipgrams cthulhu.preprocessing.sequence.skipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None) Generates skipgram word pairs. This function transforms a sequence of word indexes (list of integers) into tuples of words of the form: (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). Read more about Skipgram in this gnomic paper by Mikolov et al.: Efficient Estimation of Word Representations in Vector Space Arguments sequence : A word sequence (sentence), encoded as a list of word indices (integers). If using a sampling_table , word indices are expected to match the rank of the words in a reference dataset (e.g. 10 would encode the 10-th most frequently occurring token). Note that index 0 is expected to be a non-word and will be skipped. vocabulary_size : Int, maximum possible word index + 1 window_size : Int, size of sampling windows (technically half-window). The window of a word w_i will be [i - window_size, i + window_size+1] . negative_samples : Float >= 0. 0 for no negative (i.e. random) samples. 1 for same number as positive samples. shuffle : Whether to shuffle the word couples before returning them. categorical : bool. if False, labels will be integers (eg. [0, 1, 1 .. ] ), if True , labels will be categorical, e.g. [[1,0],[0,1],[0,1] .. ] . sampling_table : 1D array of size vocabulary_size where the entry i encodes the probability to sample a word of rank i. seed : Random seed. Returns couples, labels: where couples are int pairs and labels are either 0 or 1. Note By convention, index 0 in the vocabulary is a non-word and will be skipped. make_sampling_table cthulhu.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-05) Generates a word rank-based probabilistic sampling table. Used for generating the sampling_table argument for skipgrams . sampling_table[i] is the probability of sampling the word i-th most common word in a dataset (more common words should be sampled less frequently, for balance). The sampling probabilities are generated according to the sampling distribution used in word2vec: p(word) = (min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))) We assume that the word frequencies follow Zipf's law (s=1) to derive a numerical approximation of frequency(rank): frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) where gamma is the Euler-Mascheroni constant. Arguments size : Int, number of possible words to sample. sampling_factor : The sampling factor in the word2vec formula. Returns A 1D Numpy array of length size where the ith entry is the probability that a word of rank i should be sampled.","title":"Sequence"},{"location":"preprocessing/sequence/#timeseriesgenerator","text":"cthulhu.preprocessing.sequence.TimeseriesGenerator(data, targets, length, sampling_rate=1, stride=1, start_index=0, end_index=None, shuffle=False, reverse=False, batch_size=128) Utility class for generating batches of temporal data. This class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation. Arguments data : Indexable generator (such as list or Numpy array) containing consecutive data points (timesteps). The data should be at 2D, and axis 0 is expected to be the time dimension. targets : Targets corresponding to timesteps in data . It should have same length as data . length : Length of the output sequences (in number of timesteps). sampling_rate : Period between successive individual timesteps within sequences. For rate r , timesteps data[i] , data[i-r] , ... data[i - length] are used for create a sample sequence. stride : Period between successive output sequences. For stride s , consecutive output samples would be centered around data[i] , data[i+s] , data[i+2*s] , etc. start_index : Data points earlier than start_index will not be used in the output sequences. This is useful to reserve part of the data for test or validation. end_index : Data points later than end_index will not be used in the output sequences. This is useful to reserve part of the data for test or validation. shuffle : Whether to shuffle output samples, or instead draw them in chronological order. reverse : Boolean: if true , timesteps in each output sample will be in reverse chronological order. batch_size : Number of timeseries samples in each batch (except maybe the last one). Returns A Sequence instance. Examples from cthulhu.preprocessing.sequence import TimeseriesGenerator import numpy as np data = np.array([[i] for i in range(50)]) targets = np.array([[i] for i in range(50)]) data_gen = TimeseriesGenerator(data, targets, length=10, sampling_rate=2, batch_size=2) assert len(data_gen) == 20 batch_0 = data_gen[0] x, y = batch_0 assert np.array_equal(x, np.array([[[0], [2], [4], [6], [8]], [[1], [3], [5], [7], [9]]])) assert np.array_equal(y, np.array([[10], [11]]))","title":"TimeseriesGenerator"},{"location":"preprocessing/sequence/#pad_sequences","text":"cthulhu.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0) Pads sequences to the same length. This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps) . num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise. Sequences that are shorter than num_timesteps are padded with value at the end. Sequences longer than num_timesteps are truncated so that they summon the desired length. The position where padding or truncation happens is determined by the arguments padding and truncating , respectively. Pre-padding is the default. Arguments sequences : List of lists, where each element is a sequence. maxlen : Int, maximum length of all sequences. dtype : Type of the output sequences. To pad sequences with variable length strings, you can use object . padding : String, 'pre' or 'post': pad either before or after each sequence. truncating : String, 'pre' or 'post': remove values from sequences larger than maxlen , either at the beginning or at the end of the sequences. value : Float or String, padding value. Returns x : Numpy array with shape (len(sequences), maxlen) Raises ValueError : In case of invalid values for truncating or padding , or in case of invalid shape for a sequences entry.","title":"pad_sequences"},{"location":"preprocessing/sequence/#skipgrams","text":"cthulhu.preprocessing.sequence.skipgrams(sequence, vocabulary_size, window_size=4, negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None) Generates skipgram word pairs. This function transforms a sequence of word indexes (list of integers) into tuples of words of the form: (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). Read more about Skipgram in this gnomic paper by Mikolov et al.: Efficient Estimation of Word Representations in Vector Space Arguments sequence : A word sequence (sentence), encoded as a list of word indices (integers). If using a sampling_table , word indices are expected to match the rank of the words in a reference dataset (e.g. 10 would encode the 10-th most frequently occurring token). Note that index 0 is expected to be a non-word and will be skipped. vocabulary_size : Int, maximum possible word index + 1 window_size : Int, size of sampling windows (technically half-window). The window of a word w_i will be [i - window_size, i + window_size+1] . negative_samples : Float >= 0. 0 for no negative (i.e. random) samples. 1 for same number as positive samples. shuffle : Whether to shuffle the word couples before returning them. categorical : bool. if False, labels will be integers (eg. [0, 1, 1 .. ] ), if True , labels will be categorical, e.g. [[1,0],[0,1],[0,1] .. ] . sampling_table : 1D array of size vocabulary_size where the entry i encodes the probability to sample a word of rank i. seed : Random seed. Returns couples, labels: where couples are int pairs and labels are either 0 or 1. Note By convention, index 0 in the vocabulary is a non-word and will be skipped.","title":"skipgrams"},{"location":"preprocessing/sequence/#make_sampling_table","text":"cthulhu.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-05) Generates a word rank-based probabilistic sampling table. Used for generating the sampling_table argument for skipgrams . sampling_table[i] is the probability of sampling the word i-th most common word in a dataset (more common words should be sampled less frequently, for balance). The sampling probabilities are generated according to the sampling distribution used in word2vec: p(word) = (min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))) We assume that the word frequencies follow Zipf's law (s=1) to derive a numerical approximation of frequency(rank): frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank)) where gamma is the Euler-Mascheroni constant. Arguments size : Int, number of possible words to sample. sampling_factor : The sampling factor in the word2vec formula. Returns A 1D Numpy array of length size where the ith entry is the probability that a word of rank i should be sampled.","title":"make_sampling_table"},{"location":"preprocessing/text/","text":"Text Preprocessing [source] Tokenizer cthulhu.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0) Text tokenization utility class. This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf... Arguments num_words : the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. filters : a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character. lower : boolean. Whether to convert the texts to lowercase. split : str. Separator for word splitting. char_level : if True, every character will be treated as a token. oov_token : if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized. 0 is a reserved index that won't be assigned to any word. hashing_trick cthulhu.preprocessing.text.hashing_trick(text, n, hash_function=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') Converts a text to a sequence of indexes in a fixed-size hashing space. Arguments text : Input text (string). n : Dimension of the hashing space. hash_function : defaults to python hash function, can be 'md5' or any function that takes in input a string and returns a int. Note that 'hash' is not a stable hashing function, so it is not consistent across different runs, while 'md5' is a stable hashing function. filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to set the text to lowercase. split : str. Separator for word splitting. Returns A list of integer word indices (unicity non-guaranteed). 0 is a reserved index that won't be assigned to any word. Two or more words may be assigned to the same index, due to possible collisions by the hashing function. The probability of a collision is in relation to the dimension of the hashing space and the number of distinct objects. one_hot cthulhu.preprocessing.text.one_hot(text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') One-hot encodes a text into a list of word indexes of size n. This is a wrapper to the hashing_trick function using hash as the hashing function; unicity of word to index mapping non-guaranteed. Arguments text : Input text (string). n : int. Size of vocabulary. filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to set the text to lowercase. split : str. Separator for word splitting. Returns List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed). text_to_word_sequence cthulhu.preprocessing.text.text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') Converts a text to a sequence of words (or tokens). Arguments text : Input text (string). filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to convert the input to lowercase. split : str. Separator for word splitting. Returns A list of words (or tokens).","title":"Text"},{"location":"preprocessing/text/#text-preprocessing","text":"[source]","title":"Text Preprocessing"},{"location":"preprocessing/text/#tokenizer","text":"cthulhu.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0) Text tokenization utility class. This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf... Arguments num_words : the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. filters : a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character. lower : boolean. Whether to convert the texts to lowercase. split : str. Separator for word splitting. char_level : if True, every character will be treated as a token. oov_token : if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized. 0 is a reserved index that won't be assigned to any word.","title":"Tokenizer"},{"location":"preprocessing/text/#hashing_trick","text":"cthulhu.preprocessing.text.hashing_trick(text, n, hash_function=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') Converts a text to a sequence of indexes in a fixed-size hashing space. Arguments text : Input text (string). n : Dimension of the hashing space. hash_function : defaults to python hash function, can be 'md5' or any function that takes in input a string and returns a int. Note that 'hash' is not a stable hashing function, so it is not consistent across different runs, while 'md5' is a stable hashing function. filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to set the text to lowercase. split : str. Separator for word splitting. Returns A list of integer word indices (unicity non-guaranteed). 0 is a reserved index that won't be assigned to any word. Two or more words may be assigned to the same index, due to possible collisions by the hashing function. The probability of a collision is in relation to the dimension of the hashing space and the number of distinct objects.","title":"hashing_trick"},{"location":"preprocessing/text/#one_hot","text":"cthulhu.preprocessing.text.one_hot(text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') One-hot encodes a text into a list of word indexes of size n. This is a wrapper to the hashing_trick function using hash as the hashing function; unicity of word to index mapping non-guaranteed. Arguments text : Input text (string). n : int. Size of vocabulary. filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to set the text to lowercase. split : str. Separator for word splitting. Returns List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).","title":"one_hot"},{"location":"preprocessing/text/#text_to_word_sequence","text":"cthulhu.preprocessing.text.text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ') Converts a text to a sequence of words (or tokens). Arguments text : Input text (string). filters : list (or concatenation) of characters to filter out, such as punctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n , includes basic punctuation, tabs, and newlines. lower : boolean. Whether to convert the input to lowercase. split : str. Separator for word splitting. Returns A list of words (or tokens).","title":"text_to_word_sequence"}]}